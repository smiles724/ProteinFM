{"cells":[{"cell_type":"markdown","id":"11360105","metadata":{"id":"11360105"},"source":["\n","# BigProtein-Qwen2.5 — Step‑by‑Step Test Notebook (Colab)\n","This notebook lets you **test each component** of the protein‑conditioned Qwen2.5 pipeline *before* running full training.  \n","It mirrors the main script logic, but runs **function‑by‑function** so you can see errors early with clear tracebacks.\n","\n","> **Files expected in the working directory** (upload or mount a folder containing them):  \n","> - `bigmodel_joint_train.py`  \n","> - `protein_encoder.py`  \n","> - `structure_encoder.py`\n"]},{"cell_type":"code","execution_count":1,"id":"IUYulnhL8NM-","metadata":{"id":"IUYulnhL8NM-","executionInfo":{"status":"ok","timestamp":1759416097402,"user_tz":240,"elapsed":726,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"outputs":[],"source":["#@title Mount Google Drive\n","from pathlib import Path\n","from huggingface_hub import snapshot_download\n","import os, json, pickle, pandas as pd\n","from tqdm import tqdm\n","from rich import print as rprint"]},{"cell_type":"code","execution_count":3,"id":"DxEz1S18Ys00","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxEz1S18Ys00","outputId":"edccd990-0579-4461-d97d-51359de1033e","executionInfo":{"status":"ok","timestamp":1759416243107,"user_tz":240,"elapsed":17776,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","Using Google Drive folder as BASE_DIR: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","%cd /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","\n","from pathlib import Path\n","BASE_DIR = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")\n","OUT_DIR  = BASE_DIR / \"sft_test_demo\"\n","print(f\"Using Google Drive folder as BASE_DIR: {BASE_DIR}\")\n"]},{"cell_type":"markdown","id":"ef586a49","metadata":{"id":"ef586a49"},"source":["\n","## 0) Runtime & Installs\n","If you're on Google Colab, run this cell to install dependencies.\n"]},{"cell_type":"code","execution_count":4,"id":"W8upz3dOoYqH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8upz3dOoYqH","outputId":"a9d70378-691b-41cb-a7bd-e2830629abfc","executionInfo":{"status":"ok","timestamp":1759416254350,"user_tz":240,"elapsed":11239,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Oct  2 14:44:02 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n","| N/A   31C    P0             52W /  400W |       0MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Check GPU\n","!nvidia-smi\n","\n","# Fresh pip + libs (PyTorch CUDA 12.1 build + matching libs)\n","%pip -q install --upgrade pip\n","%pip install -q --index-url https://download.pytorch.org/whl/cu126 \\\n","  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n","%pip -q install transformers==4.56.1 huggingface_hub==0.35.0 tqdm safetensors"]},{"cell_type":"code","execution_count":5,"id":"ap908Ts551uw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ap908Ts551uw","outputId":"cfc8cc53-e412-4148-85df-38a3ba338acc","executionInfo":{"status":"ok","timestamp":1759416271034,"user_tz":240,"elapsed":16682,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch            : 2.8.0+cu126\n","transformers     : 4.56.1\n","huggingface_hub  : 0.35.0\n","✅ Top-level EsmForMaskedLM import OK\n"]}],"source":["# --- Version & import sanity checks ---\n","import torch, transformers, huggingface_hub\n","print(\"torch            :\", torch.__version__)\n","print(\"transformers     :\", transformers.__version__)\n","print(\"huggingface_hub  :\", huggingface_hub.__version__)\n","\n","# Top-level ESM import should work on 4.56.1\n","try:\n","    from transformers import AutoTokenizer, EsmForMaskedLM\n","    print(\"✅ Top-level EsmForMaskedLM import OK\")\n","except Exception as e:\n","    print(\"❌ Top-level EsmForMaskedLM import failed:\", repr(e))\n","    # Fallback check (direct module path)\n","    try:\n","        from transformers.models.esm.modeling_esm import EsmForMaskedLM as _E\n","        print(\"✅ Direct modeling_esm import OK (fallback)\")\n","    except Exception as ee:\n","        print(\"❌ Direct modeling_esm import failed too:\", repr(ee))"]},{"cell_type":"markdown","id":"f9G7pU_Rp4mF","metadata":{"id":"f9G7pU_Rp4mF"},"source":["torch            : 2.8.0+cu126\n","\n","transformers     : 4.56.1\n","\n","huggingface_hub  : 0.35.0\n","\n","✅ Top-level EsmForMaskedLM import OK"]},{"cell_type":"markdown","id":"3ZDFip2psPXw","metadata":{"id":"3ZDFip2psPXw"},"source":["\n","## 1) Loading Encoder Checkpoints"]},{"cell_type":"code","execution_count":6,"id":"fd2cdf79","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd2cdf79","outputId":"1494e199-6ea3-4e4a-f0a9-c4acfc5da425","executionInfo":{"status":"ok","timestamp":1759416271800,"user_tz":240,"elapsed":761,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/protein2desc_sft_ALLFOUR_c000-009_fullcot.jsonl\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test\n"]}],"source":["\n","# === LLM & Encoders ===\n","MODEL_NAME         = \"Qwen/Qwen2.5-0.5B-Instruct\"   # Small-ish for Colab testing\n","PROTEIN_CONFIG = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\"\n","STRUCTURE_CONFIG = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\"\n","PROTREK_CKPT    = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\"\n","PROJECT_DIR = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\"\n","DATA_JSONL = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/protein2desc_sft_ALLFOUR_c000-009_fullcot.jsonl\"\n","OUT_DIR = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test\"\n","\n","for p in [PROJECT_DIR, DATA_JSONL, PROTEIN_CONFIG, STRUCTURE_CONFIG, PROTREK_CKPT, OUT_DIR]:\n","    print(\"✓ exists:\", os.path.exists(p), p)\n","\n"]},{"cell_type":"code","execution_count":7,"id":"ET3YTyMxpKOS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ET3YTyMxpKOS","outputId":"6970ab38-4faf-450f-9fa6-16db4e542166","executionInfo":{"status":"ok","timestamp":1759416271819,"user_tz":240,"elapsed":15,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}],"source":["# === Prefix/Proj ===\n","SINGLE_TOKEN_PREFIX = False     # True -> 1 token; False -> soft prefix of length PREFIX_LEN\n","PREFIX_LEN          = 4\n","PROJ_HID            = 1024\n","DROPOUT             = 0.10\n","\n","# === Training toggles ===\n","USE_LORA            = False\n","TRAIN_ENCODERS      = False    # True = end-to-end; False = freeze encoders\n","FREEZE_PROTEIN      = False    # only used if TRAIN_ENCODERS=True\n","FREEZE_STRUCTURE    = False    # only used if TRAIN_ENCODERS=True\n","GRAD_CHECKPOINT     = False\n","\n","# === Misc ===\n","DEVICE              = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","MAX_LEN             = 512\n","BSZ                 = 2\n","ACCUM               = 1\n","LR                  = 5e-5\n","WARMUP_RATIO        = 0.03\n","EPOCHS              = 1\n","OUTPUT_DIR          = \"runs/colab_smoketest\"\n","LOG_EVERY           = 1\n","\n","print(\"Device:\", DEVICE)"]},{"cell_type":"code","execution_count":null,"id":"-Qib_xuxpMXY","metadata":{"id":"-Qib_xuxpMXY"},"outputs":[],"source":["# SUBSET_JSONL = os.path.join(PROJECT_DIR, \"train_subset_100.jsonl\")\n","\n","# # Write first 1000 non-empty lines to subset\n","# count = 0\n","# with open(DATA_JSONL, \"r\", encoding=\"utf-8\") as fin, open(SUBSET_JSONL, \"w\", encoding=\"utf-8\") as fout:\n","#     for line in fin:\n","#         if not line.strip():\n","#             continue\n","#         fout.write(line)\n","#         count += 1\n","#         if count >= 100:\n","#             break\n","\n","# print(\"Wrote subset lines:\", count, \"->\", SUBSET_JSONL)"]},{"cell_type":"code","execution_count":8,"id":"DLo3zwCqpNZY","metadata":{"id":"DLo3zwCqpNZY","executionInfo":{"status":"ok","timestamp":1759416274414,"user_tz":240,"elapsed":2578,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"outputs":[],"source":["from train_prefix_qwen import train, parse_args\n","SUBSET_JSONL = os.path.join(PROJECT_DIR, \"train_subset_100.jsonl\")"]},{"cell_type":"code","execution_count":null,"id":"416Reck1rGgv","metadata":{"id":"416Reck1rGgv"},"outputs":[],"source":["import types, os\n","\n","SAVE_DIR = os.path.join(PROJECT_DIR, \"runs_colab_test\")\n","\n","args = types.SimpleNamespace(\n","    # Data\n","    train_file   = DATA_JSONL,\n","    val_file     = None,\n","    batch_size   = 1,         # adjust if you want\n","    accum_steps  = 2,\n","    max_len      = 1024,       # keep modest for speed\n","    # Model\n","    model_name   = MODEL_NAME,\n","    dtype        = \"fp32\",    # or \"bf16\" on A100 for speed\n","    prefix_len   = 4,         # try 1 or 4+\n","    prefix_gate  = 0.5,       # stabilizer on the soft prefix\n","    learnable_gate = False,\n","    freeze_llm   = False,     # True = projector-only\n","    train_encoders = True,   # keep ESM encoders frozen for speed\n","    # Encoders\n","    protein_config = PROTEIN_CONFIG,\n","    structure_config = STRUCTURE_CONFIG,\n","    protrek_ckpt  = PROTREK_CKPT,\n","    prot_slot     = 1,\n","    stru_slot     = 3,\n","    # Optim\n","    epochs      = 3,\n","    lr          = 3e-5,       # projector+LLM small LR\n","    weight_decay= 0.05,\n","    # Save/eval\n","    save_dir    = OUT_DIR,\n","    save_every  = 1500,\n","    eval_every  = 0,\n","    # Misc\n","    seed        = 42,\n",")\n","\n","# Kick off training\n","train(args)"]},{"cell_type":"markdown","id":"ce918337","metadata":{"id":"ce918337"},"source":["## ️🧪 Test training with `train_prefix_*` (imports `train`, `parse_args`)"]},{"cell_type":"code","execution_count":null,"id":"375d9b76","metadata":{"id":"375d9b76"},"outputs":[],"source":["%pip -q install --upgrade accelerate transformers peft datasets"]},{"cell_type":"code","execution_count":null,"id":"ede3e504","metadata":{"id":"ede3e504"},"outputs":[],"source":["import glob, importlib, sys\n","from pathlib import Path\n","\n","# Optionally set MODULE_NAME manually, e.g.:\n","# MODULE_NAME = 'train_prefix_qwen_fsdp'\n","MODULE_NAME = None\n","\n","candidates = sorted(glob.glob('train_prefix_*.py'))\n","print('Found candidates:', candidates)\n","if MODULE_NAME is None:\n","    if not candidates:\n","        raise FileNotFoundError('Put your train_prefix_*.py in the current directory.')\n","    MODULE_NAME = Path(candidates[-1]).stem  # pick latest by name\n","print('Importing from module:', MODULE_NAME)\n","\n","mod = importlib.import_module(MODULE_NAME)\n","from importlib import reload\n","mod = reload(mod)\n","train = getattr(mod, 'train')\n","parse_args = getattr(mod, 'parse_args')\n","print('Imported: train, parse_args from', MODULE_NAME)"]},{"cell_type":"code","execution_count":null,"id":"c8689bac","metadata":{"id":"c8689bac"},"outputs":[],"source":["import os, json\n","os.makedirs('sft_data', exist_ok=True)\n","toy = [\n","    {\n","        'prompt': 'Describe the likely function of this protein.',\n","        'response': 'This appears to be an enzyme with possible hydrolase activity.',\n","        'aa_seq': 'MKTFFVAIATGAFSATA',\n","        'stru_str': None\n","    },\n","    {\n","        'prompt': 'What domain might this protein contain?',\n","        'response': 'Likely contains a Rossmann-like fold domain.',\n","        'aa_seq': 'MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNLHGLFGRKTGQAP',\n","        'stru_str': 'ACDEFGHIKLMNPQRSTVWY'\n","    }\n","]\n","with open('sft_data/train_tiny.jsonl', 'w', encoding='utf-8') as f:\n","    for ex in toy: f.write(json.dumps(ex)+'\\n')\n","with open('sft_data/val_tiny.jsonl', 'w', encoding='utf-8') as f:\n","    for ex in toy: f.write(json.dumps(ex)+'\\n')\n","print('Wrote sft_data/train_tiny.jsonl and sft_data/val_tiny.jsonl')"]},{"cell_type":"code","execution_count":null,"id":"4329269a","metadata":{"id":"4329269a"},"outputs":[],"source":["argv = [\n","  '--train-file', 'sft_data/train_tiny.jsonl',\n","  '--val-file', 'sft_data/val_tiny.jsonl',\n","  '--model-name', 'Qwen/Qwen2.5-0.5B-Instruct',\n","  '--protein-config', 'facebook/esm2_t12_35M_UR50D',\n","  '--structure-config', 'facebook/esm2_t12_35M_UR50D',\n","  '--prefix-len', '2',\n","  '--batch-size', '1',\n","  '--accum-steps', '1',\n","  '--max-len', '256',\n","  '--epochs', '1',\n","  '--save-dir', 'runs/test',\n","  '--log-every', '1'\n","]\n","print('Args:', argv)\n","args = parse_args(argv)\n","print(args)\n","train(args)"]},{"cell_type":"markdown","id":"dd3e453d","metadata":{"id":"dd3e453d"},"source":["\n","> **Multi-GPU tip (run from a terminal, not inside this notebook cell):**\n","> ```bash\n","> accelerate launch --config_file accelerate_fsdp_bf16.yaml ${MODULE_NAME}.py \\\n",">   --train-file sft_data/train_tiny.jsonl \\\n",">   --val-file   sft_data/val_tiny.jsonl \\\n",">   --model-name Qwen/Qwen2.5-7B-Instruct \\\n",">   --protein-config facebook/esm2_t12_35M_UR50D \\\n",">   --structure-config facebook/esm2_t12_35M_UR50D \\\n",">   --prefix-len 4 --batch-size 1 --accum-steps 8 --max-len 2048 \\\n",">   --dtype bf16 --lr 1e-5 --weight-decay 0.05 \\\n",">   --save-dir ./runs --save-every 1000 --eval-every 0\n","> ```\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}