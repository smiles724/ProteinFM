{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11360105",
      "metadata": {
        "id": "11360105"
      },
      "source": [
        "\n",
        "# BigProtein-Qwen2.5 — Step‑by‑Step Test Notebook (Colab)\n",
        "This notebook lets you **test each component** of the protein‑conditioned Qwen2.5 pipeline *before* running full training.  \n",
        "It mirrors the main script logic, but runs **function‑by‑function** so you can see errors early with clear tracebacks.\n",
        "\n",
        "> **Files expected in the working directory** (upload or mount a folder containing them):  \n",
        "> - `bigmodel_joint_train.py`  \n",
        "> - `protein_encoder.py`  \n",
        "> - `structure_encoder.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "IUYulnhL8NM-",
      "metadata": {
        "id": "IUYulnhL8NM-"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "from pathlib import Path\n",
        "from huggingface_hub import snapshot_download\n",
        "import os, json, pickle, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rich import print as rprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DxEz1S18Ys00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxEz1S18Ys00",
        "outputId": "8cc13874-83ea-4872-ce5b-ec4d53b1e2eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n",
            "Using Google Drive folder as BASE_DIR: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n",
        "\n",
        "from pathlib import Path\n",
        "BASE_DIR = Path()\n",
        "OUT_DIR  = BASE_DIR / \"sft_test_demo\"\n",
        "print(f\"Using Google Drive folder as BASE_DIR: {BASE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef586a49",
      "metadata": {
        "id": "ef586a49"
      },
      "source": [
        "\n",
        "## 0) Runtime & Installs\n",
        "If you're on Google Colab, run this cell to install dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "PIiOuagG5uY9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIiOuagG5uY9",
        "outputId": "b6ad8238-7712-4df9-f311-6f103c1b123b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No local 'transformers' directory in CWD. Good to go.\n"
          ]
        }
      ],
      "source": [
        "# --- Make sure no local \"transformers\" folder shadows the pip package ---\n",
        "import sys, os, glob\n",
        "if os.path.isdir(\"transformers\"):\n",
        "    print(\"⚠️ Local 'transformers' directory detected in CWD; this will shadow the pip package.\")\n",
        "    print(\"   -> rename or remove it before continuing.\")\n",
        "else:\n",
        "    print(\"No local 'transformers' directory in CWD. Good to go.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "efTMPwP95yNK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efTMPwP95yNK",
        "outputId": "f3949d06-4008-415b-ac05-9fa5be9aa709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.0 requires huggingface-hub>=0.20.0, which is not installed.\n",
            "sentence-transformers 5.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "timm 1.0.19 requires huggingface_hub, which is not installed.\n",
            "accelerate 1.10.1 requires huggingface_hub>=0.21.0, which is not installed.\n",
            "torchtune 0.6.1 requires huggingface_hub[hf_transfer], which is not installed.\n",
            "torchtune 0.6.1 requires tokenizers, which is not installed.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, which is not installed.\n",
            "peft 0.17.1 requires transformers, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# --- Clean out conflicting wheels so we can pin exactly what we want ---\n",
        "%pip uninstall -y -q torch torchvision torchaudio xformers transformers tokenizers huggingface_hub\n",
        "\n",
        "# --- Install PyTorch 2.8.0 (CUDA 12.6 wheels) + matching torchvision/torchaudio ---\n",
        "# (If you don't have a GPU runtime, remove the --index-url line)\n",
        "%pip install -q --index-url https://download.pytorch.org/whl/cu126 \\\n",
        "  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n",
        "\n",
        "# --- Install Transformers & HF Hub (your requested versions) ---\n",
        "%pip install -q \"transformers==4.56.1\" \"huggingface_hub==0.35.0\"\n",
        "\n",
        "# --- Extras you used elsewhere ---\n",
        "%pip install -q peft accelerate datasets tqdm rich pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "MJfQXF1U5z8q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJfQXF1U5z8q",
        "outputId": "2fa08b53-f591-4fc8-cf9a-fc40993cc197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing sanitized encoder requirements (Torch/Transformers lines removed):\n",
            "  - removed: torch>=2.0.1,<2.3\n",
            "  - removed: transformers>=4.38,<5\n"
          ]
        }
      ],
      "source": [
        "# --- Install your encoder requirements, but ignore any Torch/Transformers pins inside ---\n",
        "import re, pathlib, textwrap, sys\n",
        "\n",
        "REQ_FILE = pathlib.Path(\"requirements_encoders.txt\")\n",
        "if REQ_FILE.exists():\n",
        "    lines = REQ_FILE.read_text().splitlines()\n",
        "    # Drop lines that would force Torch/TV/TA/Transformers downgrades\n",
        "    keep = [\n",
        "        ln for ln in lines\n",
        "        if not re.match(r'^\\s*(torch|torchvision|torchaudio|transformers)\\b', ln, flags=re.IGNORECASE)\n",
        "    ]\n",
        "    tmp = pathlib.Path(\"/content/encoders_req_no_torch.txt\")\n",
        "    tmp.write_text(\"\\n\".join(keep) + \"\\n\")\n",
        "    print(\"Installing sanitized encoder requirements (Torch/Transformers lines removed):\")\n",
        "    for ln in lines:\n",
        "        if ln not in keep and ln.strip():\n",
        "            print(\"  - removed:\", ln)\n",
        "    %pip install -q -r {str(tmp)}\n",
        "else:\n",
        "    print(\"No requirements_encoders.txt found; skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ap908Ts551uw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap908Ts551uw",
        "outputId": "74c118c6-3cbd-4b2d-9b4e-9559f7a2a2db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch            : 2.8.0+cu126\n",
            "transformers     : 4.56.1\n",
            "huggingface_hub  : 0.35.0\n",
            "✅ Top-level EsmForMaskedLM import OK\n"
          ]
        }
      ],
      "source": [
        "# --- Version & import sanity checks ---\n",
        "import torch, transformers, huggingface_hub\n",
        "print(\"torch            :\", torch.__version__)\n",
        "print(\"transformers     :\", transformers.__version__)\n",
        "print(\"huggingface_hub  :\", huggingface_hub.__version__)\n",
        "\n",
        "# Top-level ESM import should work on 4.56.1\n",
        "try:\n",
        "    from transformers import AutoTokenizer, EsmForMaskedLM\n",
        "    print(\"✅ Top-level EsmForMaskedLM import OK\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Top-level EsmForMaskedLM import failed:\", repr(e))\n",
        "    # Fallback check (direct module path)\n",
        "    try:\n",
        "        from transformers.models.esm.modeling_esm import EsmForMaskedLM as _E\n",
        "        print(\"✅ Direct modeling_esm import OK (fallback)\")\n",
        "    except Exception as ee:\n",
        "        print(\"❌ Direct modeling_esm import failed too:\", repr(ee))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9G7pU_Rp4mF",
      "metadata": {
        "id": "f9G7pU_Rp4mF"
      },
      "source": [
        "torch            : 2.8.0+cu126\n",
        "\n",
        "transformers     : 4.56.1\n",
        "\n",
        "huggingface_hub  : 0.35.0\n",
        "\n",
        "✅ Top-level EsmForMaskedLM import OK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ZDFip2psPXw",
      "metadata": {
        "id": "3ZDFip2psPXw"
      },
      "source": [
        "\n",
        "## 1) Loading Encoder Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d35cfada",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35cfada",
        "outputId": "e470c275-2825-450f-be5d-9c7e118ec4fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloaded modules.\n",
            "Torch: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "#@title  Reloading encoders\n",
        "import importlib, os, torch\n",
        "\n",
        "import protein_encoder as protein_encoder_mod\n",
        "import structure_encoder as structure_encoder_mod\n",
        "import bigmodel_joint_train as train_mod\n",
        "\n",
        "def reload_all():\n",
        "    importlib.reload(protein_encoder_mod)\n",
        "    importlib.reload(structure_encoder_mod)\n",
        "    importlib.reload(train_mod)\n",
        "    print(\"Reloaded modules.\")\n",
        "\n",
        "reload_all()\n",
        "print(\"Torch:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "yfIvvZmm3-H0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfIvvZmm3-H0",
        "outputId": "d60892ee-9725-4d79-a4aa-2113dfc80998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers: 4.56.1\n",
            "torch: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"torch:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8xDmkNRoUjx0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xDmkNRoUjx0",
        "outputId": "43690c7d-c167-4003-f8d7-20b25a48e711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Checkpoint tensors: 633\n",
            "Detected slots: [0, 1, 2, 3] \n",
            "Candidate slot for ProteinEncoder : 1 (exact-key hits=215)\n",
            "Candidate slot for StructureEncoder: 1 (exact-key hits=215)\n",
            "\n",
            "[ProteinEncoder] slot=1  loaded_tensors=216\n",
            "  matched=215  missing=0  unexpected=1\n",
            "  matched keys (first 20): ['model.esm.embeddings.word_embeddings.weight', 'model.esm.encoder.emb_layer_norm_after.bias', 'model.esm.encoder.emb_layer_norm_after.weight', 'model.esm.encoder.layer.0.LayerNorm.bias', 'model.esm.encoder.layer.0.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.LayerNorm.bias', 'model.esm.encoder.layer.0.attention.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.output.dense.bias', 'model.esm.encoder.layer.0.attention.output.dense.weight', 'model.esm.encoder.layer.0.attention.self.key.bias', 'model.esm.encoder.layer.0.attention.self.key.weight', 'model.esm.encoder.layer.0.attention.self.query.bias', 'model.esm.encoder.layer.0.attention.self.query.weight', 'model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'model.esm.encoder.layer.0.attention.self.value.bias', 'model.esm.encoder.layer.0.attention.self.value.weight', 'model.esm.encoder.layer.0.intermediate.dense.bias', 'model.esm.encoder.layer.0.intermediate.dense.weight', 'model.esm.encoder.layer.0.output.dense.bias', 'model.esm.encoder.layer.0.output.dense.weight']\n",
            "  unexpected (first 12): ['model.esm.embeddings.position_ids']\n",
            "\n",
            "[StructureEncoder] slot=3  loaded_tensors=216\n",
            "  matched=215  missing=0  unexpected=1\n",
            "  matched keys (first 20): ['model.esm.embeddings.word_embeddings.weight', 'model.esm.encoder.emb_layer_norm_after.bias', 'model.esm.encoder.emb_layer_norm_after.weight', 'model.esm.encoder.layer.0.LayerNorm.bias', 'model.esm.encoder.layer.0.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.LayerNorm.bias', 'model.esm.encoder.layer.0.attention.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.output.dense.bias', 'model.esm.encoder.layer.0.attention.output.dense.weight', 'model.esm.encoder.layer.0.attention.self.key.bias', 'model.esm.encoder.layer.0.attention.self.key.weight', 'model.esm.encoder.layer.0.attention.self.query.bias', 'model.esm.encoder.layer.0.attention.self.query.weight', 'model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'model.esm.encoder.layer.0.attention.self.value.bias', 'model.esm.encoder.layer.0.attention.self.value.weight', 'model.esm.encoder.layer.0.intermediate.dense.bias', 'model.esm.encoder.layer.0.intermediate.dense.weight', 'model.esm.encoder.layer.0.output.dense.bias', 'model.esm.encoder.layer.0.output.dense.weight']\n",
            "  unexpected (first 12): ['model.esm.embeddings.position_ids']\n",
            "\n",
            "Cosine(protein  before vs after) : -0.01761733740568161\n",
            "Cosine(structure before vs after) : -0.020264290273189545\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# ==== Robust loader for ParameterList-style ProTrek checkpoints ====\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "import importlib, torch\n",
        "\n",
        "# --- Your paths ---\n",
        "PROTEIN_CONFIG   = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\")\n",
        "STRUCTURE_CONFIG = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\")\n",
        "CKPT             = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\")\n",
        "\n",
        "# --- Import/reload your encoder classes ---\n",
        "import protein_encoder as protein_encoder_mod\n",
        "import structure_encoder as structure_encoder_mod\n",
        "importlib.reload(protein_encoder_mod)\n",
        "importlib.reload(structure_encoder_mod)\n",
        "ProteinEncoder   = protein_encoder_mod.ProteinEncoder\n",
        "StructureEncoder = structure_encoder_mod.StructureEncoder\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# --- Build fresh (random-init) encoders from configs only ---\n",
        "prot_enc = ProteinEncoder(str(PROTEIN_CONFIG), out_dim=1024, load_pretrained=False).to(DEVICE).eval()\n",
        "stru_enc = StructureEncoder(str(STRUCTURE_CONFIG), out_dim=1024, load_pretrained=False).to(DEVICE).eval()\n",
        "\n",
        "# --- Utility: cosine similarity between two batches of embeddings ---\n",
        "@torch.no_grad()\n",
        "def cosine_sim(a, b):\n",
        "    a = torch.nn.functional.normalize(a, dim=-1)\n",
        "    b = torch.nn.functional.normalize(b, dim=-1)\n",
        "    return float((a*b).sum(dim=-1).mean().cpu())\n",
        "\n",
        "# --- Make baseline outputs BEFORE loading (to prove they change) ---\n",
        "@torch.no_grad()\n",
        "def get_prot_repr(model):\n",
        "    seqs = [\"MKTFFVAIATGAFSATA\", \"MGDVEKGKKIFIMKCSQCHTVEK\"]  # toy AA\n",
        "    return model.get_repr(seqs, batch_size=2, verbose=False).to(\"cpu\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_stru_repr(model):\n",
        "    seqs = [\"acdefghiklmnpqrstvwy\", \"acdefghi\"]  # toy 3Di (foldseek) strings\n",
        "    return model.get_repr(seqs, batch_size=2, verbose=False).to(\"cpu\")\n",
        "\n",
        "prot_before = get_prot_repr(prot_enc)\n",
        "stru_before = get_stru_repr(stru_enc)\n",
        "\n",
        "# --- Load checkpoint & locate the real state_dict (handles common wrappers) ---\n",
        "raw = torch.load(str(CKPT), map_location=\"cpu\")\n",
        "\n",
        "def locate_state_dict(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        for k in (\"model\", \"state_dict\", \"model_state_dict\", \"weights\", \"params\"):\n",
        "            if k in obj and isinstance(obj[k], dict) and any(torch.is_tensor(v) for v in obj[k].values()):\n",
        "                return obj[k]\n",
        "        if any(torch.is_tensor(v) for v in obj.values()):\n",
        "            return obj\n",
        "    return obj  # fallback\n",
        "\n",
        "sd = locate_state_dict(raw)\n",
        "print(f\"Checkpoint tensors: {len(sd)}\")\n",
        "\n",
        "# --- Split by ParameterList slot: top-level token before the first dot, if it's an integer ---\n",
        "slots = defaultdict(dict)\n",
        "for k, v in sd.items():\n",
        "    head = k.split(\".\", 1)[0]\n",
        "    if head.isdigit():\n",
        "        slots[int(head)][k[len(head)+1:]] = v  # strip \"N.\" prefix\n",
        "    else:\n",
        "        slots[None][k] = v  # non-slotted (rare)\n",
        "\n",
        "print(\"Detected slots:\", sorted([s for s in slots.keys() if s is not None]),\n",
        "      \"(None present)\" if None in slots else \"\")\n",
        "\n",
        "# --- Pick best slot for each encoder by # of exact key matches against target state_dict keys ---\n",
        "def best_slot_for(module, slots_dict):\n",
        "    tgt = set(module.state_dict().keys())\n",
        "    best = (None, 0)\n",
        "    for s, sub in slots_dict.items():\n",
        "        if s is None:  # skip un-slotted container for this matching\n",
        "            continue\n",
        "        hits = sum(1 for k in sub.keys() if k in tgt)\n",
        "        if hits > best[1]:\n",
        "            best = (s, hits)\n",
        "    return best  # (slot, hits)\n",
        "\n",
        "prot_slot, prot_hits = best_slot_for(prot_enc, slots)\n",
        "stru_slot, stru_hits = best_slot_for(stru_enc, slots)\n",
        "\n",
        "print(f\"Candidate slot for ProteinEncoder : {prot_slot} (exact-key hits={prot_hits})\")\n",
        "print(f\"Candidate slot for StructureEncoder: {stru_slot} (exact-key hits={stru_hits})\")\n",
        "\n",
        "# --- Load with strict=False and report matched/missing/unexpected + preview of matched keys ---\n",
        "def load_from_slot(module, slot_idx, tag):\n",
        "    sub = slots.get(slot_idx, {})\n",
        "    tgt_keys = set(module.state_dict().keys())\n",
        "    matched_keys = sorted([k for k in sub.keys() if k in tgt_keys])\n",
        "    missing, unexpected = module.load_state_dict(sub, strict=False)\n",
        "    print(f\"\\n[{tag}] slot={slot_idx}  loaded_tensors={len(sub)}\")\n",
        "    print(f\"  matched={len(matched_keys)}  missing={len(missing)}  unexpected={len(unexpected)}\")\n",
        "    print(\"  matched keys (first 20):\", matched_keys[:20])\n",
        "    if missing:\n",
        "        print(\"  missing (first 12):\", list(missing)[:12])\n",
        "    if unexpected:\n",
        "        print(\"  unexpected (first 12):\", list(unexpected)[:12])\n",
        "\n",
        "load_from_slot(prot_enc, 1, \"ProteinEncoder\")\n",
        "load_from_slot(stru_enc, 3, \"StructureEncoder\")\n",
        "\n",
        "# --- Prove it changed: cosine(before, after) ---\n",
        "prot_after = get_prot_repr(prot_enc)\n",
        "stru_after = get_stru_repr(stru_enc)\n",
        "\n",
        "print(\"\\nCosine(protein  before vs after) :\", cosine_sim(prot_before, prot_after))\n",
        "print(\"Cosine(structure before vs after) :\", cosine_sim(stru_before, stru_after))\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dj_c84SUq7v9",
      "metadata": {
        "id": "Dj_c84SUq7v9"
      },
      "source": [
        "ParameterList indexing : 0=temp, 1=protein, 2=text, 3=structure\n",
        "\n",
        "(hard coded by protrek, just use, no need for fix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Omxmdq0Tr5MD",
      "metadata": {
        "id": "Omxmdq0Tr5MD"
      },
      "source": [
        "Interpretation tips:\n",
        "\n",
        "  • If 'matched' above is large (hundreds+) and cosine(sim) << 0.99, weights changed -> checkpoint likely loaded.\n",
        "\n",
        "  • If matched=0 and cosine(sim)≈1.0, nothing changed -> still random.\n",
        "  \n",
        "  • identical_params counts exact-equal tensors before vs after; lower is better (more changed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6h3RCxGGrNmK",
      "metadata": {
        "id": "6h3RCxGGrNmK"
      },
      "source": [
        "Currently, Position idx is not correctly loaded from .pt files to the encoder model for unknown reason. However, it is not included in current model; I will ignore it unless there are further evidence showing this actually matters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "MDbBfFynANM-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDbBfFynANM-",
        "outputId": "8206bf60-43a0-4559-988e-f04859f46a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "\n",
            "== ProteinEncoder expected keys (target) ==\n",
            "Total keys: 215\n",
            "Sample keys: ['model.esm.embeddings.word_embeddings.weight', 'model.esm.encoder.layer.0.attention.self.query.weight', 'model.esm.encoder.layer.0.attention.self.query.bias', 'model.esm.encoder.layer.0.attention.self.key.weight', 'model.esm.encoder.layer.0.attention.self.key.bias', 'model.esm.encoder.layer.0.attention.self.value.weight', 'model.esm.encoder.layer.0.attention.self.value.bias', 'model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'model.esm.encoder.layer.0.attention.output.dense.weight', 'model.esm.encoder.layer.0.attention.output.dense.bias', 'model.esm.encoder.layer.0.attention.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.LayerNorm.bias', 'model.esm.encoder.layer.0.intermediate.dense.weight', 'model.esm.encoder.layer.0.intermediate.dense.bias', 'model.esm.encoder.layer.0.output.dense.weight', 'model.esm.encoder.layer.0.output.dense.bias', 'model.esm.encoder.layer.0.LayerNorm.weight', 'model.esm.encoder.layer.0.LayerNorm.bias', 'model.esm.encoder.layer.1.attention.self.query.weight', 'model.esm.encoder.layer.1.attention.self.query.bias', 'model.esm.encoder.layer.1.attention.self.key.weight', 'model.esm.encoder.layer.1.attention.self.key.bias', 'model.esm.encoder.layer.1.attention.self.value.weight', 'model.esm.encoder.layer.1.attention.self.value.bias', 'model.esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq']\n",
            "\n",
            "Top-level prefixes:\n",
            "  model                        213\n",
            "  out                          2\n",
            "\n",
            "Two-token prefixes:\n",
            "  model.esm                    207\n",
            "  model.lm_head                6\n",
            "  out.weight                   1\n",
            "  out.bias                     1\n",
            "\n",
            "== StructureEncoder expected keys (target) ==\n",
            "Total keys: 215\n",
            "Sample keys: ['model.esm.embeddings.word_embeddings.weight', 'model.esm.encoder.layer.0.attention.self.query.weight', 'model.esm.encoder.layer.0.attention.self.query.bias', 'model.esm.encoder.layer.0.attention.self.key.weight', 'model.esm.encoder.layer.0.attention.self.key.bias', 'model.esm.encoder.layer.0.attention.self.value.weight', 'model.esm.encoder.layer.0.attention.self.value.bias', 'model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'model.esm.encoder.layer.0.attention.output.dense.weight', 'model.esm.encoder.layer.0.attention.output.dense.bias', 'model.esm.encoder.layer.0.attention.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.LayerNorm.bias', 'model.esm.encoder.layer.0.intermediate.dense.weight', 'model.esm.encoder.layer.0.intermediate.dense.bias', 'model.esm.encoder.layer.0.output.dense.weight', 'model.esm.encoder.layer.0.output.dense.bias', 'model.esm.encoder.layer.0.LayerNorm.weight', 'model.esm.encoder.layer.0.LayerNorm.bias', 'model.esm.encoder.layer.1.attention.self.query.weight', 'model.esm.encoder.layer.1.attention.self.query.bias', 'model.esm.encoder.layer.1.attention.self.key.weight', 'model.esm.encoder.layer.1.attention.self.key.bias', 'model.esm.encoder.layer.1.attention.self.value.weight', 'model.esm.encoder.layer.1.attention.self.value.bias', 'model.esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq']\n",
            "\n",
            "Top-level prefixes:\n",
            "  model                        213\n",
            "  out                          2\n",
            "\n",
            "Two-token prefixes:\n",
            "  model.esm                    207\n",
            "  model.lm_head                6\n",
            "  out.weight                   1\n",
            "  out.bias                     1\n",
            "\n",
            "== Checkpoint keys found in: ProTrek_35M.pt ==\n",
            "Total keys: 633\n",
            "Sample keys: ['0', '1.model.esm.embeddings.position_ids', '1.model.esm.embeddings.word_embeddings.weight', '1.model.esm.encoder.layer.0.attention.self.query.weight', '1.model.esm.encoder.layer.0.attention.self.query.bias', '1.model.esm.encoder.layer.0.attention.self.key.weight', '1.model.esm.encoder.layer.0.attention.self.key.bias', '1.model.esm.encoder.layer.0.attention.self.value.weight', '1.model.esm.encoder.layer.0.attention.self.value.bias', '1.model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', '1.model.esm.encoder.layer.0.attention.output.dense.weight', '1.model.esm.encoder.layer.0.attention.output.dense.bias', '1.model.esm.encoder.layer.0.attention.LayerNorm.weight', '1.model.esm.encoder.layer.0.attention.LayerNorm.bias', '1.model.esm.encoder.layer.0.intermediate.dense.weight', '1.model.esm.encoder.layer.0.intermediate.dense.bias', '1.model.esm.encoder.layer.0.output.dense.weight', '1.model.esm.encoder.layer.0.output.dense.bias', '1.model.esm.encoder.layer.0.LayerNorm.weight', '1.model.esm.encoder.layer.0.LayerNorm.bias', '1.model.esm.encoder.layer.1.attention.self.query.weight', '1.model.esm.encoder.layer.1.attention.self.query.bias', '1.model.esm.encoder.layer.1.attention.self.key.weight', '1.model.esm.encoder.layer.1.attention.self.key.bias', '1.model.esm.encoder.layer.1.attention.self.value.weight']\n",
            "\n",
            "Top-level prefixes:\n",
            "  1                            216\n",
            "  3                            216\n",
            "  2                            200\n",
            "  0                            1\n",
            "\n",
            "Two-token prefixes:\n",
            "  1.model                      214\n",
            "  3.model                      214\n",
            "  2.model                      198\n",
            "  1.out                        2\n",
            "  2.out                        2\n",
            "  3.out                        2\n",
            "  0                            1\n",
            "\n",
            "Protein slice prefix chosen: ''  (#tensors=633)\n",
            "Structure slice prefix chosen: '' (#tensors=633)\n",
            "\n",
            "Protein sub-keys sample: ['0', '1.model.esm.embeddings.position_ids', '1.model.esm.embeddings.word_embeddings.weight', '1.model.esm.encoder.layer.0.attention.self.query.weight', '1.model.esm.encoder.layer.0.attention.self.query.bias', '1.model.esm.encoder.layer.0.attention.self.key.weight', '1.model.esm.encoder.layer.0.attention.self.key.bias', '1.model.esm.encoder.layer.0.attention.self.value.weight', '1.model.esm.encoder.layer.0.attention.self.value.bias', '1.model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', '1.model.esm.encoder.layer.0.attention.output.dense.weight', '1.model.esm.encoder.layer.0.attention.output.dense.bias', '1.model.esm.encoder.layer.0.attention.LayerNorm.weight', '1.model.esm.encoder.layer.0.attention.LayerNorm.bias', '1.model.esm.encoder.layer.0.intermediate.dense.weight', '1.model.esm.encoder.layer.0.intermediate.dense.bias', '1.model.esm.encoder.layer.0.output.dense.weight', '1.model.esm.encoder.layer.0.output.dense.bias', '1.model.esm.encoder.layer.0.LayerNorm.weight', '1.model.esm.encoder.layer.0.LayerNorm.bias']\n",
            "Structure sub-keys sample: ['0', '1.model.esm.embeddings.position_ids', '1.model.esm.embeddings.word_embeddings.weight', '1.model.esm.encoder.layer.0.attention.self.query.weight', '1.model.esm.encoder.layer.0.attention.self.query.bias', '1.model.esm.encoder.layer.0.attention.self.key.weight', '1.model.esm.encoder.layer.0.attention.self.key.bias', '1.model.esm.encoder.layer.0.attention.self.value.weight', '1.model.esm.encoder.layer.0.attention.self.value.bias', '1.model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', '1.model.esm.encoder.layer.0.attention.output.dense.weight', '1.model.esm.encoder.layer.0.attention.output.dense.bias', '1.model.esm.encoder.layer.0.attention.LayerNorm.weight', '1.model.esm.encoder.layer.0.attention.LayerNorm.bias', '1.model.esm.encoder.layer.0.intermediate.dense.weight', '1.model.esm.encoder.layer.0.intermediate.dense.bias', '1.model.esm.encoder.layer.0.output.dense.weight', '1.model.esm.encoder.layer.0.output.dense.bias', '1.model.esm.encoder.layer.0.LayerNorm.weight', '1.model.esm.encoder.layer.0.LayerNorm.bias']\n",
            "\n",
            "Best mapping for PROTEIN: as_is  (hits=0/633)\n",
            "Best mapping for STRUCT  : as_is  (hits=0/633)\n",
            "[ProteinEncoder] loaded_tensors=633  missing=215  unexpected=633\n",
            "  • missing (first 12):    ['model.esm.embeddings.word_embeddings.weight', 'model.esm.encoder.layer.0.attention.self.query.weight', 'model.esm.encoder.layer.0.attention.self.query.bias', 'model.esm.encoder.layer.0.attention.self.key.weight', 'model.esm.encoder.layer.0.attention.self.key.bias', 'model.esm.encoder.layer.0.attention.self.value.weight', 'model.esm.encoder.layer.0.attention.self.value.bias', 'model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'model.esm.encoder.layer.0.attention.output.dense.weight', 'model.esm.encoder.layer.0.attention.output.dense.bias', 'model.esm.encoder.layer.0.attention.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.LayerNorm.bias']\n",
            "  • unexpected (first 12): ['0', '1.model.esm.embeddings.position_ids', '1.model.esm.embeddings.word_embeddings.weight', '1.model.esm.encoder.layer.0.attention.self.query.weight', '1.model.esm.encoder.layer.0.attention.self.query.bias', '1.model.esm.encoder.layer.0.attention.self.key.weight', '1.model.esm.encoder.layer.0.attention.self.key.bias', '1.model.esm.encoder.layer.0.attention.self.value.weight', '1.model.esm.encoder.layer.0.attention.self.value.bias', '1.model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', '1.model.esm.encoder.layer.0.attention.output.dense.weight', '1.model.esm.encoder.layer.0.attention.output.dense.bias']\n",
            "[StructureEncoder] loaded_tensors=633  missing=215  unexpected=633\n",
            "  • missing (first 12):    ['model.esm.embeddings.word_embeddings.weight', 'model.esm.encoder.layer.0.attention.self.query.weight', 'model.esm.encoder.layer.0.attention.self.query.bias', 'model.esm.encoder.layer.0.attention.self.key.weight', 'model.esm.encoder.layer.0.attention.self.key.bias', 'model.esm.encoder.layer.0.attention.self.value.weight', 'model.esm.encoder.layer.0.attention.self.value.bias', 'model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'model.esm.encoder.layer.0.attention.output.dense.weight', 'model.esm.encoder.layer.0.attention.output.dense.bias', 'model.esm.encoder.layer.0.attention.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.LayerNorm.bias']\n",
            "  • unexpected (first 12): ['0', '1.model.esm.embeddings.position_ids', '1.model.esm.embeddings.word_embeddings.weight', '1.model.esm.encoder.layer.0.attention.self.query.weight', '1.model.esm.encoder.layer.0.attention.self.query.bias', '1.model.esm.encoder.layer.0.attention.self.key.weight', '1.model.esm.encoder.layer.0.attention.self.key.bias', '1.model.esm.encoder.layer.0.attention.self.value.weight', '1.model.esm.encoder.layer.0.attention.self.value.bias', '1.model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', '1.model.esm.encoder.layer.0.attention.output.dense.weight', '1.model.esm.encoder.layer.0.attention.output.dense.bias']\n",
            "\n",
            "Cosine(protein random vs loaded)  : 0.017522500827908516\n",
            "Cosine(struct  random vs loaded)  : -0.014480819925665855\n"
          ]
        }
      ],
      "source": [
        "# ===== Check what ESM expects vs. what the checkpoint provides (and optionally load) =====\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import torch, re, importlib\n",
        "\n",
        "# --- config paths (already set above normally) ---\n",
        "# PROTEIN_CONFIG, STRUCTURE_CONFIG, CKPT\n",
        "\n",
        "# --- (re)import encoders to be safe ---\n",
        "import protein_encoder as protein_encoder_mod\n",
        "import structure_encoder as structure_encoder_mod\n",
        "importlib.reload(protein_encoder_mod)\n",
        "importlib.reload(structure_encoder_mod)\n",
        "ProteinEncoder   = protein_encoder_mod.ProteinEncoder\n",
        "StructureEncoder = structure_encoder_mod.StructureEncoder\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Build fresh (random) encoders to inspect what keys THEY expect\n",
        "prot = ProteinEncoder(str(PROTEIN_CONFIG), out_dim=1024, load_pretrained=False).to(DEVICE).eval()\n",
        "stru = StructureEncoder(str(STRUCTURE_CONFIG), out_dim=1024, load_pretrained=False).to(DEVICE).eval()\n",
        "\n",
        "prot_keys = list(prot.state_dict().keys())\n",
        "stru_keys = list(stru.state_dict().keys())\n",
        "\n",
        "def prefix_hist(keys, title=\"\"):\n",
        "    print(f\"\\n== {title} ==\")\n",
        "    print(f\"Total keys: {len(keys)}\")\n",
        "    print(\"Sample keys:\", keys[:25])\n",
        "    def head1(k): return k.split(\".\", 1)[0]\n",
        "    def head2(k):\n",
        "        parts = k.split(\".\")\n",
        "        return \".\".join(parts[:2]) if len(parts)>=2 else parts[0]\n",
        "    c1 = Counter(head1(k) for k in keys)\n",
        "    c2 = Counter(head2(k) for k in keys)\n",
        "    print(\"\\nTop-level prefixes:\")\n",
        "    for k,v in c1.most_common(12): print(f\"  {k:28s} {v}\")\n",
        "    print(\"\\nTwo-token prefixes:\")\n",
        "    for k,v in c2.most_common(12): print(f\"  {k:28s} {v}\")\n",
        "\n",
        "prefix_hist(prot_keys, \"ProteinEncoder expected keys (target)\")\n",
        "prefix_hist(stru_keys, \"StructureEncoder expected keys (target)\")\n",
        "\n",
        "# ---- Load checkpoint and locate the real state dict inside it ----\n",
        "CKPT = Path(CKPT)\n",
        "raw = torch.load(str(CKPT), map_location=\"cpu\")\n",
        "\n",
        "def locate_state_dict(obj, max_depth=3):\n",
        "    if isinstance(obj, dict):\n",
        "        # common containers\n",
        "        for k in (\"state_dict\",\"model_state_dict\",\"weights\",\"params\",\"model\"):\n",
        "            if k in obj and isinstance(obj[k], dict) and any(torch.is_tensor(v) for v in obj[k].values()):\n",
        "                return obj[k]\n",
        "        if any(torch.is_tensor(v) for v in obj.values()):\n",
        "            return obj\n",
        "        if max_depth > 0:\n",
        "            for v in obj.values():\n",
        "                if isinstance(v, dict):\n",
        "                    sd = locate_state_dict(v, max_depth=max_depth-1)\n",
        "                    if sd is not None: return sd\n",
        "    return None\n",
        "\n",
        "sd0 = locate_state_dict(raw) if isinstance(raw, dict) else raw\n",
        "if sd0 is None:\n",
        "    print(\"\\n❌ Could not locate a tensor state_dict in checkpoint; top-level keys:\",\n",
        "          list(raw.keys())[:20] if isinstance(raw, dict) else type(raw))\n",
        "    raise SystemExit\n",
        "\n",
        "ckpt_keys = list(sd0.keys())\n",
        "prefix_hist(ckpt_keys, f\"Checkpoint keys found in: {CKPT.name}\")\n",
        "\n",
        "# ---- Try to slice protein/structure parts by plausible prefixes ----\n",
        "def strip_prefix(d, prefix):\n",
        "    if prefix and not any(k.startswith(prefix) for k in d): return d\n",
        "    return {(k[len(prefix):] if k.startswith(prefix) else k): v for k,v in d.items()}\n",
        "\n",
        "def extract_slice(sd, candidates):\n",
        "    sd = strip_prefix(strip_prefix(sd, \"model.\"), \"module.\")\n",
        "    best = {}\n",
        "    best_pref = None\n",
        "    for pref in candidates:\n",
        "        sub = {(k[len(pref):] if k.startswith(pref) else k): v\n",
        "               for k,v in sd.items() if (pref==\"\" or k.startswith(pref))}\n",
        "        if len(sub) > len(best):\n",
        "            best, best_pref = sub, pref\n",
        "    return best_pref, best\n",
        "\n",
        "# widen if needed after you see the histogram\n",
        "prot_prefix_candidates = (\n",
        "    \"protein_encoder.\", \"module.protein_encoder.\", \"model.protein_encoder.\",\n",
        "    \"protein.\", \"seq_encoder.\", \"sequence_encoder.\",  # extra guesses\n",
        "    \"\"\n",
        ")\n",
        "stru_prefix_candidates = (\n",
        "    \"structure_encoder.\", \"module.structure_encoder.\", \"model.structure_encoder.\",\n",
        "    \"structure.\", \"struct_encoder.\", \"foldseek_encoder.\",                    # extra guesses\n",
        "    \"\"\n",
        ")\n",
        "\n",
        "prot_pref, prot_sub = extract_slice(sd0, prot_prefix_candidates)\n",
        "stru_pref, stru_sub = extract_slice(sd0, stru_prefix_candidates)\n",
        "\n",
        "print(f\"\\nProtein slice prefix chosen: {repr(prot_pref)}  (#tensors={len(prot_sub)})\")\n",
        "print(f\"Structure slice prefix chosen: {repr(stru_pref)} (#tensors={len(stru_sub)})\")\n",
        "\n",
        "# ---- Show a few sub-keys (post-slicing) ----\n",
        "print(\"\\nProtein sub-keys sample:\", list(prot_sub.keys())[:20])\n",
        "print(\"Structure sub-keys sample:\", list(stru_sub.keys())[:20])\n",
        "\n",
        "# ---- Build mapping functions and pick the one with most hits ----\n",
        "def choose_mapping(sub_keys, target_keys):\n",
        "    # conservative + esm-aware mappers\n",
        "    def as_is(k): return k\n",
        "    def add_model(k):\n",
        "        if k.startswith((\"model.\",\"out.\")): return k\n",
        "        return \"model.\"+k\n",
        "    def esm_to_model_esm(k):\n",
        "        if k.startswith((\"model.\",\"out.\")): return k\n",
        "        # common ESM bits to sit under model.esm.*\n",
        "        head = k.split(\".\",1)[0]\n",
        "        if head in (\"esm\",\"encoder\",\"embeddings\",\"contact_head\"):\n",
        "            return \"model.esm.\"+k if not k.startswith(\"esm.\") else \"model.\"+k   # handle \"esm.*\" already complete\n",
        "        if head in (\"lm_head\",):\n",
        "            return \"model.\"+k\n",
        "        return \"model.\"+k\n",
        "\n",
        "    candidates = {\n",
        "        \"as_is\": as_is,\n",
        "        \"add_model\": add_model,\n",
        "        \"esm_to_model_esm\": esm_to_model_esm,\n",
        "    }\n",
        "    best_name, best_fn, best_hits = \"as_is\", as_is, 0\n",
        "    tgt = set(target_keys)\n",
        "    for name, fn in candidates.items():\n",
        "        hits = sum(1 for k in sub_keys if fn(k) in tgt)\n",
        "        if hits > best_hits:\n",
        "            best_name, best_fn, best_hits = name, fn, hits\n",
        "    return best_name, best_fn, best_hits\n",
        "\n",
        "prot_map_name, prot_map_fn, prot_hits = choose_mapping(set(prot_sub.keys()), prot_keys)\n",
        "stru_map_name, stru_map_fn, stru_hits = choose_mapping(set(stru_sub.keys()), stru_keys)\n",
        "\n",
        "print(f\"\\nBest mapping for PROTEIN: {prot_map_name}  (hits={prot_hits}/{len(prot_sub)})\")\n",
        "print(f\"Best mapping for STRUCT  : {stru_map_name}  (hits={stru_hits}/{len(stru_sub)})\")\n",
        "\n",
        "# ---- Optionally apply (dry-run or real) ----\n",
        "DO_LOAD = True  # set False to only analyze keys without loading\n",
        "\n",
        "def remap_and_load(module, sub, map_fn, tag):\n",
        "    remapped = {map_fn(k): v for k,v in sub.items()}\n",
        "    missing, unexpected = module.load_state_dict(remapped, strict=False)\n",
        "    print(f\"[{tag}] loaded_tensors={len(remapped)}  missing={len(missing)}  unexpected={len(unexpected)}\")\n",
        "    if missing:    print(\"  • missing (first 12):   \", list(missing)[:12])\n",
        "    if unexpected: print(\"  • unexpected (first 12):\", list(unexpected)[:12])\n",
        "\n",
        "if DO_LOAD:\n",
        "    remap_and_load(prot, prot_sub, prot_map_fn, \"ProteinEncoder\")\n",
        "    remap_and_load(stru, stru_sub, stru_map_fn, \"StructureEncoder\")\n",
        "\n",
        "# ---- Concrete proof: compare outputs before/after on same inputs ----\n",
        "@torch.no_grad()\n",
        "def cosine_sim(a, b):\n",
        "    a = torch.nn.functional.normalize(a, dim=-1)\n",
        "    b = torch.nn.functional.normalize(b, dim=-1)\n",
        "    return float((a*b).sum(dim=-1).mean().cpu())\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_prot(prot_model):\n",
        "    seqs = [\"MKTFFVAIATGAFSATA\", \"MGDVEKGKKIFIMKCSQCHTVEK\"]\n",
        "    return prot_model.get_repr(seqs, batch_size=2, verbose=False).to(\"cpu\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_stru(stru_model):\n",
        "    seqs = [\"acdefghiklmnpqrstvwy\", \"acdefghi\"]\n",
        "    return stru_model.get_repr(seqs, batch_size=2, verbose=False).to(\"cpu\")\n",
        "\n",
        "# fresh random baseline encoders\n",
        "prot_rand = ProteinEncoder(str(PROTEIN_CONFIG), out_dim=1024, load_pretrained=False).to(DEVICE).eval()\n",
        "stru_rand = StructureEncoder(str(STRUCTURE_CONFIG), out_dim=1024, load_pretrained=False).to(DEVICE).eval()\n",
        "\n",
        "prot_before, prot_after = get_prot(prot_rand), get_prot(prot)\n",
        "stru_before, stru_after = get_stru(stru_rand), get_stru(stru)\n",
        "\n",
        "print(\"\\nCosine(protein random vs loaded)  :\", cosine_sim(prot_before, prot_after))\n",
        "print(\"Cosine(struct  random vs loaded)  :\", cosine_sim(stru_before, stru_after))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0b5ba38",
      "metadata": {
        "id": "e0b5ba38"
      },
      "source": [
        "\n",
        "## 3) Quick Configs (Edit Me)\n",
        "Set your small model variants here. You can pass either a **local path** to a config/checkpoint or a **Hugging Face ID**.\n",
        "For first tests, pick small backbones to speed things up (e.g., `Qwen/Qwen2.5-0.5B-Instruct`, `facebook/esm2_t12_35M_UR50D`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fd2cdf79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd2cdf79",
        "outputId": "36c92ae1-645d-4fa5-8a78-8161552fe24c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === LLM & Encoders ===\n",
        "MODEL_NAME         = \"Qwen/Qwen2.5-0.5B-Instruct\"   # Small-ish for Colab testing\n",
        "PROTEIN_CONFIG     = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\"  # or local path to a config/checkpoint\n",
        "STRUCTURE_CONFIG   = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\"  # for structure encoder (uses same ESM family for demo)\n",
        "\n",
        "# === Prefix/Proj ===\n",
        "SINGLE_TOKEN_PREFIX = False     # True -> 1 token; False -> soft prefix of length PREFIX_LEN\n",
        "PREFIX_LEN          = 4\n",
        "PROJ_HID            = 1024\n",
        "DROPOUT             = 0.10\n",
        "\n",
        "# === Training toggles ===\n",
        "USE_LORA            = False\n",
        "TRAIN_ENCODERS      = False    # True = end-to-end; False = freeze encoders\n",
        "FREEZE_PROTEIN      = False    # only used if TRAIN_ENCODERS=True\n",
        "FREEZE_STRUCTURE    = False    # only used if TRAIN_ENCODERS=True\n",
        "GRAD_CHECKPOINT     = False\n",
        "\n",
        "# === Misc ===\n",
        "DEVICE              = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_LEN             = 512\n",
        "BSZ                 = 2\n",
        "ACCUM               = 1\n",
        "LR                  = 5e-5\n",
        "WARMUP_RATIO        = 0.03\n",
        "EPOCHS              = 1\n",
        "OUTPUT_DIR          = \"runs/colab_smoketest\"\n",
        "LOG_EVERY           = 1\n",
        "\n",
        "print(\"Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d160eee8",
      "metadata": {
        "id": "d160eee8"
      },
      "source": [
        "\n",
        "## 4) Minimal JSONL Toy Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "q8zowffP6Q1Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8zowffP6Q1Y",
        "outputId": "d17b237a-2e98-4503-d238-b6445277e0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/LLM/Bioreasoner/data//hf/proteinDT/sft_jsonl/sft_test_sample.jsonl OK\n",
            "/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D OK\n",
            "/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M OK\n",
            "/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt OK\n"
          ]
        }
      ],
      "source": [
        "# --- PATHS: edit these three if needed ---\n",
        "SFT_JSONL = \"/content/drive/MyDrive/LLM/Bioreasoner/data//hf/proteinDT/sft_jsonl/sft_test_sample.jsonl\"  # copy your toy file here\n",
        "PROTEIN_CONFIG   = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\"\n",
        "STRUCTURE_CONFIG = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\"\n",
        "CKPT             = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\"  # ProTrek .pt\n",
        "\n",
        "from pathlib import Path\n",
        "for p in [SFT_JSONL, PROTEIN_CONFIG, STRUCTURE_CONFIG, CKPT]:\n",
        "    print(str(p), \"OK\" if Path(p).exists() else \"MISSING\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "eo7ho9eo63HV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo7ho9eo63HV",
        "outputId": "f03abc07-571d-4109-b564-ffede93c38fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔ valid lines: 5\n",
            " avg prompt len: 184 chars | avg response len: 259 chars\n",
            "No schema/character issues found.\n"
          ]
        }
      ],
      "source": [
        "import json, re\n",
        "from statistics import mean\n",
        "\n",
        "AA_RE  = re.compile(r\"^[ACDEFGHIKLMNPQRSTVWY]+$\")   # uppercase 20 AA\n",
        "FS_RE  = re.compile(r\"^[a-z]+$\")                    # loose: lowercase letters\n",
        "\n",
        "def validate_jsonl(path, max_show=10):\n",
        "    ok, issues = 0, []\n",
        "    lens_prompt, lens_response = [], []\n",
        "    with open(path, \"r\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "            except Exception as e:\n",
        "                issues.append((i, f\"bad json: {e}\")); continue\n",
        "\n",
        "            # required\n",
        "            if \"prompt\" not in obj or \"response\" not in obj:\n",
        "                issues.append((i, \"missing prompt/response\")); continue\n",
        "            if not isinstance(obj[\"prompt\"], str) or not isinstance(obj[\"response\"], str):\n",
        "                issues.append((i, \"prompt/response not string\")); continue\n",
        "\n",
        "            # optional aa_seq / stru_str\n",
        "            if \"aa_seq\" in obj and obj[\"aa_seq\"] is not None:\n",
        "                if not AA_RE.match(obj[\"aa_seq\"]):\n",
        "                    issues.append((i, \"aa_seq contains non-standard chars\"))\n",
        "            if \"stru_str\" in obj and obj[\"stru_str\"] is not None:\n",
        "                if not FS_RE.match(obj[\"stru_str\"]):\n",
        "                    issues.append((i, \"stru_str must be lowercase alpha (3Di-like)\"))\n",
        "\n",
        "            lens_prompt.append(len(obj[\"prompt\"]))\n",
        "            lens_response.append(len(obj[\"response\"]))\n",
        "            ok += 1\n",
        "\n",
        "    print(f\"✔ valid lines: {ok}\")\n",
        "    if lens_prompt and lens_response:\n",
        "        print(f\" avg prompt len: {int(mean(lens_prompt))} chars | avg response len: {int(mean(lens_response))} chars\")\n",
        "    if issues:\n",
        "        print(f\"⚠ found {len(issues)} issues (showing first {max_show}):\")\n",
        "        for row, msg in issues[:max_show]:\n",
        "            print(f\"  line {row}: {msg}\")\n",
        "    else:\n",
        "        print(\"No schema/character issues found.\")\n",
        "\n",
        "validate_jsonl(SFT_JSONL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "732e12b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "732e12b6",
        "outputId": "652af818-479e-4c6b-d1a3-195fe318f943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Available slots in checkpoint: [0, 1, 2, 3]\n",
            "\n",
            "[ProteinEncoder] slot loaded: matched=215  missing=0  unexpected=0\n",
            "  matched (first 20): ['model.esm.embeddings.word_embeddings.weight', 'model.esm.encoder.emb_layer_norm_after.bias', 'model.esm.encoder.emb_layer_norm_after.weight', 'model.esm.encoder.layer.0.LayerNorm.bias', 'model.esm.encoder.layer.0.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.LayerNorm.bias', 'model.esm.encoder.layer.0.attention.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.output.dense.bias', 'model.esm.encoder.layer.0.attention.output.dense.weight', 'model.esm.encoder.layer.0.attention.self.key.bias', 'model.esm.encoder.layer.0.attention.self.key.weight', 'model.esm.encoder.layer.0.attention.self.query.bias', 'model.esm.encoder.layer.0.attention.self.query.weight', 'model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'model.esm.encoder.layer.0.attention.self.value.bias', 'model.esm.encoder.layer.0.attention.self.value.weight', 'model.esm.encoder.layer.0.intermediate.dense.bias', 'model.esm.encoder.layer.0.intermediate.dense.weight', 'model.esm.encoder.layer.0.output.dense.bias', 'model.esm.encoder.layer.0.output.dense.weight']\n",
            "\n",
            "[StructureEncoder] slot loaded: matched=215  missing=0  unexpected=0\n",
            "  matched (first 20): ['model.esm.embeddings.word_embeddings.weight', 'model.esm.encoder.emb_layer_norm_after.bias', 'model.esm.encoder.emb_layer_norm_after.weight', 'model.esm.encoder.layer.0.LayerNorm.bias', 'model.esm.encoder.layer.0.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.LayerNorm.bias', 'model.esm.encoder.layer.0.attention.LayerNorm.weight', 'model.esm.encoder.layer.0.attention.output.dense.bias', 'model.esm.encoder.layer.0.attention.output.dense.weight', 'model.esm.encoder.layer.0.attention.self.key.bias', 'model.esm.encoder.layer.0.attention.self.key.weight', 'model.esm.encoder.layer.0.attention.self.query.bias', 'model.esm.encoder.layer.0.attention.self.query.weight', 'model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'model.esm.encoder.layer.0.attention.self.value.bias', 'model.esm.encoder.layer.0.attention.self.value.weight', 'model.esm.encoder.layer.0.intermediate.dense.bias', 'model.esm.encoder.layer.0.intermediate.dense.weight', 'model.esm.encoder.layer.0.output.dense.bias', 'model.esm.encoder.layer.0.output.dense.weight']\n",
            "\n",
            "Embeddings:\n",
            "  Protein vecs: (5, 1024)\n",
            "  Struct  vecs: (5, 1024)\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 3 (hard-coded slots: protein=1, structure=3) ---\n",
        "import importlib, json, torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Import your encoders\n",
        "import protein_encoder as protein_encoder_mod\n",
        "import structure_encoder as structure_encoder_mod\n",
        "importlib.reload(protein_encoder_mod)\n",
        "importlib.reload(structure_encoder_mod)\n",
        "ProteinEncoder   = protein_encoder_mod.ProteinEncoder\n",
        "StructureEncoder = structure_encoder_mod.StructureEncoder\n",
        "\n",
        "# Build encoders from configs ONLY (random init)\n",
        "prot_enc = ProteinEncoder(str(PROTEIN_CONFIG), out_dim=1024, load_pretrained=False).to(DEVICE).eval()\n",
        "stru_enc = StructureEncoder(str(STRUCTURE_CONFIG), out_dim=1024, load_pretrained=False).to(DEVICE).eval()\n",
        "\n",
        "# Load checkpoint and slice by ParameterList slots\n",
        "raw = torch.load(str(CKPT), map_location=\"cpu\")\n",
        "sd  = raw[\"model\"] if (isinstance(raw, dict) and \"model\" in raw) else (\n",
        "      raw.get(\"state_dict\", raw) if isinstance(raw, dict) else raw)\n",
        "\n",
        "# Build slot dict: {slot_index: {param_key_without_prefix: tensor}}\n",
        "slots = {}\n",
        "for k, v in sd.items():\n",
        "    head = k.split(\".\", 1)[0]\n",
        "    if head.isdigit():\n",
        "        slots.setdefault(int(head), {})[k[len(head)+1:]] = v\n",
        "\n",
        "# Hard-code slots (adjust if your checkpoint order differs)\n",
        "PROT_SLOT = 1   # protein encoder lives here in your .pt\n",
        "STRU_SLOT = 3   # structure encoder lives here in your .pt\n",
        "print(\"Available slots in checkpoint:\", sorted(slots.keys()))\n",
        "\n",
        "if PROT_SLOT not in slots or STRU_SLOT not in slots:\n",
        "    raise KeyError(\n",
        "        f\"Hard-coded slots not found in checkpoint. \"\n",
        "        f\"Have slots={sorted(slots.keys())}, need PROT_SLOT={PROT_SLOT}, STRU_SLOT={STRU_SLOT}.\"\n",
        "        # If needed, you can re-enable the tiny auto-finder:\n",
        "        # def best_slot_for(module):\n",
        "        #     tgt = set(module.state_dict().keys())\n",
        "        #     return max(slots.items(), key=lambda kv: sum(1 for k in kv[1] if k in tgt))[0]\n",
        "        # PROT_SLOT = best_slot_for(prot_enc); STRU_SLOT = best_slot_for(stru_enc)\n",
        "    )\n",
        "\n",
        "# Optionally drop harmless extras (e.g., position_ids when using RoPE)\n",
        "def drop_extras(sd_sub: dict):\n",
        "    bad = [k for k in sd_sub if \"embeddings.position_ids\" in k]\n",
        "    for k in bad: sd_sub.pop(k)\n",
        "    return sd_sub\n",
        "\n",
        "prot_sub = drop_extras(dict(slots[PROT_SLOT]))\n",
        "stru_sub = drop_extras(dict(slots[STRU_SLOT]))\n",
        "\n",
        "# Report matches, then load with strict=False\n",
        "def report_and_load(module, sub, tag):\n",
        "    tgt = set(module.state_dict().keys())\n",
        "    matched_keys = sorted([k for k in sub if k in tgt])\n",
        "    missing, unexpected = module.load_state_dict(sub, strict=False)\n",
        "    print(f\"\\n[{tag}] slot loaded: matched={len(matched_keys)}  missing={len(missing)}  unexpected={len(unexpected)}\")\n",
        "    print(\"  matched (first 20):\", matched_keys[:20])\n",
        "    if missing:    print(\"  missing (first 12):   \", list(missing)[:12])\n",
        "    if unexpected: print(\"  unexpected (first 12):\", list(unexpected)[:12])\n",
        "\n",
        "report_and_load(prot_enc, prot_sub, \"ProteinEncoder\")\n",
        "report_and_load(stru_enc, stru_sub, \"StructureEncoder\")\n",
        "\n",
        "# Quick forward on toy JSONL (ensure 'toy' exists; otherwise read it)\n",
        "try:\n",
        "    toy\n",
        "except NameError:\n",
        "    toy = [json.loads(x) for x in open(SFT_JSONL)]\n",
        "\n",
        "aa_list   = [ex.get(\"aa_seq\") or \"\"    for ex in toy]\n",
        "stru_list = [ex.get(\"stru_str\") or \"\"  for ex in toy]\n",
        "\n",
        "with torch.no_grad():\n",
        "    prot_vecs = prot_enc.get_repr([s for s in aa_list if s],   batch_size=8, verbose=False) if any(aa_list) else None\n",
        "    stru_vecs = stru_enc.get_repr([s for s in stru_list if s], batch_size=8, verbose=False) if any(stru_list) else None\n",
        "\n",
        "print(\"\\nEmbeddings:\")\n",
        "print(\"  Protein vecs:\", None if prot_vecs is None else tuple(prot_vecs.shape))\n",
        "print(\"  Struct  vecs:\", None if stru_vecs is None else tuple(stru_vecs.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "zrBidTFM-G9B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrBidTFM-G9B",
        "outputId": "bbe8ba47-5bde-4136-a9ea-29e5110ec976"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM hidden size: 896\n",
            "Pad token id: 151643 | EOS id: 151645\n",
            "LLM logits: (1, 97, 151936)\n",
            "LLM last hidden: (1, 97, 896)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "# Tip: prefer dtype= over torch_dtype= (newer API wording)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    dtype=torch.float32#(torch.float32 if DEVICE == \"cuda\" else torch.float32)\n",
        ").to(DEVICE).eval()\n",
        "\n",
        "print(\"LLM hidden size:\", llm.config.hidden_size)\n",
        "print(\"Pad token id:\", tok.pad_token_id, \"| EOS id:\", tok.eos_token_id)\n",
        "\n",
        "# Smoke test: plain forward on text\n",
        "batch = tok([toy[0][\"prompt\"] + \"\\n\\n\" + toy[0][\"response\"]],\n",
        "            return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = llm(**batch, return_dict=True)  # default: no hidden states\n",
        "print(\"LLM logits:\", tuple(out.logits.shape))  # (B, T, vocab)\n",
        "\n",
        "# If you ALSO want last hidden states:\n",
        "with torch.no_grad():\n",
        "    out_h = llm(**batch, output_hidden_states=True, return_dict=True)\n",
        "print(\"LLM last hidden:\", tuple(out_h.hidden_states[-1].shape))  # (B, T, H)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZL6n4nas-5NX",
      "metadata": {
        "id": "ZL6n4nas-5NX"
      },
      "source": [
        "Cell 5 — Projector + prefix injection (single-token prefix by default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ucy7Bvbl-5q9",
      "metadata": {
        "id": "ucy7Bvbl-5q9"
      },
      "outputs": [],
      "source": [
        "# ==== Projector + prefix injection with proper padding & dtype alignment ====\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "PREFIX_LEN = 1\n",
        "D_PROT, D_STRU = 1024, 1024\n",
        "D_IN = D_PROT + D_STRU\n",
        "D_HID = llm.config.hidden_size\n",
        "\n",
        "# NEW: capture the model's dtype (fp16 on GPU in your setup)\n",
        "MODEL_DTYPE = next(llm.parameters()).dtype  # e.g., torch.float16\n",
        "\n",
        "projector = nn.Sequential(\n",
        "    nn.Linear(D_IN, D_HID),\n",
        "    nn.SiLU(),\n",
        "    nn.Linear(D_HID, D_HID * PREFIX_LEN)\n",
        ").to(DEVICE, dtype=MODEL_DTYPE)  # NEW: ensure projector params are same dtype as LLM\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_protein_pair(aa_seq: str, stru_str: str):\n",
        "    if aa_seq:\n",
        "        prot = prot_enc.get_repr([aa_seq], batch_size=1, verbose=False)[0].to(DEVICE)\n",
        "    else:\n",
        "        prot = torch.zeros(D_PROT, device=DEVICE)\n",
        "    if stru_str:\n",
        "        stru = stru_enc.get_repr([stru_str], batch_size=1, verbose=False)[0].to(DEVICE)\n",
        "    else:\n",
        "        stru = torch.zeros(D_STRU, device=DEVICE)\n",
        "    return torch.cat([prot, stru], dim=-1)  # returns float32; we'll cast before projector\n",
        "\n",
        "def build_batch(examples, max_len=1024):\n",
        "    we = llm.get_input_embeddings()\n",
        "    pad_id = tok.pad_token_id\n",
        "    eos_id = tok.eos_token_id\n",
        "\n",
        "    enc_prompts   = tok([e[\"prompt\"] for e in examples], add_special_tokens=False)\n",
        "    enc_responses = tok([e[\"response\"] + tok.eos_token for e in examples], add_special_tokens=False)\n",
        "\n",
        "    text_token_ids, prompt_lens = [], []\n",
        "    for i in range(len(examples)):\n",
        "        ids_p = enc_prompts[\"input_ids\"][i]\n",
        "        ids_r = enc_responses[\"input_ids\"][i]\n",
        "        ids   = (ids_p + ids_r)[:max_len]\n",
        "        text_token_ids.append(ids)\n",
        "        prompt_lens.append(min(len(ids_p), len(ids)))\n",
        "\n",
        "    T_max = max(len(ids) for ids in text_token_ids) if text_token_ids else 0\n",
        "\n",
        "    inputs_embeds_list, attention_mask_list, labels_list = [], [], []\n",
        "\n",
        "    for i, ex in enumerate(examples):\n",
        "        ids = text_token_ids[i]\n",
        "        t_i = len(ids)\n",
        "\n",
        "        # Text embeds -> cast to model dtype\n",
        "        ids_tensor = torch.tensor(ids, device=DEVICE).unsqueeze(0)\n",
        "        text_embeds = we(ids_tensor).to(MODEL_DTYPE)  # NEW\n",
        "\n",
        "        # Labels: mask prompt, train on response\n",
        "        L = [-100]*prompt_lens[i] + ids[prompt_lens[i]:]\n",
        "\n",
        "        # Protein prefix: cast input to projector dtype before matmul\n",
        "        pvec = encode_protein_pair(ex.get(\"aa_seq\") or \"\", ex.get(\"stru_str\") or \"\")\n",
        "        pvec = pvec.to(MODEL_DTYPE)  # NEW: match projector/LLM dtype\n",
        "        pref = projector(pvec.unsqueeze(0)).view(1, PREFIX_LEN, D_HID)  # already MODEL_DTYPE\n",
        "\n",
        "        # Concat prefix + text (both MODEL_DTYPE)\n",
        "        combined = torch.cat([pref, text_embeds], dim=1)  # (1, P+t_i, H)\n",
        "        att = torch.tensor([1]*PREFIX_LEN + [1]*t_i, device=DEVICE, dtype=torch.long).unsqueeze(0)\n",
        "        lab = torch.tensor([-100]*PREFIX_LEN + L, device=DEVICE, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        # Pad to (P + T_max)\n",
        "        pad_steps = (PREFIX_LEN + T_max) - combined.size(1)\n",
        "        if pad_steps > 0:\n",
        "            pad_vec = we(torch.tensor([[pad_id]], device=DEVICE)).to(MODEL_DTYPE)  # NEW\n",
        "            pad_block = pad_vec.expand(1, pad_steps, D_HID)\n",
        "            combined = torch.cat([combined, pad_block], dim=1)\n",
        "            att = torch.cat([att, torch.zeros(1, pad_steps, device=DEVICE, dtype=torch.long)], dim=1)\n",
        "            lab = torch.cat([lab, torch.full((1, pad_steps), -100, device=DEVICE, dtype=torch.long)], dim=1)\n",
        "\n",
        "        inputs_embeds_list.append(combined)\n",
        "        attention_mask_list.append(att)\n",
        "        labels_list.append(lab)\n",
        "\n",
        "    inputs_embeds  = torch.cat(inputs_embeds_list, dim=0)                # (B, L, H)\n",
        "    attention_mask = torch.cat(attention_mask_list, dim=0)               # (B, L)\n",
        "    labels         = torch.cat(labels_list, dim=0)                       # (B, L)\n",
        "    # safety: ensure final dtype matches model\n",
        "    inputs_embeds  = inputs_embeds.to(MODEL_DTYPE)                       # NEW\n",
        "    return inputs_embeds, attention_mask, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fLeyhBT5GDJ0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLeyhBT5GDJ0",
        "outputId": "4f64a5c2-b8b1-43f1-ad18-ca2ba6c34960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs_embeds : (5, 99, 896)\n",
            "attention_mask: (5, 99)\n",
            "labels        : (5, 99)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Build a tiny batch (all toy examples)\n",
        "inputs_embeds, attention_mask, labels = build_batch(toy, max_len=256)\n",
        "print(\"inputs_embeds :\", tuple(inputs_embeds.shape))\n",
        "print(\"attention_mask:\", tuple(attention_mask.shape))\n",
        "print(\"labels        :\", tuple(labels.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "sMNkP0OmGMk5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMNkP0OmGMk5",
        "outputId": "b86d98a4-5523-47e8-87a0-1c870e2e06d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B=5, L=99, H=896\n",
            "sample 0: att_on=99  pad=0  ignored=38  supervised=61\n",
            "sample 1: att_on=95  pad=4  ignored=42  supervised=57\n",
            "sample 2: att_on=96  pad=3  ignored=41  supervised=58\n",
            "sample 3: att_on=92  pad=7  ignored=45  supervised=54\n",
            "sample 4: att_on=85  pad=14  ignored=52  supervised=47\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def inspect_batch(inputs_embeds, attention_mask, labels, prefix_len=1):\n",
        "    B, L, H = inputs_embeds.shape\n",
        "    print(f\"B={B}, L={L}, H={H}\")\n",
        "    for i in range(B):\n",
        "        att_on   = int(attention_mask[i].sum().item())\n",
        "        n_pad    = L - att_on\n",
        "        n_ign    = int((labels[i] == -100).sum().item())\n",
        "        n_sup    = L - n_ign  # supervised tokens\n",
        "        print(f\"sample {i}: att_on={att_on}  pad={n_pad}  ignored={n_ign}  supervised={n_sup}\")\n",
        "\n",
        "        # spot-check masking: prefix tokens ignored & attended\n",
        "        assert (labels[i, :prefix_len] == -100).all()\n",
        "        assert (attention_mask[i, :prefix_len] == 1).all()\n",
        "\n",
        "        # padded area (if any) should be att=0 and labels=-100\n",
        "        if att_on < L:\n",
        "            assert (attention_mask[i, att_on:] == 0).all()\n",
        "            assert (labels[i, att_on:] == -100).all()\n",
        "\n",
        "inspect_batch(inputs_embeds, attention_mask, labels, prefix_len=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DjnWpdjFGPdc",
      "metadata": {
        "id": "DjnWpdjFGPdc"
      },
      "source": [
        "inputs_embeds : (5, 99, 896)\n",
        "\n",
        "attention_mask: (5, 99)\n",
        "\n",
        "labels        : (5, 99)\n",
        "\n",
        "B=5, L=99, H=896\n",
        "\n",
        "sample 0: att_on=99  pad=0  ignored=38  supervised=61\n",
        "\n",
        "sample 1: att_on=95  pad=4  ignored=42  supervised=57\n",
        "\n",
        "sample 2: att_on=96  pad=3  ignored=41  supervised=58\n",
        "\n",
        "sample 3: att_on=92  pad=7  ignored=45  supervised=54\n",
        "\n",
        "sample 4: att_on=85  pad=14  ignored=52  supervised=47\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4PDSr_yr-8sp",
      "metadata": {
        "id": "4PDSr_yr-8sp"
      },
      "source": [
        "Cell 6 — Full forward (masked SFT loss) + tiny training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "LsV5quLX--mh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsV5quLX--mh",
        "outputId": "8f02096c-2590-407c-f699-f3104df7527a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss step1: 3.5683 | step2: 3.6503\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import AdamW\n",
        "import torch\n",
        "\n",
        "llm.train(); projector.train()\n",
        "\n",
        "# Tip: smaller LR if updating the LLM (e.g., 1e-5). If projector-only, 1e-3 is fine.\n",
        "optimizer = AdamW(list(projector.parameters()) + list(llm.parameters()), lr=1e-5)\n",
        "\n",
        "def step_once_fp32(examples):\n",
        "    inputs_embeds, attention_mask, labels = build_batch(examples, max_len=256)\n",
        "\n",
        "    # ensure there are supervised tokens\n",
        "    if int((labels != -100).sum()) == 0:\n",
        "        print(\"⚠️ No supervised tokens — skipping.\")\n",
        "        return float(\"nan\")\n",
        "\n",
        "    out = llm(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=labels)\n",
        "    loss = out.loss\n",
        "    if not torch.isfinite(loss):\n",
        "        print(\"⚠️ Non-finite loss — skipping.\")\n",
        "        return float(\"nan\")\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(list(projector.parameters()) + list(llm.parameters()), 1.0)\n",
        "    optimizer.step()\n",
        "    return float(loss.detach().cpu())\n",
        "\n",
        "batch1 = toy[:3]\n",
        "batch2 = toy[3:]\n",
        "\n",
        "loss1 = step_once_fp32(batch1)\n",
        "loss2 = step_once_fp32(batch2)\n",
        "\n",
        "llm.eval(); projector.eval()\n",
        "print(f\"loss step1: {loss1:.4f} | step2: {loss2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TucmHQ4B_Bjt",
      "metadata": {
        "id": "TucmHQ4B_Bjt"
      },
      "source": [
        "Cell 7 — Quick generation test (prefix-conditioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "xQ_NOk0w_B5O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ_NOk0w_B5O",
        "outputId": "943c1afd-ab70-4ad8-b538-b183b317773c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Aminotransferase, involved in protein degradation or proteasomal processing. The catalytic residues suggest an allosteric effect. The overall topology and sequence indicate it is part of a membrane-bound complex. This suggests an intracellular role. [Source] http://www. biochem-yr. org\n"
          ]
        }
      ],
      "source": [
        "llm.eval(); projector.eval()\n",
        "gen_ex = toy[0]  # pick one\n",
        "prompt = gen_ex[\"prompt\"]\n",
        "\n",
        "# build prefix for this sample\n",
        "pvec = encode_protein_pair(gen_ex.get(\"aa_seq\") or \"\", gen_ex.get(\"stru_str\") or \"\")\n",
        "pref = projector(pvec.unsqueeze(0))                     # (1, H*P)\n",
        "pref = pref.view(1, PREFIX_LEN, D_HID)\n",
        "\n",
        "# tokenize prompt only (no response)\n",
        "enc = tok(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "we  = llm.get_input_embeddings()\n",
        "text = we(enc[\"input_ids\"])                             # (1, T, H)\n",
        "\n",
        "inputs_embeds  = torch.cat([pref, text], dim=1)\n",
        "attention_mask = torch.ones(inputs_embeds.size()[:-1], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "gen = llm.generate(\n",
        "    inputs_embeds=inputs_embeds,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.8,\n",
        "    eos_token_id=tok.eos_token_id,\n",
        "    pad_token_id=tok.pad_token_id\n",
        ")\n",
        "\n",
        "# Drop the prefix when decoding: we passed inputs_embeds, so there's no input_ids.\n",
        "# Decode only the newly generated tokens by slicing off the prompt length; HF adds tokens to 'gen' tensor.\n",
        "print(tok.decode(gen[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "PiwV68Mh_bH9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiwV68Mh_bH9",
        "outputId": "600064a5-daac-4fc1-91df-c9229950438e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Example 1 ===\n",
            "Prompt: You are a professional protein biologist. Based on the amino-acid sequence (and structure if available), write a concise, biologically accurate 2–4 sentence description of the protein.\n",
            "Model:  This is a highly conserved serine-rich, hydrophobic protein that likely functions as a membrane-localized receptor or transmembrane helix–helix–helix–like domain.Human beings have a long history of using DNA to create life forms (such as the origin of life hypothesis). Based on\n",
            "\n",
            "=== Example 2 ===\n",
            "Prompt: You are a professional protein biologist. Based on the amino-acid sequence (and structure if available), write a concise, biologically accurate 2–4 sentence description of the protein.\n",
            "Model:  This protein shows significant homology to members of the serine/threonine kinase family, including S6K1, and interacts with the C-terminal tail. It likely functions in signal transduction or DNA-binding. The presence of a hydrophobic core around the Ser/Thr motifs suggests a role in protein-pro\n",
            "\n",
            "=== Example 3 ===\n",
            "Prompt: You are a professional protein biologist. Based on the amino-acid sequence (and structure if available), write a concise, biologically accurate 2–4 sentence description of the protein.\n",
            "Model:  The protein is likely an α-helical region of a membrane-anchored transmembrane protein, displaying a highly conserved hydrophobic core with an N–terminal helix–turn–like domain. It likely forms part of a lipid-lipid or lipid-protein channel, possibly targeting lipids for\n",
            "\n",
            "--- Unconditioned comparison on example 1 ---\n",
            "You are a professional protein biologist. Based on the amino-acid sequence (and structure if available), write a concise, biologically accurate 2–4 sentence description of the protein. The described protein has an arginine residue in its N-terminus, suggesting possible regulation through post-translational modifications or alternative splicing. It likely functions in signal transduction or cellular signaling pathways involved in cell adhesion and motility. Given its unique properties, it may play a crucial role in the molecular machinery\n"
          ]
        }
      ],
      "source": [
        "# ==== Protein-conditioned natural language generation ====\n",
        "import torch\n",
        "\n",
        "llm.eval(); projector.eval()\n",
        "\n",
        "def build_inputs_with_prefix(prompt: str, aa_seq: str | None, stru_str: str | None):\n",
        "    \"\"\"\n",
        "    Returns inputs_embeds and attention_mask ready for llm.generate(...).\n",
        "    \"\"\"\n",
        "    # 1) tokenize prompt text\n",
        "    enc = tok(prompt, return_tensors=\"pt\").to(llm.device)\n",
        "    we  = llm.get_input_embeddings()\n",
        "    text_embeds = we(enc[\"input_ids\"])                               # (1, T, H)\n",
        "\n",
        "    # 2) make protein vector -> projector -> soft prefix\n",
        "    pvec = encode_protein_pair(aa_seq or \"\", stru_str or \"\")         # (2048,)\n",
        "    pref = projector(pvec.unsqueeze(0)).view(1, PREFIX_LEN, D_HID)   # (1, P, H)\n",
        "\n",
        "    # 3) concat prefix + text\n",
        "    inputs_embeds  = torch.cat([pref, text_embeds], dim=1)           # (1, P+T, H)\n",
        "    attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long, device=llm.device)\n",
        "    return inputs_embeds, attention_mask\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_answer(prompt: str, aa_seq: str | None, stru_str: str | None,\n",
        "                    max_new_tokens=128, temperature=0.7, top_p=0.95, do_sample=True):\n",
        "    inputs_embeds, attention_mask = build_inputs_with_prefix(prompt, aa_seq, stru_str)\n",
        "    out_ids = llm.generate(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "    )\n",
        "    # when using inputs_embeds, HF returns only the newly generated tokens\n",
        "    return tok.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ---- demo on your toy set ----\n",
        "N = min(3, len(toy))   # change N if you want more\n",
        "for i in range(N):\n",
        "    ex = toy[i]\n",
        "    print(f\"\\n=== Example {i+1} ===\")\n",
        "    print(\"Prompt:\", ex[\"prompt\"])\n",
        "    ans = generate_answer(ex[\"prompt\"], ex.get(\"aa_seq\"), ex.get(\"stru_str\"),\n",
        "                          max_new_tokens=64, temperature=0.8, top_p=0.9, do_sample=True)\n",
        "    print(\"Model:\", ans)\n",
        "\n",
        "# (Optional) compare against an unconditioned baseline (no protein prefix)\n",
        "@torch.no_grad()\n",
        "def generate_unconditioned(prompt: str, max_new_tokens=128, temperature=0.7, top_p=0.95, do_sample=True):\n",
        "    enc = tok(prompt, return_tensors=\"pt\").to(llm.device)\n",
        "    out_ids = llm.generate(\n",
        "        **enc,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "    )\n",
        "    return tok.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- Unconditioned comparison on example 1 ---\")\n",
        "print(generate_unconditioned(toy[0][\"prompt\"], max_new_tokens=64, temperature=0.8, top_p=0.9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ce309d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24ce309d",
        "outputId": "414686dc-9d81-4bf1-dace-7ba38108ddaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids torch.Size([2, 24]) torch.int64\n",
            "attention_mask torch.Size([2, 24]) torch.int64\n",
            "labels torch.Size([2, 24]) torch.int64\n",
            "aa_seq <class 'list'> 2\n",
            "stru_str <class 'list'> 2\n",
            "Sample labels row (first 50 tokens): [-100, -100, -100, -100, -100, -100, -100, -100, -100, 1986, 7952, 311, 387, 458, 48142, 448, 3204, 6275, 67, 1080, 519, 5702, 13, 151645]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "JsonlDataset = train_mod.JsonlDataset\n",
        "PadAndMaskCollator = train_mod.PadAndMaskCollator\n",
        "CollateCfg = train_mod.CollateCfg\n",
        "\n",
        "train_set = JsonlDataset(\"sft_data/train_tiny.jsonl\", tokenizer, max_len=MAX_LEN)\n",
        "collate = PadAndMaskCollator(CollateCfg(tokenizer=tokenizer, max_len=MAX_LEN))\n",
        "\n",
        "batch = [train_set[0], train_set[1]]\n",
        "batch_out = collate(batch)\n",
        "\n",
        "for k, v in batch_out.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        print(k, v.shape, v.dtype)\n",
        "    else:\n",
        "        print(k, type(v), len(v))\n",
        "\n",
        "# Labels should be -100 over the prompt (and EOS), non‑masked over response.\n",
        "print(\"Sample labels row (first 50 tokens):\", batch_out[\"labels\"][0][:50].tolist())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
