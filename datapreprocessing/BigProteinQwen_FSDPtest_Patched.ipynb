{"cells":[{"cell_type":"markdown","id":"4065c312","metadata":{"id":"4065c312"},"source":["# FSDP Test Add‑on — `train_prefix_qwen_fsdp.py` (Patched)"]},{"cell_type":"markdown","id":"a34669ad","metadata":{"id":"a34669ad"},"source":["This section imports the patched trainer and runs forward/unit tests so you can see tracebacks inline."]},{"cell_type":"code","source":["#@title Mount Google Drive\n","from pathlib import Path\n","from huggingface_hub import snapshot_download\n","import os, json, pickle, pandas as pd\n","from tqdm import tqdm\n","from rich import print as rprint"],"metadata":{"id":"e_yGbsS7khsu","executionInfo":{"status":"ok","timestamp":1759419560196,"user_tz":240,"elapsed":682,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"id":"e_yGbsS7khsu","execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","%cd /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","\n","from pathlib import Path\n","BASE_DIR = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")\n","OUT_DIR  = BASE_DIR / \"sft_test_demo\"\n","print(f\"Using Google Drive folder as BASE_DIR: {BASE_DIR}\")\n"],"metadata":{"id":"u86TW8jPkjLK"},"id":"u86TW8jPkjLK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check GPU\n","!nvidia-smi\n","\n","# Fresh pip + libs (PyTorch CUDA 12.1 build + matching libs)\n","%pip -q install --upgrade pip\n","%pip install -q --index-url https://download.pytorch.org/whl/cu126 \\\n","  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n","%pip -q install transformers==4.56.1 huggingface_hub==0.35.0 tqdm safetensors"],"metadata":{"id":"ifrT2H7MkklI"},"id":"ifrT2H7MkklI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Version & import sanity checks ---\n","import torch, transformers, huggingface_hub\n","print(\"torch            :\", torch.__version__)\n","print(\"transformers     :\", transformers.__version__)\n","print(\"huggingface_hub  :\", huggingface_hub.__version__)\n","\n","# Top-level ESM import should work on 4.56.1\n","try:\n","    from transformers import AutoTokenizer, EsmForMaskedLM\n","    print(\"✅ Top-level EsmForMaskedLM import OK\")\n","except Exception as e:\n","    print(\"❌ Top-level EsmForMaskedLM import failed:\", repr(e))\n","    # Fallback check (direct module path)\n","    try:\n","        from transformers.models.esm.modeling_esm import EsmForMaskedLM as _E\n","        print(\"✅ Direct modeling_esm import OK (fallback)\")\n","    except Exception as ee:\n","        print(\"❌ Direct modeling_esm import failed too:\", repr(ee))"],"metadata":{"id":"6PCUop5skmB7"},"id":"6PCUop5skmB7","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7fc09b79","metadata":{"id":"7fc09b79"},"outputs":[],"source":["\n","# Installs (adjust if your runtime differs)\n","# %pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","# %pip -q install transformers>=4.43.0 peft accelerate datasets tqdm\n"]},{"cell_type":"code","execution_count":null,"id":"a87e7229","metadata":{"id":"a87e7229"},"outputs":[],"source":["\n","import os, sys\n","print('CWD:', os.getcwd())\n","if os.getcwd() not in sys.path:\n","    sys.path.insert(0, os.getcwd())\n","!ls -la | head -n 40\n"]},{"cell_type":"code","execution_count":null,"id":"39c62e09","metadata":{"id":"39c62e09"},"outputs":[],"source":["\n","from importlib import reload\n","import train_prefix_qwen_fsdp as tpq\n","reload(tpq)\n","\n","from train_prefix_qwen_fsdp import BigProteinQwen, CollateCfg, PadAndMaskCollator, JsonlStream, train as fsdp_train\n","print('Imported OK')\n"]},{"cell_type":"code","execution_count":null,"id":"6772f8aa","metadata":{"id":"6772f8aa"},"outputs":[],"source":["\n","MODEL_NAME       = \"Qwen/Qwen2.5-0.5B-Instruct\"\n","PROTEIN_CONFIG   = \"facebook/esm2_t12_35M_UR50D\"\n","STRUCTURE_CONFIG = \"facebook/esm2_t12_35M_UR50D\"\n","\n","# <- EDIT THESE FOR YOUR DRIVE PATH & SLOTS ->\n","PROTREK_CKPT = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\"\n","PROT_SLOT = 1\n","STRU_SLOT = 3\n","\n","SINGLE_TOKEN_PREFIX = False\n","PREFIX_LEN          = 4\n","PROJ_HID            = 1024\n","DROPOUT             = 0.10\n","DTYPE               = \"bf16\"   # or \"fp32\", \"fp16\", \"auto\"\n","\n","MAX_LEN    = 256\n","BATCH_SIZE = 2\n","\n","print(\"Configs ready\")\n"]},{"cell_type":"code","execution_count":null,"id":"1aed6226","metadata":{"id":"1aed6226"},"outputs":[],"source":["\n","# Tiny JSONL\n","import json, os\n","os.makedirs(\"sft_data\", exist_ok=True)\n","toy = [\n","    {\"prompt\":\"Describe the likely function of this protein.\",\n","     \"response\":\"This appears to be an enzyme with possible hydrolase activity.\",\n","     \"aa_seq\":\"MKTFFVAIATGAFSATA\",\"stru_str\":None},\n","    {\"prompt\":\"What domain might this protein contain?\",\n","     \"response\":\"Likely contains a Rossmann-like fold domain.\",\n","     \"aa_seq\":\"MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNLHGLFGRKTGQAP\",\n","     \"stru_str\":\"ACDEFGHIKLMNPQRSTVWY\"}\n","]\n","with open(\"sft_data/train_tiny.jsonl\",\"w\") as f:\n","    for ex in toy: f.write(json.dumps(ex)+\"\\n\")\n","with open(\"sft_data/val_tiny.jsonl\",\"w\") as f:\n","    for ex in toy: f.write(json.dumps(ex)+\"\\n\")\n","print(\"Wrote toy jsonl\")\n"]},{"cell_type":"code","execution_count":null,"id":"87946804","metadata":{"id":"87946804"},"outputs":[],"source":["\n","from transformers import AutoTokenizer\n","import torch\n","\n","tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","collate = PadAndMaskCollator(CollateCfg(tokenizer=tok, max_len=MAX_LEN))\n","\n","rows = []\n","with open(\"sft_data/train_tiny.jsonl\",\"r\") as f:\n","    for i, line in enumerate(f):\n","        if i>=2: break\n","        rows.append(json.loads(line))\n","\n","batch = collate(rows)\n","for k, v in batch.items():\n","    if isinstance(v, torch.Tensor):\n","        print(k, tuple(v.shape), v.dtype)\n","    else:\n","        print(k, type(v), len(v) if isinstance(v, list) else \"\")\n","print(\"labels[0][:40]:\", batch[\"labels\"][0][:40].tolist())\n"]},{"cell_type":"code","execution_count":null,"id":"6319daa8","metadata":{"id":"6319daa8"},"outputs":[],"source":["\n","# Forward pass\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","big = BigProteinQwen(\n","    model_name=MODEL_NAME,\n","    protein_config=PROTEIN_CONFIG,\n","    structure_config=STRUCTURE_CONFIG,\n","    protrek_ckpt=PROTREK_CKPT,\n","    prot_slot=PROT_SLOT,\n","    stru_slot=STRU_SLOT,\n","    single_token_prefix=SINGLE_TOKEN_PREFIX,\n","    prefix_len=PREFIX_LEN,\n","    proj_hid=PROJ_HID,\n","    dropout=DROPOUT,\n","    train_encoders=False,\n","    dtype_str=DTYPE,\n",").to(device)\n","\n","for k in (\"input_ids\",\"attention_mask\",\"labels\"):\n","    batch[k] = batch[k].to(device)\n","\n","with torch.no_grad():\n","    out = big(**batch)\n","print(\"Forward OK. loss:\", float(out.loss.detach().cpu()))\n"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"language_info":{"name":"python"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}