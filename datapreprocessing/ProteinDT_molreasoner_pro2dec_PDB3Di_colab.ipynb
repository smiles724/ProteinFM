{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f147bba",
      "metadata": {
        "id": "8f147bba"
      },
      "source": [
        "\n",
        "# ProteinDT (Hugging Face) — Download PDB & get 3Di"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vhtz2HvcGy8p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhtz2HvcGy8p",
        "outputId": "d11879b8-6815-48e7-e85a-1201dccc61ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n",
            "Using Google Drive folder as BASE_DIR: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive\n",
        "USE_DRIVE = True\n",
        "DRIVE_SUBDIR = \"hf/proteinDT\"\n",
        "BASE_DIR = \"\"\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n",
        "\n",
        "from pathlib import Path\n",
        "BASE_DIR = Path(\"\")\n",
        "#OUT_DIR  = BASE_DIR / \"sft_test_demo\"\n",
        "print(f\"Using Google Drive folder as BASE_DIR: {BASE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yukg-8YQ-B1f",
      "metadata": {
        "id": "Yukg-8YQ-B1f"
      },
      "source": [
        "This commendline authorize the foldseek to execute. Make sure the foldseek binary file are in correct location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5dQwKe7vc-3O",
      "metadata": {
        "id": "5dQwKe7vc-3O"
      },
      "outputs": [],
      "source": [
        "!chmod +x bin/foldseek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9805eb43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9805eb43",
        "outputId": "ca8dbf79-9eed-4e08-f5fc-ec0e04ae1f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "bigframes 2.21.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Install dependencies\n",
        "!pip -q install --upgrade huggingface_hub datasets pandas biopython lmdb tqdm rich\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Re2Kn7qJXUCW",
      "metadata": {
        "id": "Re2Kn7qJXUCW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from huggingface_hub import snapshot_download\n",
        "import os, json, lmdb, pickle, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rich import print as rprint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yPzQApH4cVTg",
      "metadata": {
        "id": "yPzQApH4cVTg"
      },
      "source": [
        "## Making PDB & 3Di tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SZjwBiKA7HF0",
      "metadata": {
        "id": "SZjwBiKA7HF0"
      },
      "source": [
        "This part of code will search through the chunk submitted to the openai request and build a worklist. It will then search on experimental mmCIFs (RCSB) or AF2 (PDB/CIF).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Don't use mmCif files!!!! Process may fail when process mmCif using foldseek!! for unknown reason**\n",
        "\n",
        "Will fix this in the future. But currently let's only use PDB files.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Technically this code will search for experimental Cif first, if files not found, then it will search on AF2(Predicted) dataset intead. However we currently cannot use CIF, so now we only use AF2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OkpB9pPQ-Z7_",
      "metadata": {
        "id": "OkpB9pPQ-Z7_"
      },
      "source": [
        "Roughly 6 min/ 1k sequence, depends on internet connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2wNkdqqDtqjb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "2wNkdqqDtqjb",
        "outputId": "a69b453c-4520-4e14-d327-fa287d78766c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found UniProt IDs in 10 meta files (chunks 0..9): 10000\n",
            "Already logged: 0  |  To download now: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AF2 PDB downloads: 100%|██████████| 10000/10000 [1:08:22<00:00,  2.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary:\n",
            "  Batches covered (chunks): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "  Total UniProt in these batches: 10000\n",
            "  Newly downloaded this run: 7806\n",
            "  Total AF2 PDB entries logged: 7806\n",
            "AF2 PDB log: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/structures_downloaded_af2_pdb.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"    pass\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"uniprot_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Q817F2\",\n          \"P21305\",\n          \"A7HX46\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdb_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"AF-Q817F2-F1-model_v4\",\n          \"AF-P21305-F1-model_v4\",\n          \"AF-A7HX46-F1-model_v4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/downloads/Q817F2/AF-Q817F2-F1-model_v4.pdb\",\n          \"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/downloads/P21305/AF-P21305-F1-model_v4.pdb\",\n          \"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/downloads/A7HX46/AF-A7HX46-F1-model_v4.pdb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AF2_PDB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c3939f61-b921-4d62-8193-6da579bc507f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniprot_id</th>\n",
              "      <th>pdb_id</th>\n",
              "      <th>file</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7801</th>\n",
              "      <td>C3KDX7</td>\n",
              "      <td>AF-C3KDX7-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>AF2_PDB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7802</th>\n",
              "      <td>Q817F2</td>\n",
              "      <td>AF-Q817F2-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>AF2_PDB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7803</th>\n",
              "      <td>A7HX46</td>\n",
              "      <td>AF-A7HX46-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>AF2_PDB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7804</th>\n",
              "      <td>Q6D222</td>\n",
              "      <td>AF-Q6D222-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>AF2_PDB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7805</th>\n",
              "      <td>P21305</td>\n",
              "      <td>AF-P21305-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>AF2_PDB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3939f61-b921-4d62-8193-6da579bc507f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c3939f61-b921-4d62-8193-6da579bc507f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c3939f61-b921-4d62-8193-6da579bc507f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-aba23348-509b-4e06-8fd7-101f7529afc3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aba23348-509b-4e06-8fd7-101f7529afc3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-aba23348-509b-4e06-8fd7-101f7529afc3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     uniprot_id                 pdb_id  \\\n",
              "7801     C3KDX7  AF-C3KDX7-F1-model_v4   \n",
              "7802     Q817F2  AF-Q817F2-F1-model_v4   \n",
              "7803     A7HX46  AF-A7HX46-F1-model_v4   \n",
              "7804     Q6D222  AF-Q6D222-F1-model_v4   \n",
              "7805     P21305  AF-P21305-F1-model_v4   \n",
              "\n",
              "                                                   file   source  \n",
              "7801  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...  AF2_PDB  \n",
              "7802  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...  AF2_PDB  \n",
              "7803  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...  AF2_PDB  \n",
              "7804  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...  AF2_PDB  \n",
              "7805  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...  AF2_PDB  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === Download AlphaFold **PDB** for all UniProt in the first 10 submitted batches (10k) ===\n",
        "import json, time, re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- paths (adjust BASE_DIR if needed) ----\n",
        "BASE_DIR       = Path(\"\")\n",
        "OUT_DIR_FIXED  = BASE_DIR / \"gpt_batch_protein2desc_fixed_answer\"\n",
        "STRUCTURE_HOME = BASE_DIR / \"downloads\"\n",
        "STRUCTURE_HOME.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "AF2_PDB_LOG_CSV = OUT_DIR_FIXED / \"structures_downloaded_af2_pdb.csv\"\n",
        "\n",
        "# AlphaFold PDB (not CIF)\n",
        "AF2_PDB_URLS = [\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v4.pdb\",\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v3.pdb\",\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v2.pdb\",\n",
        "]\n",
        "\n",
        "TIMEOUT = 25\n",
        "PAUSE   = 0.02\n",
        "\n",
        "def make_session():\n",
        "    s = requests.Session()\n",
        "    retries = Retry(\n",
        "        total=4, backoff_factor=0.5,\n",
        "        status_forcelist=[429,500,502,503,504],\n",
        "        allowed_methods=[\"GET\"]\n",
        "    )\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "session = make_session()\n",
        "\n",
        "def parse_chunk_idx(p: Path) -> int | None:\n",
        "    m = re.search(r\"_chunk(\\d{3})\\.json$\", p.name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def download_af2_pdb(uid: str, out_dir: Path) -> Path | None:\n",
        "    \"\"\"\n",
        "    Try AF2 PDB v4 -> v3 -> v2 for the (canonical) UniProt accession.\n",
        "    Writes immediately and returns saved Path or None.\n",
        "    \"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    uid_base = uid.split(\"-\")[0]  # handle isoforms like P12345-2\n",
        "    for tpl in AF2_PDB_URLS:\n",
        "        url = tpl.format(uid=uid_base)\n",
        "        out = out_dir / Path(url).name\n",
        "        try:\n",
        "            r = session.get(url, timeout=TIMEOUT)\n",
        "            if r.status_code == 200 and r.content and len(r.content) > 1024:\n",
        "                out.write_bytes(r.content)\n",
        "                return out\n",
        "        except Exception:\n",
        "            pass\n",
        "        time.sleep(PAUSE)\n",
        "    return None\n",
        "\n",
        "# ---- discover the 10 submitted batches (chunks 000–009) ----\n",
        "info_files = sorted(OUT_DIR_FIXED.glob(\"protein2desc_fixedans_batch_info_*_chunk???.json\"))\n",
        "# keep the first 10 unique chunk indices (000..009). If you want *all* submitted, drop the [:10].\n",
        "chunk_indices = sorted({parse_chunk_idx(p) for p in info_files if parse_chunk_idx(p) is not None})[:10]\n",
        "assert chunk_indices, \"No submitted batch_info files found.\"\n",
        "\n",
        "# collect matching meta files for those chunks\n",
        "meta_files = []\n",
        "for ci in chunk_indices:\n",
        "    # any meta file with this chunk index\n",
        "    meta_files += list(OUT_DIR_FIXED.glob(f\"protein2desc_fixedans_meta_*_chunk{ci:03d}.json\"))\n",
        "\n",
        "assert meta_files, \"No meta files found for the discovered chunks.\"\n",
        "\n",
        "# ---- gather unique UniProt IDs from these meta files ----\n",
        "uids = []\n",
        "for p in meta_files:\n",
        "    try:\n",
        "        meta = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "        for _cid, rec in meta.items():\n",
        "            uid = str(rec.get(\"uniprot_id\") or \"\").strip()\n",
        "            if uid:\n",
        "                uids.append(uid)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "uids = list(dict.fromkeys(uids))  # unique, preserve order\n",
        "print(f\"Found UniProt IDs in {len(meta_files)} meta files (chunks {chunk_indices[0]}..{chunk_indices[-1]}): {len(uids)}\")\n",
        "\n",
        "# ---- resume-safe: load previous AF2 PDB log ----\n",
        "if AF2_PDB_LOG_CSV.exists() and AF2_PDB_LOG_CSV.stat().st_size > 0:\n",
        "    df_log_old = pd.read_csv(AF2_PDB_LOG_CSV)\n",
        "    have_uids = set(df_log_old[\"uniprot_id\"].astype(str)) if \"uniprot_id\" in df_log_old.columns else set()\n",
        "else:\n",
        "    df_log_old = pd.DataFrame(columns=[\"uniprot_id\",\"pdb_id\",\"file\",\"source\"])\n",
        "    have_uids  = set()\n",
        "\n",
        "# Skip already-downloaded\n",
        "targets = [u for u in uids if u not in have_uids]\n",
        "print(f\"Already logged: {len(have_uids)}  |  To download now: {len(targets)}\")\n",
        "\n",
        "new_records = []\n",
        "for uid in tqdm(targets, desc=\"AF2 PDB downloads\"):\n",
        "    out_dir = STRUCTURE_HOME / uid\n",
        "    saved = download_af2_pdb(uid, out_dir)\n",
        "    if saved and saved.exists() and saved.stat().st_size > 1024:\n",
        "        new_records.append({\n",
        "            \"uniprot_id\": uid,\n",
        "            \"pdb_id\": Path(saved).stem,      # AF-<UID>-F1-model_vX\n",
        "            \"file\": str(saved),\n",
        "            \"source\": \"AF2_PDB\"\n",
        "        })\n",
        "\n",
        "# ---- write/merge log ----\n",
        "df_new = pd.DataFrame(new_records, columns=[\"uniprot_id\",\"pdb_id\",\"file\",\"source\"])\n",
        "df_out = pd.concat([df_log_old, df_new], ignore_index=True)\n",
        "df_out = df_out.drop_duplicates(subset=[\"uniprot_id\",\"pdb_id\"]).reset_index(drop=True)\n",
        "df_out.to_csv(AF2_PDB_LOG_CSV, index=False)\n",
        "\n",
        "# ---- summary ----\n",
        "n_total = len(uids)\n",
        "n_have  = df_out[\"uniprot_id\"].nunique() if \"uniprot_id\" in df_out.columns else 0\n",
        "n_new   = len(new_records)\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"  Batches covered (chunks): {chunk_indices}\")\n",
        "print(f\"  Total UniProt in these batches: {n_total}\")\n",
        "print(f\"  Newly downloaded this run: {n_new}\")\n",
        "print(f\"  Total AF2 PDB entries logged: {n_have}\")\n",
        "print(\"AF2 PDB log:\", AF2_PDB_LOG_CSV)\n",
        "\n",
        "# small preview\n",
        "try:\n",
        "    display(df_out.tail(5))\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gl7G76yL9QRy",
      "metadata": {
        "id": "gl7G76yL9QRy"
      },
      "source": [
        "in this part it wil tranform all the downloaded pdb files to 3Di Tokens using foldseek.\n",
        "\n",
        "Foldseek does not provide checkpoints, so the only two ways to use foldseek: 1) host local server (linux only, don't recommend if you are using wsl2, too many bugs) 2) use the binary files provided in protrek repo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PnM2K85G-m-a",
      "metadata": {
        "id": "PnM2K85G-m-a"
      },
      "source": [
        "Also roughly 7 min/ 1k. Normally only 80% of sequence can get full data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4wcQ4xQCcJq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4wcQ4xQCcJq",
        "outputId": "e20eae6b-b44b-4fe9-82a0-9649b34cd075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Targets from chunks 0..9: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ensure AF2 PDB & build 3Di:  18%|█▊        | 1843/10000 [07:24<46:08,  2.95it/s]"
          ]
        }
      ],
      "source": [
        "# === Build 3Di (Foldseek) for first 10 submitted batches (PDB only) — SKIP if 3Di exists ===\n",
        "import json, re, time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, List\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# -------------------- Paths (edit BASE_DIR if needed) --------------------\n",
        "BASE_DIR        = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")\n",
        "OUT_DIR_FIXED   = BASE_DIR / \"gpt_batch_protein2desc_fixed_answer\"\n",
        "STRUCTURE_HOME  = BASE_DIR / \"downloads\"\n",
        "SFT_OUT_DIR     = BASE_DIR / \"sft_build\"\n",
        "SFT_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "THREEDI_DIR     = SFT_OUT_DIR / \"foldseek_3di_pdb\"\n",
        "THREEDI_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MANIFEST_CSV    = THREEDI_DIR / \"3di_manifest_pdb.csv\"\n",
        "FAILED_TXT      = THREEDI_DIR / \"3di_failed_pdb.txt\"\n",
        "\n",
        "# Skip flag: if True, we won't recompute 3Di for UIDs that already have *.3di.txt\n",
        "SKIP_IF_3DI_EXISTS = True\n",
        "\n",
        "# -------------------- Foldseek util --------------------\n",
        "from foldseek_util import get_struc_seq\n",
        "try:\n",
        "    FOLDSEEK_BIN\n",
        "except NameError:\n",
        "    FOLDSEEK_BIN = Path(\"bin/foldseek\")  # e.g., Path(\"/usr/local/bin/foldseek\"))\n",
        "\n",
        "# -------------------- AF2 (PDB) download helpers --------------------\n",
        "AF2_PDB_URLS = [\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v4.pdb\",\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v3.pdb\",\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v2.pdb\",\n",
        "]\n",
        "TIMEOUT = 25\n",
        "PAUSE   = 0.02\n",
        "\n",
        "def make_session():\n",
        "    s = requests.Session()\n",
        "    retries = Retry(\n",
        "        total=4, backoff_factor=0.5,\n",
        "        status_forcelist=[429,500,502,503,504],\n",
        "        allowed_methods=[\"GET\"]\n",
        "    )\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "session = make_session()\n",
        "AF2_PDB_RE = re.compile(r\"^AF-(?P<uid>[^-]+)-F1-model_v(?P<v>\\d+)\\.pdb$\", re.I)\n",
        "\n",
        "def find_best_af2_pdb(uid_dir: Path) -> Path | None:\n",
        "    best, best_v = None, -1\n",
        "    for p in uid_dir.glob(\"AF-*.pdb\"):\n",
        "        m = AF2_PDB_RE.match(p.name)\n",
        "        if not m:\n",
        "            continue\n",
        "        v = int(m.group(\"v\"))\n",
        "        if v > best_v:\n",
        "            best_v, best = v, p\n",
        "    return best\n",
        "\n",
        "def ensure_af2_pdb(uid: str) -> Path | None:\n",
        "    d = STRUCTURE_HOME / uid\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "    best = find_best_af2_pdb(d)\n",
        "    if best and best.exists() and best.stat().st_size > 1024:\n",
        "        return best\n",
        "    uid_base = uid.split(\"-\")[0].strip()\n",
        "    for tpl in AF2_PDB_URLS:\n",
        "        url = tpl.format(uid=uid_base)\n",
        "        out = d / Path(url).name\n",
        "        try:\n",
        "            r = session.get(url, timeout=TIMEOUT)\n",
        "            if r.status_code == 200 and r.content and len(r.content) > 1024:\n",
        "                out.write_bytes(r.content)\n",
        "                return out\n",
        "        except Exception:\n",
        "            pass\n",
        "        time.sleep(PAUSE)\n",
        "    return None\n",
        "\n",
        "# -------------------- Batch discovery (first 10 chunks) --------------------\n",
        "def parse_chunk_idx(p: Path) -> int | None:\n",
        "    m = re.search(r\"_chunk(\\d{3})\\.json$\", p.name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "info_files = sorted(OUT_DIR_FIXED.glob(\"protein2desc_fixedans_batch_info_*_chunk???.json\"))\n",
        "chunk_indices_all = sorted({parse_chunk_idx(p) for p in info_files if parse_chunk_idx(p) is not None})\n",
        "assert chunk_indices_all, f\"No submitted batch_info files found under {OUT_DIR_FIXED}\"\n",
        "\n",
        "chunk_indices = chunk_indices_all[:10]  # first 10 batches\n",
        "\n",
        "meta_files = []\n",
        "for ci in chunk_indices:\n",
        "    meta_files += list(OUT_DIR_FIXED.glob(f\"protein2desc_fixedans_meta_*_chunk{ci:03d}.json\"))\n",
        "assert meta_files, \"No meta files found for discovered chunks.\"\n",
        "\n",
        "uids = []\n",
        "for p in meta_files:\n",
        "    try:\n",
        "        meta = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "        for _, rec in meta.items():\n",
        "            uid = str(rec.get(\"uniprot_id\") or \"\").strip()\n",
        "            if uid: uids.append(uid)\n",
        "    except Exception:\n",
        "        pass\n",
        "uids = list(dict.fromkeys(uids))\n",
        "TOTAL_TARGETS = len(uids)\n",
        "print(f\"Targets from chunks {chunk_indices[0]}..{chunk_indices[-1]}: {TOTAL_TARGETS}\")\n",
        "\n",
        "# -------------------- 3Di build config --------------------\n",
        "CHAIN_PROBE       = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n",
        "CHAIN_SEP_TOKEN   = \"<|chain_sep|>\"\n",
        "ADD_CHAIN_ID_TAG  = True\n",
        "CHAIN_ID_FMT      = \"<|chain:{cid}|>\"\n",
        "\n",
        "def concat_chains(parsed: Dict[str, Tuple[str, str]]) -> Tuple[str, str, List[Dict]]:\n",
        "    chain_ids = sorted(parsed.keys())\n",
        "    aa_parts, di_parts, meta = [], [], []\n",
        "    for cid in chain_ids:\n",
        "        aa_seq, di_seq = parsed[cid][0], parsed[cid][1]\n",
        "        if ADD_CHAIN_ID_TAG:\n",
        "            aa_parts.append(CHAIN_ID_FMT.format(cid=cid))\n",
        "            di_parts.append(CHAIN_ID_FMT.format(cid=cid))\n",
        "        aa_parts.append(aa_seq)\n",
        "        di_parts.append(di_seq.lower())\n",
        "        meta.append({\"chain\": cid, \"aa_len\": len(aa_seq), \"di_len\": len(di_seq)})\n",
        "    aa_concat = f\" {CHAIN_SEP_TOKEN} \".join(aa_parts)\n",
        "    di_concat = f\" {CHAIN_SEP_TOKEN} \".join(di_parts)\n",
        "    return aa_concat, di_concat, meta\n",
        "\n",
        "def write_outputs(uid: str, pdb_file: Path, aa_concat: str, di_concat: str, chain_meta: List[Dict]):\n",
        "    (THREEDI_DIR / f\"{uid}.3di.txt\").write_text(di_concat, encoding=\"utf-8\")\n",
        "    (THREEDI_DIR / f\"{uid}.aa.txt\").write_text(aa_concat, encoding=\"utf-8\")\n",
        "    meta = {\n",
        "        \"uniprot_id\": uid,\n",
        "        \"pdb_file\": str(pdb_file),\n",
        "        \"chain_sep_token\": CHAIN_SEP_TOKEN,\n",
        "        \"add_chain_id_tag\": ADD_CHAIN_ID_TAG,\n",
        "        \"chain_id_fmt\": CHAIN_ID_FMT,\n",
        "        \"chains\": chain_meta,\n",
        "        \"aa_total_len\": sum(m[\"aa_len\"] for m in chain_meta),\n",
        "        \"di_total_len\": sum(m[\"di_len\"] for m in chain_meta),\n",
        "    }\n",
        "    (THREEDI_DIR / f\"{uid}.chains.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    return meta\n",
        "\n",
        "# Helper: check if a UID already has a 3Di file\n",
        "def has_3di(uid: str) -> bool:\n",
        "    f = THREEDI_DIR / f\"{uid}.3di.txt\"\n",
        "    return f.exists() and f.stat().st_size > 0\n",
        "\n",
        "# -------------------- Process: ensure PDB → 3Di (skip if exists) --------------------\n",
        "rows = []\n",
        "failed = []\n",
        "have_pdb_count = 0\n",
        "three_di_ok = 0\n",
        "skipped_count = 0\n",
        "\n",
        "# If a previous manifest exists, load it so we keep old rows\n",
        "df_prev = pd.read_csv(MANIFEST_CSV) if MANIFEST_CSV.exists() and MANIFEST_CSV.stat().st_size > 0 else pd.DataFrame()\n",
        "\n",
        "for uid in tqdm(uids, desc=\"Ensure AF2 PDB & build 3Di\"):\n",
        "    # If we already have a 3Di file and skipping is enabled, move on\n",
        "    if SKIP_IF_3DI_EXISTS and has_3di(uid):\n",
        "        skipped_count += 1\n",
        "        continue\n",
        "\n",
        "    # 1) ensure PDB\n",
        "    pdb_path = ensure_af2_pdb(uid)\n",
        "    if pdb_path and pdb_path.exists() and pdb_path.stat().st_size > 1024:\n",
        "        have_pdb_count += 1\n",
        "    else:\n",
        "        failed.append(f\"{uid}\\tno_af2_pdb\")\n",
        "        continue\n",
        "\n",
        "    # 2) get all chains → 3Di\n",
        "    try:\n",
        "        parsed = get_struc_seq(str(FOLDSEEK_BIN), str(pdb_path), CHAIN_PROBE, plddt_mask=True)\n",
        "        if not parsed:\n",
        "            failed.append(f\"{uid}\\tno_chains_returned\")\n",
        "            print(f\"No chains returned for {uid}\")\n",
        "            continue\n",
        "\n",
        "        aa_concat, di_concat, chain_meta = concat_chains(parsed)\n",
        "        meta = write_outputs(uid, pdb_path, aa_concat, di_concat, chain_meta)\n",
        "        three_di_ok += 1\n",
        "\n",
        "        rows.append({\n",
        "            \"uniprot_id\": uid,\n",
        "            \"pdb_file\": str(pdb_path),\n",
        "            \"aa_total_len\": meta[\"aa_total_len\"],\n",
        "            \"di_total_len\": meta[\"di_total_len\"],\n",
        "            \"n_chains\": len(meta[\"chains\"]),\n",
        "            \"di_path\": str(THREEDI_DIR / f\"{uid}.3di.txt\"),\n",
        "            \"aa_path\": str(THREEDI_DIR / f\"{uid}.aa.txt\"),\n",
        "            \"meta_path\": str(THREEDI_DIR / f\"{uid}.chains.json\"),\n",
        "        })\n",
        "    except Exception as e:\n",
        "        failed.append(f\"{uid}\\t{type(e).__name__}:{e}\")\n",
        "\n",
        "# -------------------- Save manifest & failed list --------------------\n",
        "df_new = pd.DataFrame(rows, columns=[\"uniprot_id\",\"pdb_file\",\"aa_total_len\",\"di_total_len\",\"n_chains\",\"di_path\",\"aa_path\",\"meta_path\"])\n",
        "if not df_prev.empty:\n",
        "    df_all = pd.concat([df_prev, df_new], ignore_index=True)\n",
        "    df_all = df_all.drop_duplicates(subset=[\"uniprot_id\"], keep=\"last\").reset_index(drop=True)\n",
        "else:\n",
        "    df_all = df_new\n",
        "df_all.to_csv(MANIFEST_CSV, index=False)\n",
        "Path(FAILED_TXT).write_text(\"\\n\".join(sorted(set(failed))), encoding=\"utf-8\")\n",
        "\n",
        "# -------------------- Stats --------------------\n",
        "print(\"\\n=== STATS (10k batch set) ===\")\n",
        "print(f\"Total targets (UniProt):           {TOTAL_TARGETS}\")\n",
        "print(f\"Skipped (already had 3Di):         {skipped_count}\")\n",
        "print(f\"With AF2 PDB available (new run):  {have_pdb_count}\")\n",
        "print(f\"3Di successfully generated (new):  {three_di_ok}\")\n",
        "print(f\"Failures (see file):               {len(set(failed))}  -> {FAILED_TXT.name}\")\n",
        "print(\"Manifest written to:               \", MANIFEST_CSV)\n",
        "\n",
        "# small peek\n",
        "try:\n",
        "    display(df_all.tail(5))\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jCa4Ax7UCfUK",
      "metadata": {
        "id": "jCa4Ax7UCfUK"
      },
      "source": [
        "Targets from chunks 0..9: 10000\n",
        "\n",
        "Ensure AF2 PDB & build 3Di:  25%|██▍       | 2473/10000 [16:25<49:58,  2.51it/s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xGtl27RfBRVJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGtl27RfBRVJ",
        "outputId": "850f4404-0315-443d-ef2c-53095ee2c90e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AF2 PDB files on disk: 7807\n",
            "3Di outputs on disk: 1902\n",
            "\n",
            "No progress log yet: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/sft_build/foldseek_3di_pdb/3di_progress_log.csv\n",
            "\n",
            "No manifest yet: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/sft_build/foldseek_3di_pdb/3di_manifest_pdb.csv\n"
          ]
        }
      ],
      "source": [
        "#@title Progress Checker\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_DIR      = Path(\"\")\n",
        "THREEDI_DIR   = BASE_DIR / \"sft_build\" / \"foldseek_3di_pdb\"\n",
        "STRUCT_HOME   = BASE_DIR / \"downloads\"\n",
        "\n",
        "PROGRESS_LOG  = THREEDI_DIR / \"3di_progress_log.csv\"\n",
        "MANIFEST_CSV  = THREEDI_DIR / \"3di_manifest_pdb.csv\"\n",
        "\n",
        "# 1) how many PDBs exist on disk so far?\n",
        "pdb_files = list(STRUCT_HOME.rglob(\"AF-*-F1-model_v*.pdb\"))\n",
        "print(\"AF2 PDB files on disk:\", len(pdb_files))\n",
        "\n",
        "# 2) how many 3Di outputs exist?\n",
        "three_di_files = list(THREEDI_DIR.glob(\"*.3di.txt\"))\n",
        "print(\"3Di outputs on disk:\", len(three_di_files))\n",
        "\n",
        "# 3) tail the progress log\n",
        "if PROGRESS_LOG.exists():\n",
        "    dfp = pd.read_csv(PROGRESS_LOG)\n",
        "    print(\"\\nLog rows:\", len(dfp))\n",
        "    print(dfp.tail(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"\\nNo progress log yet:\", PROGRESS_LOG)\n",
        "\n",
        "# 4) peek the manifest\n",
        "if MANIFEST_CSV.exists():\n",
        "    dfm = pd.read_csv(MANIFEST_CSV)\n",
        "    print(\"\\nManifest rows:\", len(dfm))\n",
        "    print(dfm.tail(5))\n",
        "else:\n",
        "    print(\"\\nNo manifest yet:\", MANIFEST_CSV)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dsd9RxuI4lTz",
      "metadata": {
        "id": "dsd9RxuI4lTz"
      },
      "source": [
        "# Debug attempt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75123f6e",
      "metadata": {},
      "source": [
        "ignore the code below if you just want to run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0xhmjWm2rIZq",
      "metadata": {
        "id": "0xhmjWm2rIZq"
      },
      "outputs": [],
      "source": [
        "# === Rebuild worklist_5k.csv from your existing outputs ===\n",
        "from pathlib import Path\n",
        "import json, re\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")  # adjust if needed\n",
        "# Put the worklist in the same OUT_DIR you’re using for the structure pipeline\n",
        "OUT_DIR  = BASE_DIR / \"gpt_batch_protein2desc\"   # or \"sft_build\" if that’s what you used\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "WORKLIST_CSV = OUT_DIR / \"worklist_5k.csv\"\n",
        "\n",
        "def gather_from_batch_meta(dirpath: Path):\n",
        "    rows = []\n",
        "    for p in sorted(dirpath.glob(\"protein2desc_*meta*.json\")):\n",
        "        try:\n",
        "            meta = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "            for cid, m in meta.items():\n",
        "                uid = str(m.get(\"uniprot_id\") or \"\").strip()\n",
        "                seq = str(m.get(\"sequence\") or m.get(\"protein_sequence\") or \"\").strip()\n",
        "                if uid:\n",
        "                    rows.append((uid, seq))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return rows\n",
        "\n",
        "def gather_from_sft(dirpath: Path):\n",
        "    rows = []\n",
        "    for p in sorted(dirpath.glob(\"protein2desc_*sft*.json\")):\n",
        "        try:\n",
        "            data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "            if isinstance(data, list):\n",
        "                for r in data:\n",
        "                    inp = r.get(\"input\") or {}\n",
        "                    uid = str(inp.get(\"uniprot_id\") or \"\").strip()\n",
        "                    seq = str(inp.get(\"protein_sequence\") or \"\").strip()\n",
        "                    if uid:\n",
        "                        rows.append((uid, seq))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return rows\n",
        "\n",
        "rows = []\n",
        "# 1) batch meta JSONs created during submissions/fetch\n",
        "rows += gather_from_batch_meta(OUT_DIR)\n",
        "# 2) any SFT JSONs you built (parsed/raw)\n",
        "rows += gather_from_sft(OUT_DIR)\n",
        "\n",
        "# de-dup, prefer the first non-empty sequence we saw for a given UniProt\n",
        "uniq = {}\n",
        "for uid, seq in rows:\n",
        "    if uid not in uniq or (not uniq[uid] and seq):\n",
        "        uniq[uid] = seq\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    [{\"uniprot_id\": uid, \"protein_sequence\": uniq[uid]} for uid in uniq.keys()]\n",
        ").sort_values(\"uniprot_id\").reset_index(drop=True)\n",
        "\n",
        "# If you only want the first 5k, slice here:\n",
        "df_5k = df.head(5000).copy()\n",
        "\n",
        "df_5k.to_csv(WORKLIST_CSV, index=False)\n",
        "print(f\"Saved worklist: {WORKLIST_CSV}  rows={len(df_5k)}\")\n",
        "df_5k.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "yb6xvXa3gFIK",
      "metadata": {
        "id": "yb6xvXa3gFIK"
      },
      "outputs": [],
      "source": [
        "import os, json, re, html, random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "BASE_DIR        = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")  # Change if needed\n",
        "RESULTS_ROOT    = BASE_DIR / \"gpt_batch_protein2desc_fixed_answer\" / \"sft_results\"       # where 5k come from\n",
        "STRUCTURE_HOME  = BASE_DIR / \"downloads\"                                            # per-UniProt structure dir\n",
        "FOLDSEEK_BIN    = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/bin/foldseek\")                                              # local foldseek binary\n",
        "OUT_DIR         = BASE_DIR / \"sft_build\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "N_LIMIT         = 5000         # take 5k\n",
        "USE_GT_RESPONSE = True         # True: use ground-truth description from meta; False: parse <answer> from output\n",
        "SHUFFLE_SEED    = 7\n",
        "\n",
        "OUT_JSONL       = OUT_DIR / \"protein2desc_seq+3di.jsonl\"\n",
        "LOG_BAD         = OUT_DIR / \"protein2desc_missing_structure.log\"\n",
        "\n",
        "# Prompt template used in \"prompt\" (SFT input). Keep it natural and short.\n",
        "PROMPT_TEMPLATE = (\n",
        "    \"You are a professional protein biologist. Based on the amino-acid sequence (and structure if available), \"\n",
        "    \"write a concise, biologically accurate 2–4 sentence description of the protein.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "t9IBZYHpgRhO",
      "metadata": {
        "id": "t9IBZYHpgRhO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Provided by SaProt/ProTrek repos\n",
        "    from foldseek_util import get_struc_seq\n",
        "except Exception as e:\n",
        "    raise ImportError(\n",
        "        \"Could not import utils.foldseek_util.get_struc_seq. \"\n",
        "        \"Please ensure you have the SaProt/ProTrek utilities on your PYTHONPATH.\\n\"\n",
        "        \"Repo example (SaProt): utils/foldseek_util.py has get_struc_seq().\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "Psl5IRr6gXSX",
      "metadata": {
        "id": "Psl5IRr6gXSX"
      },
      "outputs": [],
      "source": [
        "# ---------- Helpers ----------\n",
        "def strip_html(s: str) -> str:\n",
        "    s = s or \"\"\n",
        "    s = html.unescape(s)\n",
        "    return re.sub(r\"<[^>]+>\", \" \", s).replace(\"\\xa0\",\" \").strip()\n",
        "\n",
        "def _extract_tag_loose(text: str, tag: str):\n",
        "    if not text:\n",
        "        return \"\", 0, False\n",
        "    open_pat  = re.compile(fr\"<{tag}\\b[^>]*>\", re.I)\n",
        "    close_pat = re.compile(fr\"</{tag}\\s*>\", re.I)\n",
        "    m_open = open_pat.search(text)\n",
        "    if not m_open:\n",
        "        return \"\", 0, False\n",
        "    m_close = close_pat.search(text, m_open.end())\n",
        "    if m_close:\n",
        "        content = text[m_open.end():m_close.start()]\n",
        "        endpos  = m_close.end()\n",
        "    else:\n",
        "        content = text[m_open.end():]\n",
        "        endpos  = len(text)\n",
        "    content = content.strip().rstrip('\", \\t\\r\\n')\n",
        "    return content, endpos, True\n",
        "\n",
        "def parse_thinking_answer_loose(text: str):\n",
        "    if not text:\n",
        "        return \"\", \"\"\n",
        "    thinking, th_end, th_found = _extract_tag_loose(text, \"thinking\")\n",
        "    remainder = text[th_end:] if th_found else text\n",
        "    answer, _, an_found = _extract_tag_loose(remainder, \"answer\")\n",
        "    if not th_found and an_found:\n",
        "        m_open_ans = re.search(r\"<answer\\b[^>]*>\", remainder, re.I)\n",
        "        cutoff = m_open_ans.start() if m_open_ans else len(text)\n",
        "        pre = text[:cutoff].strip().rstrip('\", \\t\\r\\n')\n",
        "        thinking = pre if pre else thinking\n",
        "    if not th_found and not an_found:\n",
        "        thinking = text.strip().rstrip('\", \\t\\r\\n')\n",
        "        answer   = \"\"\n",
        "    return thinking, answer\n",
        "\n",
        "def two_line_pairs(path: Path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        while True:\n",
        "            id_line = f.readline()\n",
        "            if not id_line: break\n",
        "            val_line = f.readline()\n",
        "            if not val_line: break\n",
        "            yield id_line.strip(), val_line.strip()\n",
        "\n",
        "def load_swissprotclap(base_dir: Path):\n",
        "    sp = base_dir / \"SwissProtCLAP\"\n",
        "    seq_fp, txt_fp = sp / \"protein_sequence.txt\", sp / \"text_sequence.txt\"\n",
        "    id2seq = {pid: seq for pid, seq in two_line_pairs(seq_fp)}\n",
        "    id2cap = {pid: strip_html(txt) for pid, txt in two_line_pairs(txt_fp)}\n",
        "    return id2seq, id2cap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9GAZy39gsQFN",
      "metadata": {
        "id": "9GAZy39gsQFN"
      },
      "source": [
        "### Downloading Cif/PDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "o4eW2dVSuhpu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4eW2dVSuhpu",
        "outputId": "ccdc2ff3-ea33-4313-9f13-c78c2a2118d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/downloads')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "STRUCTURE_HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "DBupNZN7wQFk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "DBupNZN7wQFk",
        "outputId": "dd9e2548-2d4a-4a40-92bc-e43c891cfdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Need AF2 for (no experimental + not already AF2): 4965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AF2 CIF download: 100%|██████████| 4965/4965 [46:26<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "AF2 summary:\n",
            "  New AF2 files: 3863\n",
            "  Total AF2 log rows: 3863\n",
            "  Missed AF2: 1102\n",
            "AF2 log: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/sft_build/structures_downloaded_af2.csv\n",
            "AF2 missed list: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/sft_build/structures_missing_af2.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"    display(pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"uniprot_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"A3CXH4\",\n          \"Q2JL77\",\n          \"P52409\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdb_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"AF-A3CXH4-F1-model_v4\",\n          \"AF-Q2JL77-F1-model_v4\",\n          \"AF-P52409-F1-model_v4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/downloads/A3CXH4/AF-A3CXH4-F1-model_v4.cif\",\n          \"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/downloads/Q2JL77/AF-Q2JL77-F1-model_v4.cif\",\n          \"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/downloads/P52409/AF-P52409-F1-model_v4.cif\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chains\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AF2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a4c94b00-c953-4904-b96d-a6da88c6b2b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniprot_id</th>\n",
              "      <th>pdb_id</th>\n",
              "      <th>file</th>\n",
              "      <th>chains</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A1K1S0</td>\n",
              "      <td>AF-A1K1S0-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>A</td>\n",
              "      <td>AF2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A3CXH4</td>\n",
              "      <td>AF-A3CXH4-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>A</td>\n",
              "      <td>AF2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P52409</td>\n",
              "      <td>AF-P52409-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>A</td>\n",
              "      <td>AF2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q1CT83</td>\n",
              "      <td>AF-Q1CT83-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>A</td>\n",
              "      <td>AF2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q2JL77</td>\n",
              "      <td>AF-Q2JL77-F1-model_v4</td>\n",
              "      <td>/content/drive/MyDrive/LLM/Bioreasoner/data/hf...</td>\n",
              "      <td>A</td>\n",
              "      <td>AF2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4c94b00-c953-4904-b96d-a6da88c6b2b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a4c94b00-c953-4904-b96d-a6da88c6b2b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a4c94b00-c953-4904-b96d-a6da88c6b2b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e3499c85-08d2-4248-b0e4-b9beb750c8ff\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3499c85-08d2-4248-b0e4-b9beb750c8ff')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e3499c85-08d2-4248-b0e4-b9beb750c8ff button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  uniprot_id                 pdb_id  \\\n",
              "0     A1K1S0  AF-A1K1S0-F1-model_v4   \n",
              "1     A3CXH4  AF-A3CXH4-F1-model_v4   \n",
              "2     P52409  AF-P52409-F1-model_v4   \n",
              "3     Q1CT83  AF-Q1CT83-F1-model_v4   \n",
              "4     Q2JL77  AF-Q2JL77-F1-model_v4   \n",
              "\n",
              "                                                file chains source  \n",
              "0  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...      A    AF2  \n",
              "1  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...      A    AF2  \n",
              "2  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...      A    AF2  \n",
              "3  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...      A    AF2  \n",
              "4  /content/drive/MyDrive/LLM/Bioreasoner/data/hf...      A    AF2  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === Fallback downloader: AlphaFold mmCIF by UniProt for missing entries ===\n",
        "import json, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reuse your existing paths/vars\n",
        "# BASE_DIR, OUT_DIR, STRUCTURE_HOME already defined above\n",
        "WORK_CSV        = OUT_DIR / \"worklist_5k.csv\"\n",
        "STRUCT_LOG_CSV  = OUT_DIR / \"structures_downloaded.csv\"\n",
        "AF2_LOG_CSV     = OUT_DIR / \"structures_downloaded_af2.csv\"\n",
        "AF2_MISSED_TXT  = OUT_DIR / \"structures_missing_af2.txt\"\n",
        "\n",
        "# AlphaFold file name templates (try in this order)\n",
        "AF2_CANDIDATES  = [\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v4.cif\",\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v3.cif\",\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v2.cif\",\n",
        "]\n",
        "\n",
        "TIMEOUT = 25\n",
        "PAUSE   = 0.02\n",
        "\n",
        "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def make_session():\n",
        "    s = requests.Session()\n",
        "    retries = Retry(\n",
        "        total=4, backoff_factor=0.5,\n",
        "        status_forcelist=[429,500,502,503,504],\n",
        "        allowed_methods=[\"GET\"]\n",
        "    )\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "session = make_session()\n",
        "\n",
        "def has_experimental_locally(uid: str) -> bool:\n",
        "    d = STRUCTURE_HOME / uid\n",
        "    if not d.exists(): return False\n",
        "    return any(p.suffix.lower()==\".cif\" and not p.name.startswith(\"AF-\") for p in d.glob(\"*.cif\"))\n",
        "\n",
        "def has_af2_locally(uid: str) -> bool:\n",
        "    d = STRUCTURE_HOME / uid\n",
        "    if not d.exists(): return False\n",
        "    return any(p.suffix.lower()==\".cif\" and p.name.startswith(\"AF-\") for p in d.glob(\"*.cif\"))\n",
        "\n",
        "def download_af2_cif(uniprot_id: str, out_dir: Path) -> Path | None:\n",
        "    \"\"\"\n",
        "    Try AF2 v4 -> v3 -> v2 for a UniProt (canonical; handle isoforms by stripping '-2').\n",
        "    Returns the saved Path or None.\n",
        "    \"\"\"\n",
        "    uid_base = uniprot_id.split(\"-\")[0]  # handle isoforms like P12345-2\n",
        "    ensure_dir(out_dir)\n",
        "    for url_tpl in AF2_CANDIDATES:\n",
        "        url = url_tpl.format(uid=uid_base)\n",
        "        fname = Path(url).name\n",
        "        out = out_dir / fname\n",
        "        try:\n",
        "            r = session.get(url, timeout=TIMEOUT)\n",
        "            if r.status_code == 200 and r.content and len(r.content) > 1024:\n",
        "                out.write_bytes(r.content)\n",
        "                return out\n",
        "        except Exception:\n",
        "            pass\n",
        "        time.sleep(PAUSE)\n",
        "    return None\n",
        "\n",
        "# ---- Load the worklist and figure out who is missing any CIFs\n",
        "df = pd.read_csv(WORK_CSV)\n",
        "uids = df[\"uniprot_id\"].astype(str).tolist()\n",
        "\n",
        "# Experimental log (if exists)\n",
        "exp_log = pd.read_csv(STRUCT_LOG_CSV) if STRUCT_LOG_CSV.exists() else pd.DataFrame(columns=[\"uniprot_id\",\"pdb_id\",\"file\",\"chains\"])\n",
        "have_exp = set(exp_log[\"uniprot_id\"].astype(str)) if len(exp_log) else set()\n",
        "\n",
        "# Already-downloaded AF2 log (resume-safe)\n",
        "af2_log = pd.read_csv(AF2_LOG_CSV) if AF2_LOG_CSV.exists() else pd.DataFrame(columns=[\"uniprot_id\",\"pdb_id\",\"file\",\"chains\",\"source\"])\n",
        "have_af2 = set(af2_log[\"uniprot_id\"].astype(str)) if len(af2_log) else set()\n",
        "\n",
        "to_fetch = [u for u in uids if u not in have_exp and u not in have_af2 and not has_af2_locally(u)]\n",
        "print(f\"Need AF2 for (no experimental + not already AF2): {len(to_fetch)}\")\n",
        "\n",
        "af2_records = []\n",
        "af2_missed  = []\n",
        "\n",
        "for uid in tqdm(to_fetch, desc=\"AF2 CIF download\"):\n",
        "    out_dir = STRUCTURE_HOME / uid\n",
        "    saved = download_af2_cif(uid, out_dir)\n",
        "    if saved and saved.exists() and saved.stat().st_size > 1024:\n",
        "        af2_records.append({\n",
        "            \"uniprot_id\": uid,\n",
        "            \"pdb_id\": Path(saved).stem,     # 'AF-<UID>-F1-model_vX'\n",
        "            \"file\": str(saved),\n",
        "            \"chains\": \"A\",                  # AF2 is single chain\n",
        "            \"source\": \"AF2\"\n",
        "        })\n",
        "    else:\n",
        "        af2_missed.append(uid)\n",
        "\n",
        "# Append/Save AF2 log (dedupe; resume-safe)\n",
        "if af2_records:\n",
        "    df_new = pd.DataFrame(af2_records)\n",
        "    if AF2_LOG_CSV.exists():\n",
        "        old = pd.read_csv(AF2_LOG_CSV)\n",
        "        df_out = pd.concat([old, df_new], ignore_index=True)\n",
        "    else:\n",
        "        df_out = df_new\n",
        "    df_out = df_out.drop_duplicates(subset=[\"uniprot_id\",\"pdb_id\"]).reset_index(drop=True)\n",
        "    df_out.to_csv(AF2_LOG_CSV, index=False)\n",
        "\n",
        "# Write missed list\n",
        "Path(AF2_MISSED_TXT).write_text(\"\\n\".join(sorted(set(af2_missed))), encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\nAF2 summary:\")\n",
        "print(\"  New AF2 files:\", len(af2_records))\n",
        "print(\"  Total AF2 log rows:\", (pd.read_csv(AF2_LOG_CSV).shape[0] if AF2_LOG_CSV.exists() else 0))\n",
        "print(\"  Missed AF2:\", len(af2_missed))\n",
        "print(\"AF2 log:\", AF2_LOG_CSV)\n",
        "print(\"AF2 missed list:\", AF2_MISSED_TXT)\n",
        "\n",
        "# small preview\n",
        "if AF2_LOG_CSV.exists():\n",
        "    display(pd.read_csv(AF2_LOG_CSV).head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-ynOKfypL3oa",
      "metadata": {
        "id": "-ynOKfypL3oa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1oTnXOgcsBjK",
      "metadata": {
        "id": "1oTnXOgcsBjK"
      },
      "source": [
        "#### Error Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "suEGUfTwn_4R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suEGUfTwn_4R",
        "outputId": "e9f46ac6-15d5-495f-9d93-3638353577f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE_DIR      : /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\n",
            "STRUCTURE_HOME: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/downloads\n",
            "Exists BASE_DIR?       True\n",
            "Exists STRUCTURE_HOME? True\n",
            "Targets: 4967\n",
            "Have experimental mmCIF already: 0\n",
            "Still missing experimental mmCIF: 4967\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "print(\"BASE_DIR      :\", BASE_DIR)\n",
        "print(\"STRUCTURE_HOME:\", STRUCTURE_HOME)\n",
        "print(\"Exists BASE_DIR?      \", BASE_DIR.exists())\n",
        "print(\"Exists STRUCTURE_HOME?\", STRUCTURE_HOME.exists())\n",
        "\n",
        "WORK_CSV = OUT_DIR / \"worklist_5k.csv\"\n",
        "df = pd.read_csv(WORK_CSV)\n",
        "\n",
        "def has_exp(uid: str) -> bool:\n",
        "    d = STRUCTURE_HOME / uid\n",
        "    return d.exists() and any(p.suffix.lower()==\".cif\" and not p.name.startswith(\"AF-\") for p in d.glob(\"*.cif\"))\n",
        "\n",
        "df[\"has_exp\"] = df[\"uniprot_id\"].astype(str).apply(has_exp)\n",
        "print(\"Targets:\", len(df))\n",
        "print(\"Have experimental mmCIF already:\", int(df[\"has_exp\"].sum()))\n",
        "print(\"Still missing experimental mmCIF:\", int((~df[\"has_exp\"]).sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "uXKmy2tnoCxl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXKmy2tnoCxl",
        "outputId": "31615afd-56bf-4c57-a8b0-ba1267e6718d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1K1S0 → no mapping\n",
            "A3CXH4 → no mapping\n",
            "Q9P7I4 → no mapping\n",
            "P52409 → no mapping\n",
            "Q1CT83 → no mapping\n",
            "Q2JL77 → no mapping\n",
            "Q8KCG7 → no mapping\n",
            "Q39IW5 → no mapping\n",
            "Q9LZX8 → no mapping\n",
            "D6W1X9 → no mapping\n",
            "D6VX92 → no mapping\n",
            "O86788 → no mapping\n",
            "Q9CPJ2 → no mapping\n",
            "C1CN25 → no mapping\n",
            "A1AE55 → no mapping\n",
            "B7LLT6 → no mapping\n",
            "B7GW85 → no mapping\n",
            "I7GFL2 → no mapping\n",
            "G0KA64 → no mapping\n",
            "Q2II86 → no mapping\n"
          ]
        }
      ],
      "source": [
        "import requests, time\n",
        "\n",
        "PDBe_MAP_URL = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot/{uid}\"\n",
        "TIMEOUT = 25\n",
        "\n",
        "def pdbe_map_uniprot(uid: str) -> dict:\n",
        "    try:\n",
        "        r = requests.get(PDBe_MAP_URL.format(uid=uid), timeout=TIMEOUT)\n",
        "        if r.status_code != 200:\n",
        "            return {}\n",
        "        payload = r.json()\n",
        "        out = {}\n",
        "        if isinstance(payload, dict) and uid in payload:\n",
        "            pdbs = payload[uid].get(\"PDB\") or {}\n",
        "            for pdb_id, lst in pdbs.items():\n",
        "                chains = sorted({ item.get(\"chain_id\") for item in lst if item.get(\"chain_id\") })\n",
        "                if chains:\n",
        "                    out[pdb_id.lower()] = chains\n",
        "        return out\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "sample = df.loc[~df[\"has_exp\"], \"uniprot_id\"].astype(str).head(20).tolist()\n",
        "for uid in sample:\n",
        "    m = pdbe_map_uniprot(uid)\n",
        "    print(uid, \"→\", f\"{len(m)} PDBs\" if m else \"no mapping\")\n",
        "    time.sleep(0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "MXX1_7WSoE_Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXX1_7WSoE_Y",
        "outputId": "acb1a7d8-95e4-43f3-974a-3c8333f80ebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDBs for P04637 : []\n",
            "No mapping for P04637\n"
          ]
        }
      ],
      "source": [
        "import requests, time, re\n",
        "from pathlib import Path\n",
        "\n",
        "TEST_UID = \"P04637\"  # replace with any UniProt from your 5k list if you prefer\n",
        "STRUCTURE_HOME.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def pdbe_map(uid):\n",
        "    url = f\"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot/{uid}\"\n",
        "    r = requests.get(url, timeout=25)\n",
        "    if r.status_code != 200:\n",
        "        return {}\n",
        "    payload = r.json()\n",
        "    out = {}\n",
        "    if isinstance(payload, dict) and uid in payload:\n",
        "        pdbs = payload[uid].get(\"PDB\") or {}\n",
        "        for pdb_id, lst in pdbs.items():\n",
        "            chains = sorted({ item.get(\"chain_id\") for item in lst if item.get(\"chain_id\") })\n",
        "            if chains:\n",
        "                out[pdb_id.lower()] = chains\n",
        "    return out\n",
        "\n",
        "def download_cif(pdb_id, out_path):\n",
        "    url = f\"https://files.rcsb.org/download/{pdb_id.upper()}.cif\"\n",
        "    r = requests.get(url, timeout=25)\n",
        "    if r.status_code == 200 and len(r.content) > 1024:\n",
        "        out_path.write_bytes(r.content)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "m = pdbe_map(TEST_UID)\n",
        "print(\"PDBs for\", TEST_UID, \":\", list(m.keys())[:5])\n",
        "\n",
        "if m:\n",
        "    uid_dir = STRUCTURE_HOME / TEST_UID\n",
        "    ensure_dir(uid_dir)\n",
        "    pdb_id = sorted(m.keys())[0]\n",
        "    out = uid_dir / f\"{pdb_id}.cif\"\n",
        "    ok = download_cif(pdb_id, out)\n",
        "    print(\"Saved:\", out, \"OK:\", ok, \"size(MB):\", out.stat().st_size/1024/1024 if out.exists() else 0)\n",
        "else:\n",
        "    print(\"No mapping for\", TEST_UID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g8QcLk6TsGAH",
      "metadata": {
        "id": "g8QcLk6TsGAH"
      },
      "source": [
        "### mmCIF/PDB to Foldseek 3Di"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "TSsozn2_hOwe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "TSsozn2_hOwe",
        "outputId": "967b047a-0399-496b-d5ac-dabbe8601016"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Foldseek 3Di: 100%|██████████| 3866/3866 [07:06<00:00,  9.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved per-chain 3Di to: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/sft_build/foldseek_3di_per_chain.csv  (rows=0, proteins=0)\n",
            "Success: 0  |  skipped(missing/too small): 0  |  failed(parse/errors): 3866\n",
            "Errors logged to: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/sft_build/foldseek_3di_errors.log\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "Out of range float values are not JSON compliant: nan",
              "type": "dataframe",
              "variable_name": "df_3di"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9f15c3e2-3434-4fc1-b066-da4531fa73c3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniprot_id</th>\n",
              "      <th>pdb_id</th>\n",
              "      <th>chain</th>\n",
              "      <th>stru_str</th>\n",
              "      <th>aa_len</th>\n",
              "      <th>stru_len</th>\n",
              "      <th>source_file</th>\n",
              "      <th>source_kind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f15c3e2-3434-4fc1-b066-da4531fa73c3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f15c3e2-3434-4fc1-b066-da4531fa73c3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f15c3e2-3434-4fc1-b066-da4531fa73c3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [uniprot_id, pdb_id, chain, stru_str, aa_len, stru_len, source_file, source_kind]\n",
              "Index: []"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== Patched Cell 3 (robust): Convert mmCIF to Foldseek 3Di (experimental + AF2) =====\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# foldseek util from your repo\n",
        "from foldseek_util import get_struc_seq\n",
        "\n",
        "STRUCT_LOG_CSV    = OUT_DIR / \"structures_downloaded.csv\"      # experimental (if any)\n",
        "AF2_LOG_CSV       = OUT_DIR / \"structures_downloaded_af2.csv\"  # AF2 fallback\n",
        "PER_CHAIN_CSV     = OUT_DIR / \"foldseek_3di_per_chain.csv\"\n",
        "CACHE_DIR         = OUT_DIR / \"3di_cache\"\n",
        "ERR_LOG           = OUT_DIR / \"foldseek_3di_errors.log\"\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "assert FOLDSEEK_BIN.exists() and os.access(FOLDSEEK_BIN, os.X_OK), f\"Foldseek binary not executable at {FOLDSEEK_BIN}\"\n",
        "\n",
        "# Read both logs and unify\n",
        "dfs = []\n",
        "if STRUCT_LOG_CSV.exists():\n",
        "    dfs.append(pd.read_csv(STRUCT_LOG_CSV))\n",
        "if AF2_LOG_CSV.exists():\n",
        "    dfs.append(pd.read_csv(AF2_LOG_CSV))\n",
        "if dfs:\n",
        "    df_structs = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=[\"uniprot_id\",\"pdb_id\"])\n",
        "else:\n",
        "    df_structs = pd.DataFrame(columns=[\"uniprot_id\",\"pdb_id\",\"file\",\"chains\",\"source\"])\n",
        "\n",
        "def is_af2_row(row):\n",
        "    # prefer explicit 'source' column if present\n",
        "    s = row.get(\"source\") if isinstance(row, dict) else (row[\"source\"] if \"source\" in row else \"\")\n",
        "    if isinstance(s, str) and s.strip().upper() == \"AF2\":\n",
        "        return True\n",
        "    # fallback: filename starts with AF-\n",
        "    fname = Path(str(row[\"file\"])).name\n",
        "    return fname.startswith(\"AF-\")\n",
        "\n",
        "rows = []\n",
        "errors = []\n",
        "n_ok, n_skip, n_fail = 0, 0, 0\n",
        "\n",
        "for _, r in tqdm(df_structs.iterrows(), total=len(df_structs), desc=\"Foldseek 3Di\"):\n",
        "    uid   = str(r[\"uniprot_id\"])\n",
        "    pdbid = str(r[\"pdb_id\"]).lower()\n",
        "    path  = Path(r[\"file\"])\n",
        "\n",
        "    if not path.exists() or path.stat().st_size < 1024:\n",
        "        n_skip += 1\n",
        "        continue\n",
        "\n",
        "    af2 = is_af2_row(r)\n",
        "    # chains: prefer provided list; otherwise AF2 -> [\"A\"]; else fallback to [\"A\"]\n",
        "    raw_ch = str(r.get(\"chains\", \"\")) if \"chains\" in r else \"\"\n",
        "    chains = [c for c in raw_ch.split(\",\") if c] or [\"A\"]\n",
        "\n",
        "    for ch in chains:\n",
        "        cache_txt = CACHE_DIR / f\"{uid}_{pdbid}_{ch}.3di.txt\"\n",
        "        if cache_txt.exists():\n",
        "            fs = cache_txt.read_text(encoding=\"utf-8\").strip()\n",
        "            if fs:\n",
        "                rows.append({\n",
        "                    \"uniprot_id\": uid,\n",
        "                    \"pdb_id\": pdbid,\n",
        "                    \"chain\": ch,\n",
        "                    \"stru_str\": fs,\n",
        "                    \"aa_len\": None,\n",
        "                    \"stru_len\": len(fs),\n",
        "                    \"source_file\": str(path),\n",
        "                    \"source_kind\": (\"AF2\" if af2 else \"EXP\")\n",
        "                })\n",
        "                n_ok += 1\n",
        "            else:\n",
        "                n_fail += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # AF2: enable pLDDT masking; Experimental: no mask\n",
        "            parsed = get_struc_seq(str(FOLDSEEK_BIN), str(path), [ch], plddt_mask=af2)\n",
        "            if ch in parsed and parsed[ch] and len(parsed[ch]) >= 2:\n",
        "                aa_seq = parsed[ch][0] or \"\"\n",
        "                fs3di  = (parsed[ch][1] or \"\").lower()\n",
        "                if fs3di:\n",
        "                    cache_txt.write_text(fs3di, encoding=\"utf-8\")\n",
        "                    rows.append({\n",
        "                        \"uniprot_id\": uid,\n",
        "                        \"pdb_id\": pdbid,\n",
        "                        \"chain\": ch,\n",
        "                        \"stru_str\": fs3di,\n",
        "                        \"aa_len\": len(aa_seq),\n",
        "                        \"stru_len\": len(fs3di),\n",
        "                        \"source_file\": str(path),\n",
        "                        \"source_kind\": (\"AF2\" if af2 else \"EXP\")\n",
        "                    })\n",
        "                    n_ok += 1\n",
        "                else:\n",
        "                    n_fail += 1\n",
        "                    errors.append(f\"{uid}\\t{pdbid}\\t{ch}\\tempty_3di\")\n",
        "            else:\n",
        "                n_fail += 1\n",
        "                errors.append(f\"{uid}\\t{pdbid}\\t{ch}\\tno_parsed_chain\")\n",
        "        except Exception as e:\n",
        "            n_fail += 1\n",
        "            errors.append(f\"{uid}\\t{pdbid}\\t{ch}\\t{type(e).__name__}: {e}\")\n",
        "\n",
        "# Build DataFrame with explicit columns (so it never has 0 columns)\n",
        "cols = [\"uniprot_id\",\"pdb_id\",\"chain\",\"stru_str\",\"aa_len\",\"stru_len\",\"source_file\",\"source_kind\"]\n",
        "df_3di = pd.DataFrame(rows, columns=cols).drop_duplicates(subset=[\"uniprot_id\",\"pdb_id\",\"chain\"])\n",
        "df_3di.to_csv(PER_CHAIN_CSV, index=False)\n",
        "\n",
        "# Error log (optional)\n",
        "if errors:\n",
        "    ERR_LOG.write_text(\"\\n\".join(errors), encoding=\"utf-8\")\n",
        "\n",
        "proteins = df_3di[\"uniprot_id\"].nunique() if \"uniprot_id\" in df_3di.columns else 0\n",
        "print(f\"Saved per-chain 3Di to: {PER_CHAIN_CSV}  (rows={len(df_3di)}, proteins={proteins})\")\n",
        "print(f\"Success: {n_ok}  |  skipped(missing/too small): {n_skip}  |  failed(parse/errors): {n_fail}\")\n",
        "if errors:\n",
        "    print(f\"Errors logged to: {ERR_LOG}\")\n",
        "\n",
        "# quick peek\n",
        "df_3di.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ma6s7jZ4c7ek",
      "metadata": {
        "id": "ma6s7jZ4c7ek"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "VCty2wcJPLFI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCty2wcJPLFI",
        "outputId": "6c73b1c3-ef78-45a6-89aa-29f7de6ac7dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary exists & exec? True True\n",
            "Foldseek enables fast and sensitive comparisons of large structure sets. It reaches sensitivities similar to state-of-the-art structural aligners while being at least 20,000 times faster.\n",
            "\n",
            "Please cite:\n",
            "van Kempen, M., Kim, S.S., Tumescheit, C., Mirdita, M., Lee, J., Gilchrist, C.L.M., Söding, J., and Steinegger, M. Fast and accurate protein structure search with Foldseek. Nature Biotechnology, doi:10.1038/s41587-023-01773-0 (2023)\n",
            "\n",
            "foldseek Version: ef4e960ab84fc502665eb7b84573dfff9c2aa89d\n",
            "© Michel van Kempen, Stephanie Kim, Charlotte Tumescheit, Milot Mirdita, Jeongjae Lee, Cameron L. M. Gilchrist, Johannes Söding, Martin Steinegger\n",
            "\n",
            "usage: foldseek <command> [<args>]\n",
            "\n",
            "Easy workflows for plain text input/output\n",
            "  easy-search       \tStructual search\n",
            "  easy-cluster      \tSlower, sensitive clustering\n",
            "  easy-rbh          \tFind reciprocal best hit\n",
            "  easy-complexsearch\tComplex level search\n",
            "\n",
            "Main workflows for database input/output\n",
            "  createdb          \tConvert PDB/mmCIF/tar[.gz]/DB files to a db\n",
            "  search            \tSensitive homology search\n",
            "  rbh               \tReciprocal best hit search\n",
            "  cluster           \tSlower, sensitive clustering\n",
            "\n",
            "Input database creation\n",
            "  databases         \tList and download databases\n",
            "  createindex       \tStore precomputed index on disk to reduce search overhead\n",
            "  createclusearchdb \tBuild a searchable cluster database allowing for faster searches\n",
            "\n",
            "Format conversion for downstream processing\n",
            "  convertalis       \tConvert alignment DB to BLAST-tab, SAM or custom format\n",
            "  compressca        \tCreate a new compressed C-alpha DB with 16-bit diff encoding where possible from a sequence DB\n",
            "  convert2pdb       \tConvert a foldseek structure db to a multi model PDB file\n",
            "  createcomplexreport\tConvert complex DB to tsv format\"\n",
            "\n",
            "Alignment           \n",
            "  tmalign           \tCompute tm-score \n",
            "  structurealign    \tCompute structural alignment using 3Di alphabet, amino acids and neighborhood information\n",
            "  structurerescorediagonal\tCompute sequence identity for diagonal\n",
            "  aln2tmscore       \tCompute tmscore of an alignment database \n",
            "  scorecomplex      \tGet complex level alignments from alignmentDB\n",
            "\n",
            "Clustering          \n",
            "  clust             \tCluster result by Set-Cover/Connected-Component/Greedy-Incremental\n",
            "\n",
            "\n",
            "Invalid Command: -v\n",
            "Did you mean \"/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/bin/foldseek easy-cluster\"?\n",
            "\n",
            "Foldseek enables fast and sensitive comparisons of large structure sets. It reaches sensitivities similar to state-of-the-art structural aligners while being at least 20,000 times faster.\n",
            "\n",
            "Please cite:\n",
            "van Kempen, M., Kim, S.S., Tumescheit, C., Mirdita, M., Lee, J., Gilchrist, C.L.M., Söding, J., and Steinegger, M. Fast and accurate protein structure search with Foldseek. Nature Biotechnology, doi:10.1038/s41587-023-01773-0 (2023)\n",
            "\n",
            "foldseek Version: ef4e960ab84fc502665eb7b84573dfff9c2aa89d\n",
            "© Michel van Kempen, Stephanie Kim, Charlotte Tumescheit, Milot Mirdita, Jeongjae Lee, Cameron L. M. Gilchrist, Johannes Söding, Martin Steinegger\n",
            "\n",
            "usage: foldseek <command> [<args>]\n",
            "\n",
            "Easy workflows for plain text input/output\n",
            "  easy-search       \tStructual search\n",
            "  easy-cluster      \tSlower, sensitive clustering\n",
            "  easy-rbh          \tFind reciprocal best hit\n",
            "  easy-complexsearch\tComplex level search\n",
            "\n",
            "Main workflows for database input/output\n",
            "  createdb          \tConvert PDB/mmCIF/tar[.gz]/DB files to a db\n",
            "  search            \tSensitive homology search\n",
            "  rbh               \tReciprocal best hit search\n",
            "  cluster           \tSlower, sensitive clustering\n",
            "\n",
            "Input database creation\n",
            "  databases         \tList and download databases\n",
            "  createindex       \tStore precomputed index on disk to reduce search overhead\n",
            "  createclusearchdb \tBuild a searchable cluster database allowing for faster searches\n",
            "\n",
            "Format conversion for downstream processing\n",
            "  convertalis       \tConvert alignment DB to BLAST-tab, SAM or custom format\n",
            "  compressca        \tCreate a new compressed C-alpha DB with 16-bit diff encoding where possible from a sequence DB\n",
            "  convert2pdb       \tConvert a foldseek structure db to a multi model PDB file\n",
            "  createcomplexreport\tConvert complex DB to tsv format\"\n",
            "\n",
            "Alignment           \n",
            "  tmalign           \tCompute tm-score \n",
            "  structurealign    \tCompute structural alignment using 3Di alphabet, amino acids and neighborhood information\n",
            "  structurerescorediagonal\tCompute sequence identity for diagonal\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess, os\n",
        "print(\"Binary exists & exec?\", os.path.exists(str(FOLDSEEK_BIN)), os.access(str(FOLDSEEK_BIN), os.X_OK))\n",
        "print(subprocess.run([str(FOLDSEEK_BIN), \"-v\"], capture_output=True, text=True).stdout or \"no -v\")\n",
        "print(subprocess.run([str(FOLDSEEK_BIN), \"-h\"], capture_output=True, text=True).stdout[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "fjLFiZopgYgh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "fjLFiZopgYgh",
        "outputId": "93f3d93d-97d9-4cd9-bf80-323cab04c709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing: A1K1S0 | file: AF-A1K1S0-F1-model_v4.cif | exists: True | size(MB): 0.43\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'MET'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2942904514.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"| file:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"| exists:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"| size(MB):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_struc_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDSEEK_BIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplddt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chains returned:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AA len, 3Di len:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/foldseek_util.py\u001b[0m in \u001b[0;36mget_struc_seq\u001b[0;34m(foldseek, path, chains, process_id, plddt_mask, plddt_threshold, foldseek_verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Mask low plddt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mplddt_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mplddts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_plddt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplddts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruc_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Length mismatch: {len(plddts)} != {len(struc_seq)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/foldseek_util.py\u001b[0m in \u001b[0;36mextract_plddt\u001b[0;34m(pdb_path)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;31m# If position < 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;31m# If position >= 1000, the blank will be removed, e.g. \"A 999\" -> \"A1000\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'MET'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from foldseek_util import get_struc_seq\n",
        "\n",
        "AF2_LOG_CSV = OUT_DIR / \"structures_downloaded_af2.csv\"\n",
        "df_af2 = pd.read_csv(AF2_LOG_CSV)\n",
        "row = df_af2.iloc[0]  # pick one\n",
        "path = Path(row[\"file\"])\n",
        "uid  = row[\"uniprot_id\"]\n",
        "\n",
        "print(\"Testing:\", uid, \"| file:\", path.name, \"| exists:\", path.exists(), \"| size(MB):\", round(path.stat().st_size/1024/1024, 2))\n",
        "parsed = get_struc_seq(str(FOLDSEEK_BIN), str(path), [\"A\"], plddt_mask=True)\n",
        "print(\"Chains returned:\", list(parsed.keys()))\n",
        "print(\"AA len, 3Di len:\", len(parsed[\"A\"][0]), len(parsed[\"A\"][1]))\n",
        "print(\"3Di head:\", parsed[\"A\"][1][:60])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XWFvddtKsZiM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWFvddtKsZiM",
        "outputId": "bf5d4d39-1a6b-4d78-d4eb-f533d96ee9e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing AF2 PDB for A1K1S0: AF-A1K1S0-F1-model_v4.pdb | size(MB)=0.31\n",
            "Chains returned: ['A']\n",
            "AA len, 3Di len: 512 512\n",
            "3Di head: ###DPVVVVVVVVVCVVPDDP###DWWKWWFQDDDQQKTKTFTPPPDDQQFWKAWPPRWIWGFHDDDPGITIIGTQFDDP\n"
          ]
        }
      ],
      "source": [
        "# --- Test get_struc_seq using an AlphaFold **PDB** file (chain A) ---\n",
        "import pandas as pd, requests, time\n",
        "from pathlib import Path\n",
        "from foldseek_util import get_struc_seq\n",
        "\n",
        "# Paths (adjust BASE_DIR if needed)\n",
        "BASE_DIR       = Path(\"\")\n",
        "OUT_DIR        = BASE_DIR / \"sft_build\"\n",
        "STRUCTURE_HOME = BASE_DIR / \"downloads\"\n",
        "AF2_LOG_CSV    = OUT_DIR / \"structures_downloaded_af2.csv\"\n",
        "\n",
        "# Foldseek binary location\n",
        "try:\n",
        "    FOLDSEEK_BIN\n",
        "except NameError:\n",
        "    FOLDSEEK_BIN = Path(\"bin/foldseek\")  # e.g., Path(\"/usr/local/bin/foldseek\")\n",
        "\n",
        "AF2_PDB_URLS = [\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v4.pdb\",\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v3.pdb\",\n",
        "    \"https://alphafold.ebi.ac.uk/files/AF-{uid}-F1-model_v2.pdb\",\n",
        "]\n",
        "\n",
        "def dl_af2_pdb(uid: str, out_dir: Path) -> Path | None:\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    uid_base = uid.split(\"-\")[0]\n",
        "    for tpl in AF2_PDB_URLS:\n",
        "        url = tpl.format(uid=uid_base)\n",
        "        p = out_dir / Path(url).name\n",
        "        r = requests.get(url, timeout=25)\n",
        "        if r.status_code == 200 and len(r.content) > 1024:\n",
        "            p.write_bytes(r.content)\n",
        "            return p\n",
        "        time.sleep(0.05)\n",
        "    return None\n",
        "\n",
        "# Pick one AF2 entry you already logged\n",
        "df_af2 = pd.read_csv(AF2_LOG_CSV)\n",
        "uid = str(df_af2.iloc[0][\"uniprot_id\"])\n",
        "uid_dir = STRUCTURE_HOME / uid\n",
        "\n",
        "pdb_path = dl_af2_pdb(uid, uid_dir)\n",
        "assert pdb_path and pdb_path.exists(), f\"Could not download AF2 PDB for {uid}\"\n",
        "\n",
        "print(f\"Testing AF2 PDB for {uid}: {pdb_path.name} | size(MB)={pdb_path.stat().st_size/1024/1024:.2f}\")\n",
        "parsed = get_struc_seq(str(FOLDSEEK_BIN), str(pdb_path), [\"A\"], plddt_mask=True)\n",
        "\n",
        "print(\"Chains returned:\", list(parsed.keys()))\n",
        "aa, di = parsed[\"A\"][0], parsed[\"A\"][1]\n",
        "print(\"AA len, 3Di len:\", len(aa), len(di))\n",
        "print(\"3Di head:\", di[:80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yeG6acyshTvS",
      "metadata": {
        "id": "yeG6acyshTvS"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 4: Assemble SFT JSONL (prompt, response, aa_seq, stru_str) =====\n",
        "import json, math\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "WORK_CSV       = OUT_DIR / \"worklist_5k.csv\"                  # from Cell 1\n",
        "PER_CHAIN_CSV  = OUT_DIR / \"foldseek_3di_per_chain.csv\"       # from Cell 3\n",
        "JSONL_OUT      = OUT_DIR / \"sft_protein2desc_seq+3di.jsonl\"   # final SFT\n",
        "CHAIN_POLICY   = \"best\"    # choose: \"best\" | \"concat\" | \"all_items\"\n",
        "\n",
        "PROMPT_TEXT = (\n",
        "    \"You are a professional protein biologist. Based on the amino-acid sequence (and structure if available), \"\n",
        "    \"write a concise, biologically accurate 2–4 sentence description of the protein.\"\n",
        ")\n",
        "\n",
        "# Load sources\n",
        "df_work = pd.read_csv(WORK_CSV)              # uniprot_id, seq, response\n",
        "df_3di  = pd.read_csv(PER_CHAIN_CSV) if PER_CHAIN_CSV.exists() else pd.DataFrame(columns=[\"uniprot_id\",\"pdb_id\",\"chain\",\"stru_str\"])\n",
        "\n",
        "# Build per-UniProt aggregation of 3Di\n",
        "agg = {}\n",
        "for uid, g in df_3di.groupby(\"uniprot_id\"):\n",
        "    chains = []\n",
        "    for _, r in g.iterrows():\n",
        "        chains.append({\"pdb_id\": str(r[\"pdb_id\"]), \"chain\": str(r[\"chain\"]), \"stru_str\": str(r[\"stru_str\"])})\n",
        "    agg[uid] = chains\n",
        "\n",
        "# Assemble JSONL\n",
        "written = 0\n",
        "with open(JSONL_OUT, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for _, row in df_work.iterrows():\n",
        "        uid  = str(row[\"uniprot_id\"])\n",
        "        seq  = str(row[\"seq\"])\n",
        "        resp = str(row[\"response\"])\n",
        "        chains = agg.get(uid, [])\n",
        "\n",
        "        if CHAIN_POLICY == \"best\":\n",
        "            # choose longest stru_str (proxy for coverage)\n",
        "            stru_str = \"\"\n",
        "            if chains:\n",
        "                best = max(chains, key=lambda x: len(x[\"stru_str\"]))\n",
        "                stru_str = best[\"stru_str\"]\n",
        "            rec = {\n",
        "                \"prompt\": PROMPT_TEXT,\n",
        "                \"response\": resp,\n",
        "                \"aa_seq\": seq\n",
        "            }\n",
        "            if stru_str:\n",
        "                rec[\"stru_str\"] = stru_str\n",
        "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            written += 1\n",
        "\n",
        "        elif CHAIN_POLICY == \"concat\":\n",
        "            # concatenate all chain 3Di with single space separator\n",
        "            stru_str = \"\"\n",
        "            if chains:\n",
        "                stru_str = \" \".join([c[\"stru_str\"] for c in chains if c[\"stru_str\"]])\n",
        "            rec = {\n",
        "                \"prompt\": PROMPT_TEXT,\n",
        "                \"response\": resp,\n",
        "                \"aa_seq\": seq\n",
        "            }\n",
        "            if stru_str:\n",
        "                rec[\"stru_str\"] = stru_str\n",
        "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            written += 1\n",
        "\n",
        "        elif CHAIN_POLICY == \"all_items\":\n",
        "            # one JSONL line per (UniProt, chain)\n",
        "            if not chains:\n",
        "                # still write a sequence-only example\n",
        "                rec = {\n",
        "                    \"prompt\": PROMPT_TEXT,\n",
        "                    \"response\": resp,\n",
        "                    \"aa_seq\": seq\n",
        "                }\n",
        "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "                written += 1\n",
        "            else:\n",
        "                for c in chains:\n",
        "                    if not c[\"stru_str\"]:\n",
        "                        continue\n",
        "                    rec = {\n",
        "                        \"prompt\": PROMPT_TEXT,\n",
        "                        \"response\": resp,\n",
        "                        \"aa_seq\": seq,\n",
        "                        \"stru_str\": c[\"stru_str\"]\n",
        "                    }\n",
        "                    fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "                    written += 1\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown CHAIN_POLICY={CHAIN_POLICY}\")\n",
        "\n",
        "print(f\"JSONL saved: {JSONL_OUT}  |  lines written: {written}\")\n",
        "\n",
        "# Optional: for 'concat', also save a sidecar listing per-protein chain coverage\n",
        "if CHAIN_POLICY == \"concat\":\n",
        "    side = []\n",
        "    for uid, chains in agg.items():\n",
        "        side.append({\n",
        "            \"uniprot_id\": uid,\n",
        "            \"n_chains\": len(chains),\n",
        "            \"pdb_chain_list\": \";\".join([f\"{c['pdb_id']}:{c['chain']}({len(c['stru_str'])})\" for c in chains])\n",
        "        })\n",
        "    side_csv = OUT_DIR / \"concat_chain_boundaries.csv\"\n",
        "    pd.DataFrame(side).to_csv(side_csv, index=False)\n",
        "    print(\"Saved concat chain boundaries to:\", side_csv)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1oTnXOgcsBjK"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
