{"cells":[{"cell_type":"markdown","id":"11360105","metadata":{"id":"11360105"},"source":["\n","# BigProtein-Qwen2.5 — Step‑by‑Step Test Notebook (Colab)\n","This notebook lets you **test each component** of the protein‑conditioned Qwen2.5 pipeline *before* running full training.  \n","It mirrors the main script logic, but runs **function‑by‑function** so you can see errors early with clear tracebacks.\n","\n","> **Files expected in the working directory** (upload or mount a folder containing them):  \n","> - `bigmodel_joint_train.py`  \n","> - `protein_encoder.py`  \n","> - `structure_encoder.py`\n"]},{"cell_type":"code","execution_count":1,"id":"IUYulnhL8NM-","metadata":{"executionInfo":{"elapsed":687,"status":"ok","timestamp":1759417504119,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"IUYulnhL8NM-"},"outputs":[],"source":["#@title Mount Google Drive\n","from pathlib import Path\n","from huggingface_hub import snapshot_download\n","import os, json, pickle, pandas as pd\n","from tqdm import tqdm\n","from rich import print as rprint"]},{"cell_type":"code","execution_count":2,"id":"DxEz1S18Ys00","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxEz1S18Ys00","executionInfo":{"status":"ok","timestamp":1759417522703,"user_tz":240,"elapsed":17885,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"d82bf2dc-c718-49ee-ee88-a8448bd8203f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","Using Google Drive folder as BASE_DIR: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","%cd /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","\n","from pathlib import Path\n","BASE_DIR = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")\n","OUT_DIR  = BASE_DIR / \"sft_test_demo\"\n","print(f\"Using Google Drive folder as BASE_DIR: {BASE_DIR}\")\n"]},{"cell_type":"markdown","id":"ef586a49","metadata":{"id":"ef586a49"},"source":["\n","## 0) Runtime & Installs\n","If you're on Google Colab, run this cell to install dependencies.\n"]},{"cell_type":"code","execution_count":3,"id":"W8upz3dOoYqH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11616,"status":"ok","timestamp":1759417537360,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"W8upz3dOoYqH","outputId":"ec79b677-3657-4b34-a6c8-0c5dce236560"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Oct  2 15:05:25 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n","| N/A   31C    P0             49W /  400W |       0MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Check GPU\n","!nvidia-smi\n","\n","# Fresh pip + libs (PyTorch CUDA 12.1 build + matching libs)\n","%pip -q install --upgrade pip\n","%pip install -q --index-url https://download.pytorch.org/whl/cu126 \\\n","  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n","%pip -q install transformers==4.56.1 huggingface_hub==0.35.0 tqdm safetensors"]},{"cell_type":"code","execution_count":4,"id":"ap908Ts551uw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17216,"status":"ok","timestamp":1759417576508,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"ap908Ts551uw","outputId":"431c955d-7348-4b24-a52b-05ecd9313c5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch            : 2.8.0+cu126\n","transformers     : 4.56.1\n","huggingface_hub  : 0.35.0\n","✅ Top-level EsmForMaskedLM import OK\n"]}],"source":["# --- Version & import sanity checks ---\n","import torch, transformers, huggingface_hub\n","print(\"torch            :\", torch.__version__)\n","print(\"transformers     :\", transformers.__version__)\n","print(\"huggingface_hub  :\", huggingface_hub.__version__)\n","\n","# Top-level ESM import should work on 4.56.1\n","try:\n","    from transformers import AutoTokenizer, EsmForMaskedLM\n","    print(\"✅ Top-level EsmForMaskedLM import OK\")\n","except Exception as e:\n","    print(\"❌ Top-level EsmForMaskedLM import failed:\", repr(e))\n","    # Fallback check (direct module path)\n","    try:\n","        from transformers.models.esm.modeling_esm import EsmForMaskedLM as _E\n","        print(\"✅ Direct modeling_esm import OK (fallback)\")\n","    except Exception as ee:\n","        print(\"❌ Direct modeling_esm import failed too:\", repr(ee))"]},{"cell_type":"markdown","id":"f9G7pU_Rp4mF","metadata":{"id":"f9G7pU_Rp4mF"},"source":["torch            : 2.8.0+cu126\n","\n","transformers     : 4.56.1\n","\n","huggingface_hub  : 0.35.0\n","\n","✅ Top-level EsmForMaskedLM import OK"]},{"cell_type":"markdown","id":"3ZDFip2psPXw","metadata":{"id":"3ZDFip2psPXw"},"source":["\n","## 1) Loading Encoder Checkpoints"]},{"cell_type":"code","execution_count":5,"id":"fd2cdf79","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":867,"status":"ok","timestamp":1759417580910,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"fd2cdf79","outputId":"78d52117-14af-459d-b21b-85f81376495a"},"outputs":[{"output_type":"stream","name":"stdout","text":["✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/protein2desc_sft_ALLFOUR_c000-009_fullcot.jsonl\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test\n"]}],"source":["\n","# === LLM & Encoders ===\n","MODEL_NAME         = \"Qwen/Qwen2.5-3B-Instruct\"   # Small-ish for Colab testing\n","PROTEIN_CONFIG = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\"\n","STRUCTURE_CONFIG = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\"\n","PROTREK_CKPT    = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\"\n","PROJECT_DIR = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\"\n","DATA_JSONL = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/protein2desc_sft_ALLFOUR_c000-009_fullcot.jsonl\"\n","OUT_DIR = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test\"\n","\n","for p in [PROJECT_DIR, DATA_JSONL, PROTEIN_CONFIG, STRUCTURE_CONFIG, PROTREK_CKPT, OUT_DIR]:\n","    print(\"✓ exists:\", os.path.exists(p), p)\n","\n"]},{"cell_type":"code","execution_count":6,"id":"ET3YTyMxpKOS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1759417582902,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"ET3YTyMxpKOS","outputId":"915c34fe-2fd1-49ea-e88c-04951fca0be4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}],"source":["# === Prefix/Proj ===\n","SINGLE_TOKEN_PREFIX = False     # True -> 1 token; False -> soft prefix of length PREFIX_LEN\n","PREFIX_LEN          = 4\n","PROJ_HID            = 1024\n","DROPOUT             = 0.10\n","\n","# === Training toggles ===\n","USE_LORA            = False\n","TRAIN_ENCODERS      = False    # True = end-to-end; False = freeze encoders\n","FREEZE_PROTEIN      = False    # only used if TRAIN_ENCODERS=True\n","FREEZE_STRUCTURE    = False    # only used if TRAIN_ENCODERS=True\n","GRAD_CHECKPOINT     = False\n","\n","# === Misc ===\n","DEVICE              = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","MAX_LEN             = 512\n","BSZ                 = 2\n","ACCUM               = 1\n","LR                  = 5e-5\n","WARMUP_RATIO        = 0.03\n","EPOCHS              = 1\n","OUTPUT_DIR          = \"runs/colab_smoketest\"\n","LOG_EVERY           = 1\n","\n","print(\"Device:\", DEVICE)"]},{"cell_type":"code","execution_count":null,"id":"-Qib_xuxpMXY","metadata":{"id":"-Qib_xuxpMXY"},"outputs":[],"source":["# SUBSET_JSONL = os.path.join(PROJECT_DIR, \"train_subset_100.jsonl\")\n","\n","# # Write first 1000 non-empty lines to subset\n","# count = 0\n","# with open(DATA_JSONL, \"r\", encoding=\"utf-8\") as fin, open(SUBSET_JSONL, \"w\", encoding=\"utf-8\") as fout:\n","#     for line in fin:\n","#         if not line.strip():\n","#             continue\n","#         fout.write(line)\n","#         count += 1\n","#         if count >= 100:\n","#             break\n","\n","# print(\"Wrote subset lines:\", count, \"->\", SUBSET_JSONL)"]},{"cell_type":"code","execution_count":null,"id":"DLo3zwCqpNZY","metadata":{"id":"DLo3zwCqpNZY"},"outputs":[],"source":["from train_prefix_qwen import train, parse_args\n","SUBSET_JSONL = os.path.join(PROJECT_DIR, \"train_subset_100.jsonl\")"]},{"cell_type":"code","execution_count":null,"id":"416Reck1rGgv","metadata":{"id":"416Reck1rGgv"},"outputs":[],"source":["import types, os\n","\n","SAVE_DIR = os.path.join(PROJECT_DIR, \"runs_colab_test\")\n","\n","args = types.SimpleNamespace(\n","    # Data\n","    train_file   = DATA_JSONL,\n","    val_file     = None,\n","    batch_size   = 1,         # adjust if you want\n","    accum_steps  = 2,\n","    max_len      = 1024,       # keep modest for speed\n","    # Model\n","    model_name   = MODEL_NAME,\n","    dtype        = \"fp32\",    # or \"bf16\" on A100 for speed\n","    prefix_len   = 4,         # try 1 or 4+\n","    prefix_gate  = 0.5,       # stabilizer on the soft prefix\n","    learnable_gate = False,\n","    freeze_llm   = False,     # True = projector-only\n","    train_encoders = True,   # keep ESM encoders frozen for speed\n","    # Encoders\n","    protein_config = PROTEIN_CONFIG,\n","    structure_config = STRUCTURE_CONFIG,\n","    protrek_ckpt  = PROTREK_CKPT,\n","    prot_slot     = 1,\n","    stru_slot     = 3,\n","    # Optim\n","    epochs      = 3,\n","    lr          = 3e-5,       # projector+LLM small LR\n","    weight_decay= 0.05,\n","    # Save/eval\n","    save_dir    = OUT_DIR,\n","    save_every  = 1500,\n","    eval_every  = 0,\n","    # Misc\n","    seed        = 42,\n",")\n","\n","# Kick off training\n","train(args)"]},{"cell_type":"markdown","id":"e03789fa","metadata":{"id":"e03789fa"},"source":["\n","# 🔧 FSDP Test Add-on — Import & Unit-Test `train_prefix_qwen_fsdp.py`\n","\n","This section adds **unit tests** for the multi‑GPU training script so you can see **clear tracebacks** directly in Colab, cell-by-cell.\n","It **does not** spawn multi-process; it runs pieces in‑notebook to validate shapes, masking, and forward loss.\n","When you're ready for multi‑GPU, use `accelerate launch` externally.\n"]},{"cell_type":"markdown","id":"fce221fc","metadata":{"id":"fce221fc"},"source":["\n","## 0) Install/Check Dependencies\n","Run this if your Colab doesn't already have the right versions.\n"]},{"cell_type":"code","execution_count":7,"id":"5d3f58c9","metadata":{"executionInfo":{"elapsed":3718,"status":"ok","timestamp":1759417595828,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"5d3f58c9"},"outputs":[],"source":["\n","# If needed, upgrade pip\n","# %pip -q install --upgrade pip\n","\n","# Core deps (Torch build here is CUDA 12.1; adjust per your Colab runtime/GPU)\n","%pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","%pip -q install transformers>=4.43.0 peft accelerate datasets tqdm\n"]},{"cell_type":"markdown","id":"0af053c5","metadata":{"id":"0af053c5"},"source":["\n","## 1) Ensure files are present & on `sys.path`\n","Expected files in the current working directory:\n","- `protein_encoder.py`\n","- `structure_encoder.py`\n","- `train_prefix_qwen_fsdp.py`  ← new multi-GPU ready trainer\n"]},{"cell_type":"code","execution_count":8,"id":"jDxMbJh6c_I_","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1759417599081,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"jDxMbJh6c_I_","outputId":"e7cd9296-6079-43f6-ac20-60d30757f37c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["os.getcwd()"]},{"cell_type":"code","execution_count":9,"id":"861f65a6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1759417600834,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"861f65a6","outputId":"a1bb15c0-0d28-47a1-e3d3-d54be01aae60"},"outputs":[{"output_type":"stream","name":"stdout","text":["CWD: /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","Missing files: []\n","✅ All required files found.\n"]}],"source":["\n","import os, sys, glob, textwrap\n","\n","need = [\"protein_encoder.py\", \"structure_encoder.py\", \"train_prefix_qwen_fsdp.py\"]\n","missing = [p for p in need if not os.path.exists(p)]\n","print(\"CWD:\", os.getcwd())\n","print(\"Missing files:\", missing)\n","\n","# Add CWD to path for `from train_prefix_qwen_fsdp import ...`\n","if os.getcwd() not in sys.path:\n","    sys.path.insert(0, os.getcwd())\n","\n","if missing:\n","    print(\"\\n⚠️ Some files are missing. Upload them via the Colab file browser or mount Google Drive and %cd there.\")\n","else:\n","    print(\"✅ All required files found.\")\n"]},{"cell_type":"markdown","id":"3537e713","metadata":{"id":"3537e713"},"source":["\n","## 2) Import from `train_prefix_qwen_fsdp.py`\n","We import classes/functions so we can call them directly and see tracebacks.\n"]},{"cell_type":"code","execution_count":10,"id":"9e6ebb38","metadata":{"executionInfo":{"elapsed":2313,"status":"ok","timestamp":1759417606325,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"9e6ebb38","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2546af52-5dd3-48e3-c31a-133fc2b2a2e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imported: <class 'train_prefix_qwen_fsdp.BigProteinQwen'> <class 'train_prefix_qwen_fsdp.CollateCfg'> <class 'train_prefix_qwen_fsdp.PadAndMaskCollator'> <class 'train_prefix_qwen_fsdp.JsonlStream'> <function train at 0x7c18ffd01260>\n"]}],"source":["\n","from train_prefix_qwen_fsdp import (\n","    BigProteinQwen,\n","    CollateCfg,\n","    PadAndMaskCollator,\n","    JsonlStream,\n","    train as fsdp_train,\n",")\n","\n","print(\"Imported:\", BigProteinQwen, CollateCfg, PadAndMaskCollator, JsonlStream, fsdp_train)\n"]},{"cell_type":"markdown","id":"bcb98019","metadata":{"id":"bcb98019"},"source":["\n","## 3) Quick Configs (small models for unit tests)\n","Use a tiny Qwen and small ESM so everything runs fast on Colab.\n"]},{"cell_type":"code","execution_count":11,"id":"3c5cb0d9","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1759417609643,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"},"user_tz":240},"id":"3c5cb0d9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc5f3452-8869-4cf5-a0bf-da442e051dc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Configs ready.\n"]}],"source":["\n","MODEL_NAME       = \"Qwen/Qwen2.5-0.5B-Instruct\"     # small LLM for smoke tests\n","PROTEIN_CONFIG   = \"facebook/esm2_t12_35M_UR50D\"    # small ESM\n","STRUCTURE_CONFIG = \"facebook/esm2_t12_35M_UR50D\"\n","\n","SINGLE_TOKEN_PREFIX = False    # switch to True to test single-token prefix\n","PREFIX_LEN          = 4\n","PROJ_HID            = 1024\n","DROPOUT             = 0.10\n","\n","MAX_LEN    = 256\n","BATCH_SIZE = 2\n","\n","print(\"Configs ready.\")\n","\n","PROTREK_CKPT   = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\"\n","PROT_SLOT = 1\n","STRU_SLOT = 3\n"]},{"cell_type":"markdown","id":"d6c193be","metadata":{"id":"d6c193be"},"source":["\n","## 4) Create Tiny JSONL Data (seq-only and seq+structure)\n"]},{"cell_type":"code","execution_count":17,"id":"c0923522","metadata":{"id":"c0923522","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759417776974,"user_tz":240,"elapsed":44,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"63414ed1-02cf-41bc-e7ef-76f07a6880eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote sft_data/train_tiny.jsonl & val_tiny.jsonl\n"]}],"source":["\n","import json, os\n","os.makedirs(\"sft_data\", exist_ok=True)\n","\n","toy = [\n","    {\n","        \"prompt\": \"Describe the likely function of this protein.\",\n","        \"response\": \"This appears to be an enzyme with possible hydrolase activity.\",\n","        \"aa_seq\": \"MKTFFVAIATGAFSATA\",\n","        \"stru_str\": \"ACDEFGHIKLMNPQRSTVWY\"\n","    },\n","    {\n","        \"prompt\": \"What domain might this protein contain?\",\n","        \"response\": \"Likely contains a Rossmann-like fold domain.\",\n","        \"aa_seq\": \"MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNLHGLFGRKTGQAP\",\n","        \"stru_str\": \"ACDEFGHIKLMNPQRSTVWY\"\n","    }\n","]\n","\n","with open(\"sft_data/train_tiny.jsonl\", \"w\", encoding=\"utf-8\") as f:\n","    for ex in toy:\n","        f.write(json.dumps(ex) + \"\\n\")\n","\n","with open(\"sft_data/val_tiny.jsonl\", \"w\", encoding=\"utf-8\") as f:\n","    for ex in toy:\n","        f.write(json.dumps(ex) + \"\\n\")\n","\n","print(\"Wrote sft_data/train_tiny.jsonl & val_tiny.jsonl\")\n"]},{"cell_type":"markdown","id":"65e8b29e","metadata":{"id":"65e8b29e"},"source":["\n","## 5) Collator sanity check (masking & shapes)\n"]},{"cell_type":"code","execution_count":18,"id":"3d1739b3","metadata":{"id":"3d1739b3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759417778714,"user_tz":240,"elapsed":598,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"93208539-2ce6-4df0-e80b-373fdc780ebe"},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["input_ids (2, 24) torch.int64\n","attention_mask (2, 24) torch.int64\n","labels (2, 24) torch.int64\n","aa_seq <class 'list'> 2\n","stru_str <class 'list'> 2\n","Sample labels (first 40): [-100, -100, -100, -100, -100, -100, -100, -100, -100, 1986, 7952, 311, 387, 458, 48142, 448, 3204, 6275, 67, 1080, 519, 5702, 13, 151645]\n"]}],"source":["\n","from transformers import AutoTokenizer\n","import torch\n","\n","tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","collate = PadAndMaskCollator(CollateCfg(tokenizer=tok, max_len=MAX_LEN))\n","\n","# Load two rows manually (not streaming) to see clear errors if any\n","rows = []\n","with open(\"sft_data/train_tiny.jsonl\", \"r\", encoding=\"utf-8\") as f:\n","    for i, line in enumerate(f):\n","        if i >= 2: break\n","        rows.append(json.loads(line))\n","\n","batch = collate(rows)\n","for k, v in batch.items():\n","    if isinstance(v, torch.Tensor):\n","        print(k, tuple(v.shape), v.dtype)\n","    else:\n","        if isinstance(v, list):\n","            print(k, type(v), len(v))\n","        else:\n","            print(k, type(v))\n","\n","print(\"Sample labels (first 40):\", batch[\"labels\"][0][:40].tolist())\n"]},{"cell_type":"markdown","id":"7dd6f4d1","metadata":{"id":"7dd6f4d1"},"source":["\n","## 6) Model forward pass (no FSDP wrapping here; pure unit test)\n"]},{"cell_type":"code","execution_count":19,"id":"2b52b292","metadata":{"id":"2b52b292","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1759417857057,"user_tz":240,"elapsed":3018,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"c18af856-9e66-42fc-e71c-912edfd5dfa1"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 must have the same dtype, but got BFloat16 and Float","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1757814812.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Forward OK. loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/train_prefix_qwen_fsdp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels, aa_seq, stru_str)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Build protein prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mprot_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_protein_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstru_str\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 1024/2048]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mpref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprot_vec\u001b[0m\u001b[0;34m)\u001b[0m                      \u001b[0;31m# [B, P, H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;31m# Concat prefix + text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/train_prefix_qwen_fsdp.py\u001b[0m in \u001b[0;36mbuild_prefix\u001b[0;34m(self, protein_vec)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprotein_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mpref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_2048\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_vec\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T, H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mprotein_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mpref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_1024\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_vec\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T, H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/train_prefix_qwen_fsdp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, protein_vec)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotein_vec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# protein_vec: [B, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_vec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B, T*H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got BFloat16 and Float"]}],"source":["\n","import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","big = BigProteinQwen(\n","    model_name=MODEL_NAME,\n","    protein_config=PROTEIN_CONFIG,\n","    structure_config=STRUCTURE_CONFIG,\n","    protrek_ckpt=PROTREK_CKPT,\n","    prot_slot=PROT_SLOT,\n","    stru_slot=STRU_SLOT,\n","    single_token_prefix=SINGLE_TOKEN_PREFIX,\n","    prefix_len=PREFIX_LEN,\n","    proj_hid=PROJ_HID,\n","    dropout=DROPOUT,\n","    train_encoders=True,      # freeze encoders in quick test\n",").to(device)\n","\n","for k in (\"input_ids\",\"attention_mask\",\"labels\"):\n","    batch[k] = batch[k].to(device)\n","\n","with torch.no_grad():\n","    out = big(**batch)\n","print(\"Forward OK. loss:\", float(out.loss.detach().cpu()))\n"]},{"cell_type":"markdown","id":"e44a12d5","metadata":{"id":"e44a12d5"},"source":["\n","## 7) One optimizer step (unit test)\n"]},{"cell_type":"code","execution_count":null,"id":"9750b2f7","metadata":{"id":"9750b2f7"},"outputs":[],"source":["\n","import torch\n","params = [p for p in big.parameters() if p.requires_grad]\n","opt = torch.optim.AdamW(params, lr=5e-5)\n","\n","out = big(**batch)\n","loss = out.loss\n","loss.backward()\n","torch.nn.utils.clip_grad_norm_(params, 1.0)\n","opt.step(); opt.zero_grad(set_to_none=True)\n","print(\"1 step OK. loss:\", float(loss.detach().cpu()))\n"]},{"cell_type":"markdown","id":"2af07d30","metadata":{"id":"2af07d30"},"source":["\n","## 8) End-to-end smoke test of `train()` on tiny data (single process)\n","This calls `train()` directly (so you can see tracebacks in‑notebook). It uses Accelerate but **won't** spawn multi‑process here.\n","For real multi‑GPU runs, use `accelerate launch` outside the notebook.\n"]},{"cell_type":"code","execution_count":null,"id":"d0b03c22","metadata":{"id":"d0b03c22"},"outputs":[],"source":["\n","from argparse import Namespace\n","\n","args = Namespace(\n","    train_file=\"sft_data/train_tiny.jsonl\",\n","    val_file=\"sft_data/val_tiny.jsonl\",\n","    max_len=MAX_LEN,\n","    batch_size=1,\n","    accum_steps=2,\n","    model_name=MODEL_NAME,\n","    protein_config=PROTEIN_CONFIG,\n","    structure_config=STRUCTURE_CONFIG,\n","    single_token_prefix=SINGLE_TOKEN_PREFIX,\n","    prefix_len=PREFIX_LEN,\n","    proj_hid=PROJ_HID,\n","    dropout=DROPOUT,\n","    protrek_ckpt=PROTREK_CKPT,\n","    prot_slot=PROT_SLOT,\n","    stru_slot=STRU_SLOT,\n","    train_encoders=False,\n","    epochs=1,\n","    lr=5e-5,\n","    warmup_ratio=0.03,\n","    weight_decay=0.05,\n","    save_dir=\"./runs_colab_fsdp_smoke\",\n","    save_every=0,\n","    log_every=1,\n","    seed=42,\n","    dtype=\"bf16\",\n",")\n","\n","# This will run a tiny epoch in the notebook so you can see errors inline.\n","# It's a quick sanity check before using accelerate launch with multiple GPUs.\n","fsdp_train(args)\n","print(\"train() smoke test complete.\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}