{"cells":[{"cell_type":"markdown","id":"11360105","metadata":{"id":"11360105"},"source":["\n","# BigProtein-Qwen2.5 — Step‑by‑Step Test Notebook (Colab)\n","This notebook lets you **test each component** of the protein‑conditioned Qwen2.5 pipeline *before* running full training.  \n","It mirrors the main script logic, but runs **function‑by‑function** so you can see errors early with clear tracebacks.\n","\n","> **Files expected in the working directory** (upload or mount a folder containing them):  \n","> - `bigmodel_joint_train.py`  \n","> - `protein_encoder.py`  \n","> - `structure_encoder.py`\n"]},{"cell_type":"code","source":["#@title Mount Google Drive\n","from pathlib import Path\n","from huggingface_hub import snapshot_download\n","import os, json, pickle, pandas as pd\n","from tqdm import tqdm\n","from rich import print as rprint"],"metadata":{"id":"IUYulnhL8NM-","executionInfo":{"status":"ok","timestamp":1759374637302,"user_tz":240,"elapsed":537,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"id":"IUYulnhL8NM-","execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","%cd /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","\n","from pathlib import Path\n","BASE_DIR = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")\n","OUT_DIR  = BASE_DIR / \"sft_test_demo\"\n","print(f\"Using Google Drive folder as BASE_DIR: {BASE_DIR}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxEz1S18Ys00","executionInfo":{"status":"ok","timestamp":1759374640467,"user_tz":240,"elapsed":3163,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"7431e7b4-4b66-46f8-d411-8c813f4c6768"},"id":"DxEz1S18Ys00","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","Using Google Drive folder as BASE_DIR: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\n"]}]},{"cell_type":"markdown","id":"ef586a49","metadata":{"id":"ef586a49"},"source":["\n","## 0) Runtime & Installs\n","If you're on Google Colab, run this cell to install dependencies.\n"]},{"cell_type":"code","source":["# Check GPU\n","!nvidia-smi\n","\n","# Fresh pip + libs (PyTorch CUDA 12.1 build + matching libs)\n","%pip -q install --upgrade pip\n","%pip install -q --index-url https://download.pytorch.org/whl/cu126 \\\n","  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n","%pip -q install transformers==4.56.1 huggingface_hub==0.35.0 tqdm safetensors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8upz3dOoYqH","executionInfo":{"status":"ok","timestamp":1759374648016,"user_tz":240,"elapsed":7548,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"fcd3d948-77ca-451d-8513-0df570c9e856"},"id":"W8upz3dOoYqH","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Oct  2 03:10:40 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n","| N/A   37C    P0             56W /  400W |       0MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# --- Version & import sanity checks ---\n","import torch, transformers, huggingface_hub\n","print(\"torch            :\", torch.__version__)\n","print(\"transformers     :\", transformers.__version__)\n","print(\"huggingface_hub  :\", huggingface_hub.__version__)\n","\n","# Top-level ESM import should work on 4.56.1\n","try:\n","    from transformers import AutoTokenizer, EsmForMaskedLM\n","    print(\"✅ Top-level EsmForMaskedLM import OK\")\n","except Exception as e:\n","    print(\"❌ Top-level EsmForMaskedLM import failed:\", repr(e))\n","    # Fallback check (direct module path)\n","    try:\n","        from transformers.models.esm.modeling_esm import EsmForMaskedLM as _E\n","        print(\"✅ Direct modeling_esm import OK (fallback)\")\n","    except Exception as ee:\n","        print(\"❌ Direct modeling_esm import failed too:\", repr(ee))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ap908Ts551uw","executionInfo":{"status":"ok","timestamp":1759374657189,"user_tz":240,"elapsed":9172,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"219fbd39-f440-42ab-cb94-e36eb47917fa"},"id":"ap908Ts551uw","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch            : 2.8.0+cu126\n","transformers     : 4.56.1\n","huggingface_hub  : 0.35.0\n","✅ Top-level EsmForMaskedLM import OK\n"]}]},{"cell_type":"markdown","source":["torch            : 2.8.0+cu126\n","\n","transformers     : 4.56.1\n","\n","huggingface_hub  : 0.35.0\n","\n","✅ Top-level EsmForMaskedLM import OK"],"metadata":{"id":"f9G7pU_Rp4mF"},"id":"f9G7pU_Rp4mF"},{"cell_type":"markdown","source":["\n","## 1) Loading Encoder Checkpoints"],"metadata":{"id":"3ZDFip2psPXw"},"id":"3ZDFip2psPXw"},{"cell_type":"code","execution_count":5,"id":"fd2cdf79","metadata":{"id":"fd2cdf79","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759374657216,"user_tz":240,"elapsed":24,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"e81d4491-5895-45fd-96a8-98e20cb62a50"},"outputs":[{"output_type":"stream","name":"stdout","text":["✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/protein2desc_sft_ALLFOUR_c000-009_fullcot.jsonl\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\n","✓ exists: True /content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test\n"]}],"source":["\n","# === LLM & Encoders ===\n","MODEL_NAME         = \"Qwen/Qwen2.5-0.5B-Instruct\"   # Small-ish for Colab testing\n","PROTEIN_CONFIG = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D\"\n","STRUCTURE_CONFIG = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M\"\n","PROTREK_CKPT    = \"/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt\"\n","PROJECT_DIR = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines\"\n","DATA_JSONL = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/protein2desc_sft_ALLFOUR_c000-009_fullcot.jsonl\"\n","OUT_DIR = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test\"\n","\n","for p in [PROJECT_DIR, DATA_JSONL, PROTEIN_CONFIG, STRUCTURE_CONFIG, PROTREK_CKPT, OUT_DIR]:\n","    print(\"✓ exists:\", os.path.exists(p), p)\n","\n"]},{"cell_type":"code","source":["# === Prefix/Proj ===\n","SINGLE_TOKEN_PREFIX = False     # True -> 1 token; False -> soft prefix of length PREFIX_LEN\n","PREFIX_LEN          = 4\n","PROJ_HID            = 1024\n","DROPOUT             = 0.10\n","\n","# === Training toggles ===\n","USE_LORA            = False\n","TRAIN_ENCODERS      = False    # True = end-to-end; False = freeze encoders\n","FREEZE_PROTEIN      = False    # only used if TRAIN_ENCODERS=True\n","FREEZE_STRUCTURE    = False    # only used if TRAIN_ENCODERS=True\n","GRAD_CHECKPOINT     = False\n","\n","# === Misc ===\n","DEVICE              = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","MAX_LEN             = 512\n","BSZ                 = 2\n","ACCUM               = 1\n","LR                  = 5e-5\n","WARMUP_RATIO        = 0.03\n","EPOCHS              = 1\n","OUTPUT_DIR          = \"runs/colab_smoketest\"\n","LOG_EVERY           = 1\n","\n","print(\"Device:\", DEVICE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ET3YTyMxpKOS","executionInfo":{"status":"ok","timestamp":1759374657220,"user_tz":240,"elapsed":3,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"3e8d7a90-be21-4cee-bb99-ed729196aff6"},"id":"ET3YTyMxpKOS","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["SUBSET_JSONL = os.path.join(PROJECT_DIR, \"train_subset_100.jsonl\")\n","\n","# Write first 1000 non-empty lines to subset\n","count = 0\n","with open(DATA_JSONL, \"r\", encoding=\"utf-8\") as fin, open(SUBSET_JSONL, \"w\", encoding=\"utf-8\") as fout:\n","    for line in fin:\n","        if not line.strip():\n","            continue\n","        fout.write(line)\n","        count += 1\n","        if count >= 100:\n","            break\n","\n","print(\"Wrote subset lines:\", count, \"->\", SUBSET_JSONL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Qib_xuxpMXY","executionInfo":{"status":"ok","timestamp":1759372408051,"user_tz":240,"elapsed":3,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"6b679940-21ea-48ec-94a9-f224527b7d0a"},"id":"-Qib_xuxpMXY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote subset lines: 100 -> /content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/train_subset_100.jsonl\n"]}]},{"cell_type":"code","source":["from train_prefix_qwen import train, parse_args\n","SUBSET_JSONL = os.path.join(PROJECT_DIR, \"train_subset_100.jsonl\")"],"metadata":{"id":"DLo3zwCqpNZY","executionInfo":{"status":"ok","timestamp":1759374808509,"user_tz":240,"elapsed":151288,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"id":"DLo3zwCqpNZY","execution_count":7,"outputs":[]},{"cell_type":"code","source":["import types, os\n","\n","SAVE_DIR = os.path.join(PROJECT_DIR, \"runs_colab_test\")\n","\n","args = types.SimpleNamespace(\n","    # Data\n","    train_file   = SUBSET_JSONL,\n","    val_file     = None,\n","    batch_size   = 4,         # adjust if you want\n","    accum_steps  = 1,\n","    max_len      = 2560,       # keep modest for speed\n","    # Model\n","    model_name   = \"Qwen/Qwen2.5-0.5B-Instruct\",\n","    dtype        = \"fp32\",    # or \"bf16\" on A100 for speed\n","    prefix_len   = 4,         # try 1 or 4+\n","    prefix_gate  = 1.0,       # stabilizer on the soft prefix\n","    learnable_gate = False,\n","    freeze_llm   = False,     # True = projector-only\n","    train_encoders = True,   # keep ESM encoders frozen for speed\n","    # Encoders\n","    protein_config = PROTEIN_CONFIG,\n","    structure_config = STRUCTURE_CONFIG,\n","    protrek_ckpt  = PROTREK_CKPT,\n","    prot_slot     = 1,\n","    stru_slot     = 3,\n","    # Optim\n","    epochs      = 2,\n","    lr          = 1e-3,       # projector+LLM small LR\n","    weight_decay= 0.0,\n","    # Save/eval\n","    save_dir    = OUT_DIR,\n","    save_every  = 0,\n","    eval_every  = 0,\n","    # Misc\n","    seed        = 42,\n",")\n","\n","# Kick off training\n","train(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"416Reck1rGgv","executionInfo":{"status":"ok","timestamp":1759374628453,"user_tz":240,"elapsed":54270,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"fffcb96b-5c79-4154-d837-17364c2fcbb4"},"id":"416Reck1rGgv","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loaded LLM: Qwen/Qwen2.5-0.5B-Instruct | hidden_size=896 | dtype=torch.float32\n","[ProteinEncoder] loaded from slot 1 | missing=0 unexpected=0\n","[StructureEncoder] loaded from slot 3 | missing=0 unexpected=0\n","Finished epoch 1. Elapsed 16s\n","[ep 2] step 50 | loss=3.4017 | supervised_tokens=36468 | time=32s\n","Finished epoch 2. Elapsed 32s\n","Saved final checkpoint to /content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test/final.pt\n"]}]},{"cell_type":"code","source":["import os, json, pprint, torch\n","\n","# Point this to your latest checkpoint\n","CKPT = \"/content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test/final.pt\"  # <-- change if needed\n","\n","sd = torch.load(CKPT, map_location=\"cpu\")\n","print(\"final.pt keys:\", list(sd.keys()))\n","print(\"global/final step:\", sd.get(\"final_step\"))\n","print(\"\\nargs:\")\n","pprint.pprint(sd.get(\"args\"), indent=2)\n","\n","opt_state = sd.get(\"optimizer\", {})\n","print(\"\\nOptimizer param groups:\", len(opt_state.get(\"param_groups\", [])))\n","num_states = sum(len(v) for v in opt_state.get(\"state\", {}).values())\n","print(\"Total tensors tracked by optimizer state:\", num_states)\n","\n","# Encoders presence (None if not trained or not saved)\n","enc = sd.get(\"encoders\", {})\n","print(\"\\nEncoders saved?\")\n","print(\"  protein :\", \"yes\" if (isinstance(enc, dict) and enc.get(\"protein\") is not None) else \"no\")\n","print(\"  structure:\", \"yes\" if (isinstance(enc, dict) and enc.get(\"structure\") is not None) else \"no\")"],"metadata":{"id":"29A2jFtLt6e6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759374814403,"user_tz":240,"elapsed":5901,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"d02e6b1e-e772-4504-c99e-d8a495168009"},"id":"29A2jFtLt6e6","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["final.pt keys: ['projector', 'llm', 'encoders', 'prefix_gate', 'optimizer', 'args', 'final_step']\n","global/final step: 50\n","\n","args:\n","{ 'accum_steps': 1,\n","  'batch_size': 4,\n","  'dtype': 'fp32',\n","  'epochs': 2,\n","  'eval_every': 0,\n","  'freeze_llm': False,\n","  'learnable_gate': False,\n","  'lr': 0.001,\n","  'max_len': 2560,\n","  'model_name': 'Qwen/Qwen2.5-0.5B-Instruct',\n","  'prefix_gate': 1.0,\n","  'prefix_len': 4,\n","  'prot_slot': 1,\n","  'protein_config': '/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/esm2_t12_35M_UR50D',\n","  'protrek_ckpt': '/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/ProTrek_35M.pt',\n","  'save_dir': '/content/drive/MyDrive/LLM/Bioreasoner/testing_notebooks/runs_colab_test',\n","  'save_every': 0,\n","  'seed': 42,\n","  'stru_slot': 3,\n","  'structure_config': '/content/drive/MyDrive/LLM/Bioreasoner/protrek/weights/ProTrek_35M/foldseek_t12_35M',\n","  'train_encoders': True,\n","  'train_file': '/content/drive/MyDrive/LLM/Bioreasoner/testing_pipelines/train_subset_100.jsonl',\n","  'val_file': None,\n","  'weight_decay': 0.0}\n","\n","Optimizer param groups: 1\n","Total tensors tracked by optimizer state: 2064\n","\n","Encoders saved?\n","  protein : yes\n","  structure: yes\n"]}]},{"cell_type":"code","source":["# === Cell 2: Parameter delta checks vs fresh ===\n","import torch\n","import torch.nn as nn\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","sd   = torch.load(CKPT, map_location=\"cpu\")\n","args = sd[\"args\"]\n","\n","# Map dtype\n","DTYPE = {\"fp32\": torch.float32, \"bf16\": torch.bfloat16, \"fp16\": torch.float16}[args[\"dtype\"]]\n","MODEL = args[\"model_name\"]\n","P     = args[\"prefix_len\"]\n","\n","# --- Helpers ---\n","def compare_modules(m0, m1, name, topk=10):\n","    total, changed = 0, 0\n","    rel = []\n","    for (n0,p0),(n1,p1) in zip(m0.state_dict().items(), m1.state_dict().items()):\n","        assert n0 == n1, f\"name mismatch: {n0} vs {n1}\"\n","        a0 = p0.detach().to(torch.float32).view(-1)\n","        a1 = p1.detach().to(torch.float32).view(-1)\n","        total += a0.numel()\n","        diff = (a1 - a0)\n","        rel.append((n0, float(diff.norm() / (a0.norm() + 1e-12))))\n","        changed += int((~torch.isclose(a0, a1)).sum().item())\n","    print(f\"[{name}] total_elems={total:,}  approx_changed_frac={changed/total:.4f}\")\n","    print(f\"[{name}] top-{topk} by relative Δ:\")\n","    for n, r in sorted(rel, key=lambda x: -x[1])[:topk]:\n","        print(f\"  {n:55s}  relΔ={r:.6f}\")\n","\n","# --- LLM (base vs trained) ---\n","tok = AutoTokenizer.from_pretrained(MODEL)\n","if tok.pad_token is None: tok.pad_token = tok.eos_token\n","base_llm = AutoModelForCausalLM.from_pretrained(MODEL, dtype=DTYPE)\n","H = base_llm.config.hidden_size\n","print(f\"LLM hidden={H}, dtype={DTYPE}\")\n","\n","trained_llm_state = sd.get(\"llm\")\n","if trained_llm_state is None:\n","    print(\"\\nLLM was frozen (no llm weights saved). Skipping LLM delta.\")\n","else:\n","    trained_llm = AutoModelForCausalLM.from_pretrained(MODEL, dtype=DTYPE)\n","    missing, unexpected = trained_llm.load_state_dict(trained_llm_state, strict=False)\n","    print(\"\\nLoaded trained LLM. missing:\", len(missing), \"unexpected:\", len(unexpected))\n","    compare_modules(base_llm, trained_llm, \"llm\", topk=15)\n","\n","# --- Projector (fresh-arch vs trained) ---\n","def make_projector(hid=H, d_in=2048, P=P, dtype=DTYPE):\n","    m = nn.Sequential(\n","        nn.Linear(d_in, hid),\n","        nn.SiLU(),\n","        nn.Linear(hid, hid * P),\n","    )\n","    return m.to(dtype=torch.float32)  # store in fp32 for clean delta math\n","\n","base_proj    = make_projector()\n","trained_proj = make_projector()\n","trained_proj.load_state_dict(sd[\"projector\"], strict=True)\n","print(\"\\n== Projector delta vs fresh init ==\")\n","compare_modules(base_proj, trained_proj, \"projector\", topk=10)\n","\n","# --- Encoders (only if saved) ---\n","enc_sd = sd.get(\"encoders\", {})\n","if isinstance(enc_sd, dict) and (enc_sd.get(\"protein\") is not None or enc_sd.get(\"structure\") is not None):\n","    import protein_encoder as protein_encoder_mod\n","    import structure_encoder as structure_encoder_mod\n","\n","    # Fresh (random-init) encoders from configs\n","    base_prot = protein_encoder_mod.ProteinEncoder(args[\"protein_config\"], out_dim=1024, load_pretrained=False)\n","    base_stru = structure_encoder_mod.StructureEncoder(args[\"structure_config\"], out_dim=1024, load_pretrained=False)\n","\n","    if enc_sd.get(\"protein\") is not None:\n","        trained_prot = protein_encoder_mod.ProteinEncoder(args[\"protein_config\"], out_dim=1024, load_pretrained=False)\n","        trained_prot.load_state_dict(enc_sd[\"protein\"], strict=False)\n","        print(\"\\n== ProteinEncoder delta vs fresh init ==\")\n","        compare_modules(base_prot, trained_prot, \"protein_encoder\", topk=10)\n","    else:\n","        print(\"\\nProteinEncoder not saved (probably not trained).\")\n","\n","    if enc_sd.get(\"structure\") is not None:\n","        trained_stru = structure_encoder_mod.StructureEncoder(args[\"structure_config\"], out_dim=1024, load_pretrained=False)\n","        trained_stru.load_state_dict(enc_sd[\"structure\"], strict=False)\n","        print(\"\\n== StructureEncoder delta vs fresh init ==\")\n","        compare_modules(base_stru, trained_stru, \"structure_encoder\", topk=10)\n","    else:\n","        print(\"\\nStructureEncoder not saved (probably not trained).\")\n","else:\n","    print(\"\\nNo encoders saved in checkpoint (encoders likely frozen or not saved).\")"],"metadata":{"id":"awXYZOZTuJ7n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759374829592,"user_tz":240,"elapsed":15176,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"157e0b6c-cbd0-413f-eb6b-8640fae132a5"},"id":"awXYZOZTuJ7n","execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["LLM hidden=896, dtype=torch.float32\n","\n","Loaded trained LLM. missing: 0 unexpected: 0\n","[llm] total_elems=630,167,424  approx_changed_frac=1.0000\n","[llm] top-15 by relative Δ:\n","  model.layers.0.self_attn.v_proj.weight                   relΔ=0.546569\n","  model.layers.2.self_attn.v_proj.weight                   relΔ=0.538066\n","  model.layers.1.self_attn.v_proj.weight                   relΔ=0.522997\n","  model.layers.0.self_attn.o_proj.weight                   relΔ=0.459119\n","  model.layers.0.self_attn.v_proj.bias                     relΔ=0.421852\n","  model.embed_tokens.weight                                relΔ=0.414175\n","  lm_head.weight                                           relΔ=0.414175\n","  model.layers.1.self_attn.o_proj.weight                   relΔ=0.399209\n","  model.layers.2.self_attn.o_proj.weight                   relΔ=0.395096\n","  model.layers.3.self_attn.v_proj.weight                   relΔ=0.371228\n","  model.layers.6.self_attn.v_proj.weight                   relΔ=0.366560\n","  model.layers.8.self_attn.v_proj.weight                   relΔ=0.366251\n","  model.layers.3.self_attn.o_proj.weight                   relΔ=0.364602\n","  model.layers.5.mlp.down_proj.weight                      relΔ=0.356670\n","  model.layers.1.mlp.down_proj.weight                      relΔ=0.355228\n","\n","== Projector delta vs fresh init ==\n","[projector] total_elems=5,050,752  approx_changed_frac=1.0000\n","[projector] top-10 by relative Δ:\n","  0.bias                                                   relΔ=1.493953\n","  0.weight                                                 relΔ=1.485239\n","  2.bias                                                   relΔ=1.451782\n","  2.weight                                                 relΔ=1.447169\n","\n","== ProteinEncoder delta vs fresh init ==\n","[protein_encoder] total_elems=34,009,681  approx_changed_frac=1.0000\n","[protein_encoder] top-10 by relative Δ:\n","  model.lm_head.bias                                       relΔ=35723218518016.000000\n","  model.esm.encoder.layer.11.attention.self.key.bias       relΔ=27300659724288.000000\n","  model.esm.encoder.layer.10.attention.self.key.bias       relΔ=24839393902592.000000\n","  model.esm.encoder.layer.8.attention.self.key.bias        relΔ=24127341592576.000000\n","  model.esm.encoder.layer.7.attention.self.key.bias        relΔ=21107159597056.000000\n","  model.esm.encoder.layer.9.attention.self.key.bias        relΔ=19084106268672.000000\n","  model.esm.encoder.layer.6.attention.self.key.bias        relΔ=17199043444736.000000\n","  model.esm.encoder.layer.3.attention.self.key.bias        relΔ=16549821808640.000000\n","  model.esm.encoder.layer.4.attention.self.key.bias        relΔ=15480689524736.000000\n","  model.esm.encoder.layer.5.attention.self.key.bias        relΔ=14109490806784.000000\n","\n","== StructureEncoder delta vs fresh init ==\n","[structure_encoder] total_elems=34,002,954  approx_changed_frac=1.0000\n","[structure_encoder] top-10 by relative Δ:\n","  model.lm_head.bias                                       relΔ=8691029377024.000000\n","  model.esm.encoder.layer.11.attention.self.key.bias       relΔ=5328171696128.000000\n","  model.esm.encoder.layer.1.attention.self.key.bias        relΔ=2645376106496.000000\n","  model.esm.encoder.layer.10.attention.self.key.bias       relΔ=2563069706240.000000\n","  model.esm.encoder.layer.5.attention.self.key.bias        relΔ=2128049078272.000000\n","  model.esm.encoder.layer.9.attention.self.key.bias        relΔ=2084317691904.000000\n","  model.esm.encoder.layer.2.attention.self.key.bias        relΔ=2062745993216.000000\n","  model.esm.encoder.layer.0.attention.self.key.bias        relΔ=1818476544000.000000\n","  model.lm_head.layer_norm.bias                            relΔ=1727311642624.000000\n","  model.esm.encoder.layer.7.attention.self.key.bias        relΔ=1633719025664.000000\n"]}]},{"cell_type":"code","source":["# === Cell 3: Functional loss check (baseline zero prefix vs trained protein prefix) ===\n","import os, json, itertools, torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import protein_encoder as protein_encoder_mod\n","import structure_encoder as structure_encoder_mod\n","import torch.nn as nn\n","\n","sd   = torch.load(CKPT, map_location=\"cpu\")\n","args = sd[\"args\"]\n","\n","MODEL = args[\"model_name\"]\n","P     = args[\"prefix_len\"]\n","DTYPE = {\"fp32\": torch.float32, \"bf16\": torch.bfloat16, \"fp16\": torch.float16}[args[\"dtype\"]]\n","\n","# Small eval slice (use your train jsonl or a held-out jsonl)\n","EVAL_JSONL = args[\"train_file\"]  # or a separate val file path\n","N_EVAL     = 100\n","\n","examples = []\n","with open(EVAL_JSONL, \"r\", encoding=\"utf-8\") as f:\n","    for line in itertools.islice((l for l in f if l.strip()), N_EVAL):\n","        examples.append(json.loads(line))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Tokenizer & LLM (trained weights if present)\n","tok = AutoTokenizer.from_pretrained(MODEL)\n","if tok.pad_token is None: tok.pad_token = tok.eos_token\n","\n","llm = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=DTYPE).to(device)\n","if sd.get(\"llm\") is not None:\n","    llm.load_state_dict(sd[\"llm\"], strict=False)\n","llm.eval()\n","we = llm.get_input_embeddings()\n","H  = llm.config.hidden_size\n","\n","# Projector (trained)\n","def make_projector(hid=H, d_in=2048, P=P):\n","    return nn.Sequential(\n","        nn.Linear(d_in, hid),\n","        nn.SiLU(),\n","        nn.Linear(hid, hid * P),\n","    )\n","\n","projector = make_projector().to(device, dtype=DTYPE)\n","projector.load_state_dict(sd[\"projector\"], strict=True)\n","projector.eval()\n","\n","# Gate\n","pg = sd.get(\"prefix_gate\")\n","if isinstance(pg, torch.Tensor):\n","    prefix_gate = pg.to(device=device, dtype=DTYPE)\n","else:\n","    prefix_gate = torch.tensor(float(pg) if pg is not None else 1.0, device=device, dtype=DTYPE)\n","\n","# Encoders: load base from configs, then overlay trained encoders if saved\n","prot_enc = protein_encoder_mod.ProteinEncoder(args[\"protein_config\"], out_dim=1024, load_pretrained=False).eval()\n","stru_enc = structure_encoder_mod.StructureEncoder(args[\"structure_config\"], out_dim=1024, load_pretrained=False).eval()\n","enc_sd = sd.get(\"encoders\", {})\n","if isinstance(enc_sd, dict):\n","    if enc_sd.get(\"protein\") is not None:\n","        prot_enc.load_state_dict(enc_sd[\"protein\"], strict=False)\n","    if enc_sd.get(\"structure\") is not None:\n","        stru_enc.load_state_dict(enc_sd[\"structure\"], strict=False)\n","\n","# Simple batch builder for eval (text only)\n","def build_text_batch(batch, max_len=256):\n","    prompts   = [b[\"prompt\"] for b in batch]\n","    responses = [b[\"response\"] for b in batch]\n","    enc_p = tok(prompts, add_special_tokens=False)\n","    enc_r = tok([r + tok.eos_token for r in responses], add_special_tokens=False)\n","\n","    ids_list, prompt_lens = [], []\n","    T_max = 0\n","    for i in range(len(batch)):\n","        ids_p = enc_p[\"input_ids\"][i]\n","        ids_r = enc_r[\"input_ids\"][i]\n","        ids   = (ids_p + ids_r)[: max_len]\n","        ids_list.append(ids)\n","        p_keep = min(len(ids_p), len(ids))\n","        prompt_lens.append(p_keep)\n","        T_max = max(T_max, len(ids))\n","\n","    pad_id = tok.pad_token_id\n","    input_ids = torch.full((len(batch), T_max), pad_id, dtype=torch.long, device=device)\n","    attn_mask = torch.zeros(len(batch), T_max, dtype=torch.long, device=device)\n","    labels    = torch.full((len(batch), T_max), -100, dtype=torch.long, device=device)\n","    for i, ids in enumerate(ids_list):\n","        t = len(ids)\n","        input_ids[i, :t] = torch.tensor(ids, dtype=torch.long, device=device)\n","        attn_mask[i, :t] = 1\n","        L = [-100]*prompt_lens[i] + ids[prompt_lens[i]:]\n","        labels[i, :t]    = torch.tensor(L, dtype=torch.long, device=device)\n","    return input_ids, attn_mask, labels\n","\n","@torch.no_grad()\n","def eval_split(examples, with_protein: bool, batch_size=8, max_len=256):\n","    total, count = 0.0, 0\n","    for i in range(0, len(examples), batch_size):\n","        batch = examples[i:i+batch_size]\n","        input_ids, attn_t, labels_t = build_text_batch(batch, max_len=max_len)\n","        B, T = input_ids.shape\n","\n","        text_emb = we(input_ids).to(DTYPE)\n","\n","        if with_protein:\n","            # compute protein vectors on CPU, cast to device\n","            aa_list   = [b.get(\"aa_seq\") for b in batch]\n","            stru_list = [b.get(\"stru_str\") for b in batch]\n","            # Encode only non-empty items; rebuild to Bx2048\n","            def encode_list(enc, xs):\n","                idxs = [k for k,x in enumerate(xs) if x]\n","                vecs = None\n","                if idxs:\n","                    seqs = [xs[k] for k in idxs]\n","                    vecs = enc.get_repr(seqs, batch_size=max(1,len(seqs)), verbose=False).cpu()  # (n,1024)\n","                full = torch.zeros(len(xs), 1024, dtype=torch.float32)\n","                if idxs:\n","                    for j,k in enumerate(idxs): full[k] = vecs[j]\n","                return full\n","            prot = encode_list(prot_enc, aa_list)\n","            stru = encode_list(stru_enc, stru_list)\n","            pvec = torch.cat([prot, stru], dim=1).to(device).to(DTYPE)  # (B,2048)\n","        else:\n","            pvec = torch.zeros(B, 2048, dtype=torch.float32, device=device).to(DTYPE)\n","\n","        pref = projector(pvec).view(B, P, H) * prefix_gate\n","        inputs = torch.cat([pref, text_emb], dim=1)\n","        attn   = torch.cat([torch.ones(B, P, dtype=torch.long, device=device), attn_t], dim=1)\n","        labs   = torch.cat([torch.full((B, P), -100, dtype=torch.long, device=device), labels_t], dim=1)\n","\n","        out  = llm(inputs_embeds=inputs, attention_mask=attn, use_cache=False)\n","        log  = out.logits[:, :-1, :].contiguous().float()\n","        lab  = labs[:, 1:].contiguous()\n","        loss = F.cross_entropy(log.view(-1, log.size(-1)), lab.view(-1), ignore_index=-100, reduction=\"mean\")\n","        total += float(loss.detach().cpu()); count += 1\n","    return total / max(count, 1)\n","\n","loss_zero   = eval_split(examples, with_protein=False, batch_size=8, max_len=256)\n","loss_trained= eval_split(examples, with_protein=True,  batch_size=8, max_len=256)\n","print(f\"Baseline (zero prefix) loss:  {loss_zero:.4f}\")\n","print(f\"Trained  (protein prefix) loss: {loss_trained:.4f}  (lower is better)\")"],"metadata":{"id":"pff1ejQyuLwx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759374883208,"user_tz":240,"elapsed":53594,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"7f704caf-8539-47d4-da0b-c86d7963031c"},"id":"pff1ejQyuLwx","execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"stream","name":"stdout","text":["Baseline (zero prefix) loss:  6.4565\n","Trained  (protein prefix) loss: 6.4557  (lower is better)\n"]}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}