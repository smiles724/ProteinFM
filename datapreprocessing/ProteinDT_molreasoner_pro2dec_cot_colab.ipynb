{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f147bba",
      "metadata": {
        "id": "8f147bba"
      },
      "source": [
        "\n",
        "# ProteinDT (Hugging Face) — CoT Generation\n",
        "\n",
        "- Sample dataset\n",
        "- Generate CoT (Using GPT) with groud truth (Large Scale)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vhtz2HvcGy8p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhtz2HvcGy8p",
        "outputId": "085b459c-1f7d-4845-c740-8f7989a77391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Using Google Drive folder as BASE_DIR: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive\n",
        "USE_DRIVE = True\n",
        "DRIVE_SUBDIR = \"hf/proteinDT\"\n",
        "BASE_DIR = \"\"\n",
        "\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    from pathlib import Path as _Path\n",
        "    _drive_base = _Path('')\n",
        "    BASE_DIR = _drive_base / DRIVE_SUBDIR\n",
        "    #c.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Using Google Drive folder as BASE_DIR: {BASE_DIR}\")\n",
        "else:\n",
        "    print(f\"Using local runtime folder as BASE_DIR: {BASE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9805eb43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9805eb43",
        "outputId": "24463382-75bf-45ec-fc05-48108e141044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "bigframes 2.21.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Install dependencies\n",
        "!pip -q install --upgrade huggingface_hub datasets pandas biopython lmdb tqdm rich\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Re2Kn7qJXUCW",
      "metadata": {
        "id": "Re2Kn7qJXUCW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from huggingface_hub import snapshot_download\n",
        "import os, json, lmdb, pickle, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rich import print as rprint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m3BvbawAOWNr",
      "metadata": {
        "id": "m3BvbawAOWNr"
      },
      "source": [
        "## Generate CoT (1k Demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "SDFUPlHROuqP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDFUPlHROuqP",
        "outputId": "938466a6-80ed-4a59-b467-5e47ba487a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.108.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.69.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.69.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.69.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install anthropic\n",
        "import asyncio\n",
        "import json, os, re, tempfile, time, math\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Iterable\n",
        "from openai import OpenAI\n",
        "import torch\n",
        "import openai\n",
        "import abc\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vrLli0NHOvRV",
      "metadata": {
        "id": "vrLli0NHOvRV"
      },
      "outputs": [],
      "source": [
        "# Set the required environment variables.\n",
        "#os.environ[\"ANTHROPIC_API_KEY\"] =\n",
        "#os.environ[\"MISTRAL_API_KEY\"] =\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "hWcjsinAOyZk",
      "metadata": {
        "id": "hWcjsinAOyZk"
      },
      "outputs": [],
      "source": [
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "#anthropic_client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "OPENAI_POLL_INTERVAL = 5\n",
        "ANTHROPIC_POLL_INTERVAL = 5\n",
        "OPENAI_CHUNK_SIZE = 300\n",
        "ANTHROPIC_MAX_BATCH = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RAOFApf3sjFG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAOFApf3sjFG",
        "outputId": "5176e4cc-3d46-4db2-b9e9-b46cd1fa609d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BASE_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "QFl7gkqDRTRO",
      "metadata": {
        "id": "QFl7gkqDRTRO"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# STEP 3: Fetch & build SFT\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "from pathlib import Path\n",
        "from types import SimpleNamespace as NS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8Gfx6s1rXd",
      "metadata": {
        "id": "_F8Gfx6s1rXd"
      },
      "source": [
        "### Generate CoT with ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PWuahTWDChMZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWuahTWDChMZ",
        "outputId": "1d827c4e-177c-4572-9334-69d75910a1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[System example used] UniProt: A9M1F6\n",
            "Wrote batch input: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_input_20250925-212446.jsonl\n",
            "Wrote meta map:   /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_meta_20250925-212446.json\n",
            "\n",
            "=== SUBMITTED BATCH ===\n",
            "batch_id:       batch_68d5b32372048190b56269225499429d\n",
            "input_file_id:  file-PjazxKv8wTA3dimHQM2Eco\n",
            "status:         validating\n",
            "(Saved to /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_batch_info_20250925-212446.json)\n",
            "\n",
            "When the batch finishes, your existing fetch/merge (loose parser) cell will work as-is.\n",
            "It will recover the real system prompt as `instruction`, and parse <thinking>/<answer> from model outputs.\n"
          ]
        }
      ],
      "source": [
        "# ===================== GPT Batch: Fixed-Answer (model fills ONLY <thinking>) =====================\n",
        "# The SYSTEM prompt stays as your PROTEIN_PROMPT_WITH_EXAMPLE.\n",
        "# The USER message includes the exact <answer> (ground truth) and asks the model to fill ONLY <thinking>.\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "import os, json, re, html, random, time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Required ---\n",
        "os.environ.setdefault(\"OPENAI_API_KEY\", \"<PUT_YOUR_KEY_HERE>\")  # or set in env beforehand\n",
        "BASE_DIR         = Path(\"\")  # ProteinDT root\n",
        "MODEL            = \"gpt-4o\"\n",
        "N_SAMPLES        = 100\n",
        "TEMPERATURE      = 0.1\n",
        "MAX_TOKENS       = 10000   # thinking + (copied) answer; thinking is the only part model must generate\n",
        "BATCH_COMPLETION_WINDOW = \"24h\"\n",
        "RANDOM_SEED      = 42\n",
        "\n",
        "# --- Optional: reuse your 1k subset manifest ---\n",
        "MANIFEST_PATH    = BASE_DIR / \"subset_1k\" / \"subset_manifest_1k.csv\"  # or set to None\n",
        "MANIFEST_LIMIT   = 0  # 0 = no extra cap\n",
        "\n",
        "# --- Outputs (new folder for this variant) ---\n",
        "OUT_DIR            = BASE_DIR / \"gpt_batch_protein2desc_fixed_answer\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RUN_TAG            = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "BATCH_INPUT_JSONL  = OUT_DIR / f\"protein2desc_fixedans_input_{RUN_TAG}.jsonl\"\n",
        "BATCH_META_JSON    = OUT_DIR / f\"protein2desc_fixedans_meta_{RUN_TAG}.json\"\n",
        "BATCH_INFO_JSON    = OUT_DIR / f\"protein2desc_fixedans_batch_info_{RUN_TAG}.json\"\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# ---------------- utils ----------------\n",
        "def strip_html(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = html.unescape(s)\n",
        "    return re.sub(r\"<[^>]+>\", \" \", s).replace(\"\\xa0\",\" \").strip()\n",
        "\n",
        "def trunc(s: str, n: int) -> str:\n",
        "    s = s or \"\"\n",
        "    return (s[:n] + \"…\") if len(s) > n else s\n",
        "\n",
        "def two_line_pairs(path: Path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        while True:\n",
        "            id_line = f.readline()\n",
        "            if not id_line: break\n",
        "            val_line = f.readline()\n",
        "            if not val_line: break\n",
        "            yield id_line.strip(), val_line.strip()\n",
        "\n",
        "def load_swissprotclap(base_dir: Path):\n",
        "    sp = base_dir / \"SwissProtCLAP\"\n",
        "    seq_fp, txt_fp = sp / \"protein_sequence.txt\", sp / \"text_sequence.txt\"\n",
        "    assert seq_fp.exists() and txt_fp.exists(), f\"Missing SwissProtCLAP under {sp}\"\n",
        "    id2seq = {pid: seq for pid, seq in two_line_pairs(seq_fp)}\n",
        "    id2cap = {pid: strip_html(txt) for pid, txt in two_line_pairs(txt_fp)}\n",
        "    ids = [i for i in id2seq.keys() if i in id2cap]\n",
        "    return ids, id2seq, id2cap\n",
        "\n",
        "# ---------------- your SYSTEM prompt (unchanged) ----------------\n",
        "# Use exactly your string (paste/keep your existing definition here).\n",
        "def make_system_prompt_with_example(ex_uid: str, ex_seq: str, ex_gt: str) -> str:\n",
        "    return (\n",
        "        \"You are a professional protein biologist. Your task is to generate a natural, concise, and biologically \"\n",
        "        \"accurate description of a protein based **only** on its amino-acid sequence.\\n\\n\"\n",
        "        \"### Example:\\n\"\n",
        "        f\"Protein UniProt: {ex_uid}\\n\"\n",
        "        f\"Protein sequence: {trunc(ex_seq, 1200)}\\n\"\n",
        "        f\"Answer: <answer> {ex_gt} </answer>\\n\\n\"\n",
        "        \"### Now try this:\\n\"\n",
        "        \"Given ONLY the sequence, first think step by step about plausible features (signal peptides, transmembrane \"\n",
        "        \"helices, repeats, low-complexity segments, catalytic motifs/domains, localization signals), then produce a \"\n",
        "        \"polished 2–4 sentence description.\\n\\n\"\n",
        "        \"Your final answer **must** be returned in the format:\\n\"\n",
        "        \"<thinking>\\n[5 - 10 steps: Your reasoning steps using sequence-derived evidence]\\n</thinking>\\n\\n\"\n",
        "        \"<answer>\\n[2–4 sentences; A natural, concise scientific description of the protein]\\n</answer>\\n\"\n",
        "    )\n",
        "\n",
        "# --------------- Build messages where USER supplies the exact <answer> ---------------\n",
        "def build_messages_fill_thinking_fixed_answer(system_prompt: str, uniprot_id: str, seq: str, ground_truth: str):\n",
        "    \"\"\"\n",
        "    We give the exact <answer> block to the model and ask it to generate ONLY the <thinking> block,\n",
        "    then output both blocks with the answer copied verbatim.\n",
        "    \"\"\"\n",
        "    user = (\n",
        "        f\"Protein UniProt: {uniprot_id}\\n\"\n",
        "        f\"Protein sequence (truncated to 2000 aa if long):\\n{trunc(seq, 2000)}\\n\\n\"\n",
        "        #\"Fill ONLY the <thinking> block using sequence-derived evidence. \"\n",
        "        #\"Then output BOTH blocks exactly as below, keeping the <answer> text EXACTLY as provided.\\n\\n\"\n",
        "        \"<thinking>\\n[Your reasoning steps using sequence-derived evidence]\\n</thinking>\\n\\n\"\n",
        "        \"<answer>\\n\"\n",
        "        f\"{ground_truth}\\n\"\n",
        "        \"</answer>\"\n",
        "    )\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user},\n",
        "    ]\n",
        "\n",
        "# ---------------- Build & submit batch ----------------\n",
        "random.seed(RANDOM_SEED)\n",
        "ids, id2seq, id2cap = load_swissprotclap(BASE_DIR)\n",
        "\n",
        "# optional restriction to manifest subset\n",
        "restrict_ids = None\n",
        "if MANIFEST_PATH and MANIFEST_PATH.exists():\n",
        "    import pandas as pd\n",
        "    dfm = pd.read_csv(MANIFEST_PATH)\n",
        "    restrict_ids = [str(x) for x in dfm[\"uniprot_id\"].astype(str).tolist()]\n",
        "    if MANIFEST_LIMIT and MANIFEST_LIMIT > 0:\n",
        "        restrict_ids = restrict_ids[:MANIFEST_LIMIT]\n",
        "\n",
        "pool = restrict_ids if restrict_ids else ids[:]\n",
        "random.shuffle(pool)\n",
        "\n",
        "# Example for system prompt\n",
        "ex_uid = pool[0]\n",
        "ex_seq = id2seq[ex_uid]\n",
        "ex_gt  = id2cap[ex_uid]\n",
        "SYSTEM_PROMPT_WITH_EXAMPLE = make_system_prompt_with_example(ex_uid, ex_seq, ex_gt)\n",
        "\n",
        "# Build items (exclude the example ID from targets)\n",
        "usable = [x for x in pool if x != ex_uid]\n",
        "chosen = usable[:N_SAMPLES]\n",
        "\n",
        "meta = {}\n",
        "with open(BATCH_INPUT_JSONL, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for i, uid in enumerate(chosen):\n",
        "        seq = id2seq[uid]\n",
        "        gt  = id2cap[uid]  # ground truth that will be FIXED in <answer>\n",
        "        cid = f\"p2d_fixans_{RUN_TAG}_{i}_{uid}\"\n",
        "        meta[cid] = {\n",
        "            \"uniprot_id\": uid,\n",
        "            \"sequence\": trunc(seq, 2000),\n",
        "            \"ground_truth\": gt,\n",
        "            \"example_uid\": ex_uid\n",
        "        }\n",
        "        body = {\n",
        "            \"model\": MODEL,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"messages\": build_messages_fill_thinking_fixed_answer(SYSTEM_PROMPT_WITH_EXAMPLE, uid, seq, gt),\n",
        "            \"max_tokens\": MAX_TOKENS,\n",
        "        }\n",
        "        fout.write(json.dumps({\n",
        "            \"custom_id\": cid,\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": body\n",
        "        }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "BATCH_META_JSON.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(f\"[System example used] UniProt: {ex_uid}\")\n",
        "print(f\"Wrote batch input: {BATCH_INPUT_JSONL}\")\n",
        "print(f\"Wrote meta map:   {BATCH_META_JSON}\")\n",
        "\n",
        "with open(BATCH_INPUT_JSONL, \"rb\") as f:\n",
        "    input_file = client.files.create(file=f, purpose=\"batch\")\n",
        "\n",
        "batch = client.batches.create(\n",
        "    input_file_id=input_file.id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=BATCH_COMPLETION_WINDOW,\n",
        ")\n",
        "\n",
        "BATCH_INFO_JSON.write_text(json.dumps({\n",
        "    \"batch_id\": batch.id,\n",
        "    \"input_file_id\": input_file.id,\n",
        "    \"status\": batch.status,\n",
        "    \"run_tag\": RUN_TAG,\n",
        "    \"example_uid\": ex_uid\n",
        "}, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\n=== SUBMITTED BATCH ===\")\n",
        "print(\"batch_id:      \", batch.id)\n",
        "print(\"input_file_id: \", input_file.id)\n",
        "print(\"status:        \", batch.status)\n",
        "print(f\"(Saved to {BATCH_INFO_JSON})\")\n",
        "\n",
        "print(\"\\nWhen the batch finishes, your existing fetch/merge (loose parser) cell will work as-is.\")\n",
        "print(\"It will recover the real system prompt as `instruction`, and parse <thinking>/<answer> from model outputs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jCrOClo-Iw6E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCrOClo-Iw6E",
        "outputId": "a27dc73f-b315-437f-87b5-2844f6d9fb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'run_tag': '20250925-211122', 'kept': 100, 'parsed_out': '/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_sft_20250925-211122.json', 'raw_out': '/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_raw_20250925-211122.json', 'missing_thinking_tag': 0, 'one_tag_answer': 32, 'answer_altered_by_model': 89}\n",
            "\n",
            "--- SAMPLE (parsed) ---\n",
            "INSTRUCTION (truncated):  ...\n",
            "OUTPUT (first 300 chars): <thinking>\n",
            "1. The sequence begins with \"MSEIRKNDHRLMQVLLAPVISEKATLVADKNEQVVFEVAPDATKQEVKAAVELLFKVEVDSVNVLVQKGKQKRFGRSMGRRKDVKKAYVCLKPGQEINFEAEAK\", which suggests a lack of a signal peptide, indicating a cytoplasmic localization.\n",
            "2. The sequence is relatively short, suggesting it might be part of a l ...\n",
            "\n",
            "--- SAMPLE (raw) ---\n",
            "INSTRUCTION (truncated):  ...\n",
            "OUTPUT (first 300 chars): <thinking>\n",
            "1. The sequence begins with \"MSEIRKNDHRLMQVLLAPVISEKATLVADKNEQVVFEVAPDATKQEVKAAVELLFKVEVDSVNVLVQKGKQKRFGRSMGRRKDVKKAYVCLKPGQEINFEAEAK\", which suggests a lack of a signal peptide, indicating a cytoplasmic localization.\n",
            "2. The sequence is relatively short, suggesting it might be part of a l ...\n"
          ]
        }
      ],
      "source": [
        "# ===================== Fetch output & build SFTs (Fixed-Answer variant) =====================\n",
        "from pathlib import Path\n",
        "import json, re\n",
        "from openai import OpenAI\n",
        "\n",
        "# Base dataset dir (same as in your submit cell)\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")  # <- change if needed\n",
        "OUT_DIR  = BASE_DIR / \"gpt_batch_protein2desc_fixed_answer\"\n",
        "\n",
        "# Auto-pick the latest run by timestamp suffix\n",
        "infos = sorted(OUT_DIR.glob(\"protein2desc_fixedans_batch_info_20250925-211122.json\"))\n",
        "assert infos, f\"No batch_info JSON found under {OUT_DIR}\"\n",
        "BATCH_INFO_JSON = infos[-1]\n",
        "\n",
        "RUN_TAG = json.loads(BATCH_INFO_JSON.read_text())[\"run_tag\"]\n",
        "BATCH_OUTPUT_JSONL = OUT_DIR / f\"protein2desc_fixedans_output_{RUN_TAG}.jsonl\"\n",
        "BATCH_META_JSON    = OUT_DIR / f\"protein2desc_fixedans_meta_{RUN_TAG}.json\"\n",
        "\n",
        "# Output files\n",
        "SFT_OUT_PARSED = OUT_DIR / f\"protein2desc_fixedans_sft_{RUN_TAG}.json\"   # parsed + answer forced to GT\n",
        "SFT_OUT_RAW    = OUT_DIR / f\"protein2desc_fixedans_raw_{RUN_TAG}.json\"   # raw assistant text\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# ---- helpers ----\n",
        "def fetch_batch_output_if_ready(batch_id: str, out_jsonl: Path):\n",
        "    b = client.batches.retrieve(batch_id)\n",
        "    if not getattr(b, \"output_file_id\", None):\n",
        "        return False, str(b.status)\n",
        "    content = client.files.content(b.output_file_id)\n",
        "    data = content.read()\n",
        "    out_jsonl.write_bytes(data)\n",
        "    return True, out_jsonl\n",
        "\n",
        "def extract_assistant_content(obj) -> str:\n",
        "    resp = obj.get(\"response\") or {}\n",
        "    body = resp.get(\"body\") if isinstance(resp, dict) else None\n",
        "    src  = body if isinstance(body, dict) else resp\n",
        "    try:\n",
        "        return src[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def extract_system_message(obj) -> str:\n",
        "    req = obj.get(\"request\") or {}\n",
        "    rbody = req.get(\"body\") if isinstance(req, dict) else None\n",
        "    src = rbody if isinstance(rbody, dict) else req\n",
        "    msgs = src.get(\"messages\", []) if isinstance(src, dict) else []\n",
        "    for m in msgs:\n",
        "        if m.get(\"role\") == \"system\":\n",
        "            return m.get(\"content\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "# Loose tag extractors (handle missing closing tags)\n",
        "def _extract_tag_loose(text: str, tag: str):\n",
        "    if not text:\n",
        "        return \"\", 0, False\n",
        "    open_pat  = re.compile(fr\"<{tag}\\b[^>]*>\", re.I)\n",
        "    close_pat = re.compile(fr\"</{tag}\\s*>\", re.I)\n",
        "    m_open = open_pat.search(text)\n",
        "    if not m_open:\n",
        "        return \"\", 0, False\n",
        "    m_close = close_pat.search(text, m_open.end())\n",
        "    if m_close:\n",
        "        content = text[m_open.end():m_close.start()]\n",
        "        endpos  = m_close.end()\n",
        "    else:\n",
        "        content = text[m_open.end():]\n",
        "        endpos  = len(text)\n",
        "    content = content.strip().rstrip('\", \\t\\r\\n')\n",
        "    return content, endpos, True\n",
        "\n",
        "def parse_thinking_answer_loose(text: str):\n",
        "    if not text:\n",
        "        return \"\", \"\"\n",
        "    thinking, th_end, th_found = _extract_tag_loose(text, \"thinking\")\n",
        "    remainder = text[th_end:] if th_found else text\n",
        "    answer, _, an_found = _extract_tag_loose(remainder, \"answer\")\n",
        "    if not th_found and an_found:\n",
        "        m_open_ans = re.search(r\"<answer\\b[^>]*>\", remainder, re.I)\n",
        "        cutoff = m_open_ans.start() if m_open_ans else len(text)\n",
        "        pre = text[:cutoff].strip().rstrip('\", \\t\\r\\n')\n",
        "        thinking = pre if pre else thinking\n",
        "    if not th_found and not an_found:\n",
        "        thinking = text.strip().rstrip('\", \\t\\r\\n')\n",
        "        answer   = \"\"\n",
        "    return thinking, answer\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "# ---- run ----\n",
        "binfo = json.loads(BATCH_INFO_JSON.read_text())\n",
        "batch_id = binfo[\"batch_id\"]\n",
        "\n",
        "ok, res = fetch_batch_output_if_ready(batch_id, BATCH_OUTPUT_JSONL)\n",
        "if not ok:\n",
        "    print(f\"Batch not complete yet: status={res}  (batch_id={batch_id})\")\n",
        "else:\n",
        "    meta = json.loads(BATCH_META_JSON.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "    parsed_records = []\n",
        "    raw_records    = []\n",
        "    kept = 0\n",
        "    altered_answers = 0\n",
        "    missing_thinking_tag = 0\n",
        "    one_tag_answer = 0\n",
        "\n",
        "    with open(BATCH_OUTPUT_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            if obj.get(\"error\"):\n",
        "                continue\n",
        "\n",
        "            content = extract_assistant_content(obj) or \"\"\n",
        "            system_prompt = extract_system_message(obj)\n",
        "            cid  = obj.get(\"custom_id\")\n",
        "            mrow = meta.get(cid, {})\n",
        "            uid, seq, gt = mrow.get(\"uniprot_id\"), mrow.get(\"sequence\"), mrow.get(\"ground_truth\")\n",
        "\n",
        "            # RAW record (no parsing/overrides)\n",
        "            raw_records.append({\n",
        "                \"instruction\": system_prompt,\n",
        "                \"input\": {\"uniprot_id\": uid, \"protein_sequence\": seq},\n",
        "                \"output\": content,\n",
        "                \"meta\": {\"ground_truth_caption\": gt, \"custom_id\": cid}\n",
        "            })\n",
        "\n",
        "            # PARSED record (loose parse; force answer to GT)\n",
        "            thinking, model_answer = parse_thinking_answer_loose(content)\n",
        "            if \"<thinking\" not in content.lower():\n",
        "                missing_thinking_tag += 1\n",
        "            if \"<answer\" in content.lower() and \"</answer>\" not in content.lower():\n",
        "                one_tag_answer += 1\n",
        "\n",
        "            # Compare model's answer to GT (normalize spaces); then FORCE to GT\n",
        "            model_eq_gt = normalize_text(model_answer) == normalize_text(gt)\n",
        "            if not model_eq_gt:\n",
        "                altered_answers += 1\n",
        "\n",
        "            parsed_records.append({\n",
        "                \"instruction\": system_prompt,   # real system prompt as instruction\n",
        "                \"input\": {\"uniprot_id\": uid, \"protein_sequence\": seq},\n",
        "                \"output\": (\n",
        "                    f\"<thinking>\\n{thinking}\\n</thinking>\\n\\n\"\n",
        "                    f\"<answer>\\n{gt}\\n</answer>\"      # <-- force canonical GT answer\n",
        "                ),\n",
        "                \"meta\": {\n",
        "                    \"ground_truth_caption\": gt,\n",
        "                    \"custom_id\": cid,\n",
        "                    \"model_answer_equal_to_gt\": model_eq_gt\n",
        "                }\n",
        "            })\n",
        "            kept += 1\n",
        "\n",
        "    # Save both\n",
        "    SFT_OUT_PARSED.write_text(json.dumps(parsed_records, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    SFT_OUT_RAW.write_text(json.dumps(raw_records,    ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    print({\n",
        "        \"run_tag\": RUN_TAG,\n",
        "        \"kept\": kept,\n",
        "        \"parsed_out\": str(SFT_OUT_PARSED),\n",
        "        \"raw_out\": str(SFT_OUT_RAW),\n",
        "        \"missing_thinking_tag\": missing_thinking_tag,\n",
        "        \"one_tag_answer\": one_tag_answer,\n",
        "        \"answer_altered_by_model\": altered_answers\n",
        "    })\n",
        "\n",
        "    # Quick preview\n",
        "    if parsed_records:\n",
        "        print(\"\\n--- SAMPLE (parsed) ---\")\n",
        "        print(\"INSTRUCTION (truncated):\", parsed_records[0][\"instruction\"][:220], \"...\")\n",
        "        print(\"OUTPUT (first 300 chars):\", parsed_records[0][\"output\"][:300], \"...\")\n",
        "    if raw_records:\n",
        "        print(\"\\n--- SAMPLE (raw) ---\")\n",
        "        print(\"INSTRUCTION (truncated):\", raw_records[0][\"instruction\"][:220], \"...\")\n",
        "        print(\"OUTPUT (first 300 chars):\", raw_records[0][\"output\"][:300], \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PE1WsUBltdjX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE1WsUBltdjX",
        "outputId": "8828b4bf-c19a-4789-db60-86c15ce46053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovered versions:\n",
            " - [v1_free:default]  /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc/protein2desc_cot_sft.json\n",
            " - [v1_free:run_b0778017]  /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc/protein2desc_cot_sft_run_b0778017.json\n",
            " - [fixed_answer:20250925-180228]  /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_sft_20250925-180228.json\n",
            " - [fixed_answer:20250925-204511]  /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_sft_20250925-204511.json\n",
            " - [fixed_answer:20250925-212446]  /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_sft_20250925-212446.json\n",
            " - [fixed_answer:20250925-211122]  /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/protein2desc_fixedans_sft_20250925-211122.json\n",
            "\n",
            "=== CoT Length Summary by Version ===\n",
            "                         set  count  empty_count  empty_pct  chars_all_mean  chars_all_median  chars_nonempty_mean  chars_nonempty_median  words_all_mean  words_all_median  words_nonempty_mean  words_nonempty_median  chars_all_p10  chars_all_p90  words_all_p10  words_all_p90  chars_all_min  chars_all_max  words_all_min  words_all_max\n",
            "fixed_answer:20250925-180228    100            0        0.0         1040.14            1024.5              1040.14                 1024.5          146.16             142.5               146.16                  142.5          824.3         1258.0          115.7          181.1            692           1990             97            252\n",
            "fixed_answer:20250925-204511    100            0        0.0         1202.17            1168.0              1202.17                 1168.0          165.06             164.5               165.06                  164.5          987.6         1429.6          140.9          187.1            894           2029            123            223\n",
            "fixed_answer:20250925-211122    100            0        0.0         1548.62            1518.5              1548.62                 1518.5          209.26             207.5               209.26                  207.5         1323.0         1807.8          182.0          232.1           1156           2611            156            266\n",
            "fixed_answer:20250925-212446    100            0        0.0         1577.25            1533.0              1577.25                 1533.0          211.51             208.5               211.51                  208.5         1319.5         1845.9          188.0          238.2           1125           2622            160            271\n",
            "             v1_free:default    100            0        0.0         1562.70            1552.0              1562.70                 1552.0          196.00             201.5               196.00                  201.5         1265.0         1801.4          140.7          235.3            994           2513             87            259\n",
            "        v1_free:run_b0778017    100            0        0.0         1849.29            1809.5              1849.29                 1809.5          239.95             238.5               239.95                  238.5         1508.7         2104.7          203.0          280.2           1153           3515            173            331\n",
            "\n",
            "Saved per-item lengths to: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/cot_length_comparison_all_versions.csv\n",
            "Saved summary to: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/cot_length_summary_all_versions.csv\n"
          ]
        }
      ],
      "source": [
        "# ===================== Compare CoT Lengths across ALL versions/tags =====================\n",
        "from pathlib import Path\n",
        "import json, re\n",
        "import pandas as pd\n",
        "\n",
        "# ---- base path (edit if needed) ----\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")\n",
        "\n",
        "V1_DIR     = BASE_DIR / \"gpt_batch_protein2desc\"\n",
        "FIXED_DIR  = BASE_DIR / \"gpt_batch_protein2desc_fixed_answer\"\n",
        "\n",
        "# ---- where to save results ----\n",
        "OUT_PER_ITEM_CSV = BASE_DIR / \"cot_length_comparison_all_versions.csv\"\n",
        "OUT_SUMMARY_CSV  = BASE_DIR / \"cot_length_summary_all_versions.csv\"\n",
        "\n",
        "# ---- loose tag extraction (handles missing closers) ----\n",
        "def _extract_tag_loose(text: str, tag: str):\n",
        "    if not text:\n",
        "        return \"\", 0, False\n",
        "    open_pat  = re.compile(fr\"<{tag}\\b[^>]*>\", re.I)\n",
        "    close_pat = re.compile(fr\"</{tag}\\s*>\", re.I)\n",
        "    m_open = open_pat.search(text)\n",
        "    if not m_open:\n",
        "        return \"\", 0, False\n",
        "    m_close = close_pat.search(text, m_open.end())\n",
        "    if m_close:\n",
        "        content = text[m_open.end():m_close.start()]\n",
        "        endpos  = m_close.end()\n",
        "    else:\n",
        "        # no closing tag → capture to end\n",
        "        content = text[m_open.end():]\n",
        "        endpos  = len(text)\n",
        "    content = content.strip().rstrip('\", \\t\\r\\n')\n",
        "    return content, endpos, True\n",
        "\n",
        "def extract_thinking(text: str) -> str:\n",
        "    thinking, _, found = _extract_tag_loose(text or \"\", \"thinking\")\n",
        "    if found:\n",
        "        return thinking\n",
        "    # fallback: treat whole text as \"thinking\"\n",
        "    return (text or \"\").strip().rstrip('\", \\t\\r\\n')\n",
        "\n",
        "def load_sft(path: Path):\n",
        "    data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(f\"Expected list at {path}\")\n",
        "    return data\n",
        "\n",
        "def lengths_for(records, set_label: str, tag: str, src_file: Path):\n",
        "    rows = []\n",
        "    for i, r in enumerate(records):\n",
        "        out = (r.get(\"output\") or \"\")\n",
        "        t = extract_thinking(out)\n",
        "        rows.append({\n",
        "            \"family\": set_label,                          # 'v1_free' or 'fixed_answer'\n",
        "            \"tag\": tag,                                   # e.g. timestamp or 'default'\n",
        "            \"set\": f\"{set_label}:{tag}\",                  # combined label\n",
        "            \"idx\": i,\n",
        "            \"uniprot_id\": ((r.get(\"input\") or {}).get(\"uniprot_id\") or \"\"),\n",
        "            \"char_len\": len(t),\n",
        "            \"word_len\": len(t.split()),\n",
        "            \"empty\": (len(t) == 0),\n",
        "            \"source_file\": str(src_file)\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def summarize_group(df, set_label: str):\n",
        "    sub = df[df[\"set\"] == set_label]\n",
        "    n = len(sub)\n",
        "    n_empty = int(sub[\"empty\"].sum())\n",
        "    non_empty = sub[~sub[\"empty\"]]\n",
        "    def stats_series(series):\n",
        "        if len(series) == 0:\n",
        "            return {\"mean\": 0, \"median\": 0, \"p10\": 0, \"p90\": 0, \"min\": 0, \"max\": 0}\n",
        "        return {\n",
        "            \"mean\": float(series.mean()),\n",
        "            \"median\": float(series.median()),\n",
        "            \"p10\": float(series.quantile(0.10)),\n",
        "            \"p90\": float(series.quantile(0.90)),\n",
        "            \"min\": int(series.min()),\n",
        "            \"max\": int(series.max()),\n",
        "        }\n",
        "    ch_all = stats_series(sub[\"char_len\"])\n",
        "    wd_all = stats_series(sub[\"word_len\"])\n",
        "    ch_ne  = stats_series(non_empty[\"char_len\"])\n",
        "    wd_ne  = stats_series(non_empty[\"word_len\"])\n",
        "    return {\n",
        "        \"set\": set_label,\n",
        "        \"count\": n,\n",
        "        \"empty_count\": n_empty,\n",
        "        \"empty_pct\": round(100 * n_empty / n, 2) if n else 0.0,\n",
        "        \"chars_all_mean\": round(ch_all[\"mean\"], 2),\n",
        "        \"chars_all_median\": ch_all[\"median\"],\n",
        "        \"chars_nonempty_mean\": round(ch_ne[\"mean\"], 2),\n",
        "        \"chars_nonempty_median\": ch_ne[\"median\"],\n",
        "        \"words_all_mean\": round(wd_all[\"mean\"], 2),\n",
        "        \"words_all_median\": wd_all[\"median\"],\n",
        "        \"words_nonempty_mean\": round(wd_ne[\"mean\"], 2),\n",
        "        \"words_nonempty_median\": wd_ne[\"median\"],\n",
        "        \"chars_all_p10\": ch_all[\"p10\"],\n",
        "        \"chars_all_p90\": ch_all[\"p90\"],\n",
        "        \"words_all_p10\": wd_all[\"p10\"],\n",
        "        \"words_all_p90\": wd_all[\"p90\"],\n",
        "        \"chars_all_min\": ch_all[\"min\"],\n",
        "        \"chars_all_max\": ch_all[\"max\"],\n",
        "        \"words_all_min\": wd_all[\"min\"],\n",
        "        \"words_all_max\": wd_all[\"max\"],\n",
        "    }\n",
        "\n",
        "# ---- discover all SFT files & tags ----\n",
        "found_files = []\n",
        "\n",
        "# v1 variants:\n",
        "# - protein2desc_cot_sft.json                → tag = 'default'\n",
        "# - protein2desc_cot_sft_<TAG>.json          → tag = <TAG>\n",
        "if V1_DIR.exists():\n",
        "    for p in V1_DIR.glob(\"protein2desc_cot_sft.json\"):\n",
        "        found_files.append((\"v1_free\", \"default\", p))\n",
        "    for p in V1_DIR.glob(\"protein2desc_cot_sft_*.json\"):\n",
        "        # capture tag after 'sft_'\n",
        "        m = re.match(r\"protein2desc_cot_sft_(?P<tag>[^.]+)\\.json$\", p.name)\n",
        "        tag = m.group(\"tag\") if m else p.stem.replace(\"protein2desc_cot_sft_\", \"\")\n",
        "        found_files.append((\"v1_free\", tag, p))\n",
        "\n",
        "# fixed-answer variants:\n",
        "# - protein2desc_fixedans_sft_<TAG>.json     → tag = <TAG>\n",
        "if FIXED_DIR.exists():\n",
        "    for p in FIXED_DIR.glob(\"protein2desc_fixedans_sft_*.json\"):\n",
        "        m = re.match(r\"protein2desc_fixedans_sft_(?P<tag>[^.]+)\\.json$\", p.name)\n",
        "        if not m:\n",
        "            continue\n",
        "        tag = m.group(\"tag\")\n",
        "        found_files.append((\"fixed_answer\", tag, p))\n",
        "\n",
        "if not found_files:\n",
        "    raise FileNotFoundError(\"No SFT files found in either folder.\")\n",
        "\n",
        "print(\"Discovered versions:\")\n",
        "for fam, tag, p in found_files:\n",
        "    print(f\" - [{fam}:{tag}]  {p}\")\n",
        "\n",
        "# ---- load all, compute per-item rows ----\n",
        "rows = []\n",
        "for fam, tag, p in found_files:\n",
        "    try:\n",
        "        recs = load_sft(p)\n",
        "        rows += lengths_for(recs, set_label=fam, tag=tag, src_file=p)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Skipping {p}: {e}\")\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "assert not df.empty, \"No records loaded from discovered SFT files.\"\n",
        "\n",
        "# ---- build summary for each version (set) ----\n",
        "sets = sorted(df[\"set\"].unique())\n",
        "summary_rows = [summarize_group(df, s) for s in sets]\n",
        "summary_df = pd.DataFrame(summary_rows).sort_values([\"set\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== CoT Length Summary by Version ===\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# ---- save outputs ----\n",
        "OUT_PER_ITEM_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(OUT_PER_ITEM_CSV, index=False)\n",
        "summary_df.to_csv(OUT_SUMMARY_CSV, index=False)\n",
        "\n",
        "print(\"\\nSaved per-item lengths to:\", OUT_PER_ITEM_CSV)\n",
        "print(\"Saved summary to:\", OUT_SUMMARY_CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KHelBpAWbZjE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHelBpAWbZjE",
        "outputId": "2069f89b-d613-44bc-e428-ea0d8c17e978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SRC] /content/gpt_batch_protein2desc  files=5  size=9.33 MB\n",
            "[DEST] /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc  files=5  size=9.33 MB\n",
            "[OK] Moved and removed source: /content/gpt_batch_protein2desc\n",
            "\n",
            "Key files now under: /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc\n",
            " - /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc/protein2desc_input.jsonl [OK]\n",
            " - /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc/protein2desc_meta.json [OK]\n",
            " - /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc/protein2desc_batch_info.json [OK]\n",
            " - /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc/protein2desc_output.jsonl [OK]\n",
            " - /content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc/protein2desc_cot_sft.json [OK]\n"
          ]
        }
      ],
      "source": [
        "# moving files if you set wrong working directory.\n",
        "import os, shutil, hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "SRC  = Path(\"/content/gpt_batch_protein2desc\")\n",
        "DEST = BASE_DIR / \"gpt_batch_protein2desc\"\n",
        "\n",
        "def count_files_and_bytes(root: Path):\n",
        "    n, b = 0, 0\n",
        "    for p in root.rglob(\"*\"):\n",
        "        if p.is_file():\n",
        "            n += 1\n",
        "            try:\n",
        "                b += p.stat().st_size\n",
        "            except Exception:\n",
        "                pass\n",
        "    return n, b\n",
        "\n",
        "if not SRC.exists():\n",
        "    raise FileNotFoundError(f\"Source folder not found: {SRC}\")\n",
        "\n",
        "DEST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "src_files, src_bytes = count_files_and_bytes(SRC)\n",
        "print(f\"[SRC] {SRC}  files={src_files}  size={src_bytes/1e6:.2f} MB\")\n",
        "\n",
        "if not any(DEST.iterdir()):\n",
        "\n",
        "    for item in SRC.iterdir():\n",
        "        shutil.move(str(item), str(DEST))\n",
        "else:\n",
        "\n",
        "    for root, dirs, files in os.walk(SRC):\n",
        "        rel = Path(root).relative_to(SRC)\n",
        "        dest_dir = DEST / rel\n",
        "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "        for d in dirs:\n",
        "            (dest_dir / d).mkdir(parents=True, exist_ok=True)\n",
        "        for f in files:\n",
        "            s = Path(root) / f\n",
        "            t = dest_dir / f\n",
        "            if t.exists():\n",
        "                try:\n",
        "                    t.unlink()\n",
        "                except IsADirectoryError:\n",
        "                    shutil.rmtree(t)\n",
        "            try:\n",
        "                shutil.move(str(s), str(t))\n",
        "            except Exception:\n",
        "                shutil.copy2(str(s), str(t))\n",
        "                s.unlink()\n",
        "\n",
        "dest_files, dest_bytes = count_files_and_bytes(DEST)\n",
        "print(f\"[DEST] {DEST}  files={dest_files}  size={dest_bytes/1e6:.2f} MB\")\n",
        "\n",
        "if dest_files >= src_files and dest_bytes >= src_bytes * 0.999:\n",
        "    try:\n",
        "        shutil.rmtree(SRC)\n",
        "        print(f\"[OK] Moved and removed source: {SRC}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not remove source (remove manually if needed): {e}\")\n",
        "else:\n",
        "    raise RuntimeError(\"Sanity check failed: destination counts smaller than source; aborting deletion.\")\n",
        "\n",
        "important = [\n",
        "    \"protein2desc_input.jsonl\",\n",
        "    \"protein2desc_meta.json\",\n",
        "    \"protein2desc_batch_info.json\",\n",
        "    \"protein2desc_output.jsonl\",\n",
        "    \"protein2desc_cot_sft.json\",\n",
        "]\n",
        "print(\"\\nKey files now under:\", DEST)\n",
        "for name in important:\n",
        "    p = DEST / name\n",
        "    print(\" -\", p, (\"[OK]\" if p.exists() else \"[missing]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XsOZjExRogO8",
      "metadata": {
        "id": "XsOZjExRogO8"
      },
      "source": [
        "## Large Scale CoT generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "70gfxgsXok5r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70gfxgsXok5r",
        "outputId": "5216ab39-f928-46a8-a74f-6df46b9720de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NOTE] Existing order planned 541158; current N_TOTAL=541159. Proceeding with saved order length=541158.\n",
            "[RESUME] Loaded order file with 541158 ids; ex_uid=Q2LSF9\n",
            "\n",
            "[PROGRESS] total_ids=541158  chunk_size=1000  total_chunks=542\n",
            "  already_submitted_chunks=5  remaining_chunks=537\n",
            "  plan_this_run: submit_items=5000  =>  chunks=5 -> [5, 6, 7, 8, 9]\n",
            "[SUBMITTED] chunk=005 items=1000 batch_id=batch_68dd98823ca08190aceab6a40d72f912 status=validating\n",
            "[SUBMITTED] chunk=006 items=1000 batch_id=batch_68dd9883a0d881908455cbefc8245f4f status=validating\n",
            "[SUBMITTED] chunk=007 items=1000 batch_id=batch_68dd9884f1308190bd9275dadf5ed177 status=validating\n",
            "[SUBMITTED] chunk=008 items=1000 batch_id=batch_68dd9886813081909f1d6cd1b97ecfc4 status=validating\n",
            "[SUBMITTED] chunk=009 items=1000 batch_id=batch_68dd9888a4908190a107f824c63b6eda status=validating\n",
            "\n",
            "Done submitting this run.\n",
            "Tip: re-run this cell later to submit the next 10k (or adjust ITEMS_PER_RUN).\n"
          ]
        }
      ],
      "source": [
        "# ===================== Fixed-Answer batches with pause/resume (50k split; submit N items per run) =====================\n",
        "import os, json, re, html, random, time, math\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "BASE_DIR          = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")  # change if needed\n",
        "OUT_DIR           = BASE_DIR / \"gpt_batch_protein2desc_fixed_answer\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Model/Batch\n",
        "MODEL             = \"gpt-4o\"\n",
        "TEMPERATURE       = 0.1\n",
        "MAX_TOKENS        = 4000\n",
        "COMPLETION_WINDOW = \"24h\"          # \"24h\" or \"48h\"\n",
        "SUBMIT_SLEEP_SEC  = 0.0            # sleep between chunk submissions if desired\n",
        "\n",
        "# Dataset size & chunking\n",
        "N_TOTAL           = 541159         # total target items\n",
        "CHUNK_SIZE        = 1_000          # items per batch/chunk\n",
        "ITEMS_PER_RUN     = 5000        # how many to submit THIS run (e.g., 10k now, 10k later)\n",
        "SHUFFLE_SEED      = 42\n",
        "\n",
        "# Optional: restrict pool via a CSV manifest with \"uniprot_id\"\n",
        "MANIFEST_PATH     = None           # e.g., BASE_DIR/\"subset_50k/manifest_50k.csv\"\n",
        "MANIFEST_LIMIT    = 0              # 0 = no extra cap\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def strip_html(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = html.unescape(s)\n",
        "    return re.sub(r\"<[^>]+>\", \" \", s).replace(\"\\xa0\", \" \").strip()\n",
        "\n",
        "def trunc(s: str, n: int) -> str:\n",
        "    s = s or \"\"\n",
        "    return (s[:n] + \"…\") if len(s) > n else s\n",
        "\n",
        "def two_line_pairs(path: Path):\n",
        "    \"\"\"SwissProtCLAP: alternating lines: id then value.\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        while True:\n",
        "            id_line = f.readline()\n",
        "            if not id_line: break\n",
        "            val_line = f.readline()\n",
        "            if not val_line: break\n",
        "            yield id_line.strip(), val_line.strip()\n",
        "\n",
        "def load_swissprotclap(base_dir: Path):\n",
        "    sp = base_dir / \"SwissProtCLAP\"\n",
        "    seq_fp, txt_fp = sp / \"protein_sequence.txt\", sp / \"text_sequence.txt\"\n",
        "    assert seq_fp.exists() and txt_fp.exists(), f\"Missing SwissProtCLAP under {sp}\"\n",
        "    id2seq = {pid: seq for pid, seq in two_line_pairs(seq_fp)}\n",
        "    id2cap = {pid: strip_html(txt) for pid, txt in two_line_pairs(txt_fp)}\n",
        "    ids = [i for i in id2seq.keys() if i in id2cap]\n",
        "    return ids, id2seq, id2cap\n",
        "\n",
        "\n",
        "# ---------------- your SYSTEM prompt (unchanged) ----------------\n",
        "# Use exactly your string (paste/keep your existing definition here).\n",
        "def make_system_prompt_with_example(ex_uid: str, ex_seq: str, ex_gt: str) -> str:\n",
        "    return (\n",
        "        \"You are a professional protein biologist. Your task is to generate a natural, concise, and biologically \"\n",
        "        \"accurate description of a protein based **only** on its amino-acid sequence.\\n\\n\"\n",
        "        \"### Example:\\n\"\n",
        "        f\"Protein UniProt: {ex_uid}\\n\"\n",
        "        f\"Protein sequence: {trunc(ex_seq, 1200)}\\n\"\n",
        "        f\"Answer: <answer> {ex_gt} </answer>\\n\\n\"\n",
        "        \"### Now try this:\\n\"\n",
        "        \"Given ONLY the sequence, first think step by step about plausible features (signal peptides, transmembrane \"\n",
        "        \"helices, repeats, low-complexity segments, catalytic motifs/domains, localization signals), then produce a \"\n",
        "        \"polished 2–4 sentence description.\\n\\n\"\n",
        "        \"Your final answer **must** be returned in the format:\\n\"\n",
        "        \"<thinking>\\n[steps: Your reasoning steps using sequence-derived evidence]\\n</thinking>\\n\\n\"\n",
        "        \"<answer>\\n[2–4 sentences; A natural, concise scientific description of the protein]\\n</answer>\\n\"\n",
        "    )\n",
        "\n",
        "# --------------- Build messages where USER supplies the exact <answer> ---------------\n",
        "def build_messages_fill_thinking_fixed_answer(system_prompt: str, uniprot_id: str, seq: str, ground_truth: str):\n",
        "    \"\"\"\n",
        "    We give the exact <answer> block to the model and ask it to generate ONLY the <thinking> block,\n",
        "    then output both blocks with the answer copied verbatim.\n",
        "    \"\"\"\n",
        "    user = (\n",
        "        f\"Protein UniProt: {uniprot_id}\\n\"\n",
        "        f\"Protein sequence (truncated to 2000 aa if long):\\n{trunc(seq, 2000)}\\n\\n\"\n",
        "        #\"Fill ONLY the <thinking> block using sequence-derived evidence. \"\n",
        "        #\"Then output BOTH blocks exactly as below, keeping the <answer> text EXACTLY as provided.\\n\\n\"\n",
        "        \"<thinking>\\n[Your reasoning steps using sequence-derived evidence]\\n</thinking>\\n\\n\"\n",
        "        \"<answer>\\n\"\n",
        "        f\"{ground_truth}\\n\"\n",
        "        \"</answer>\"\n",
        "    )\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user},\n",
        "    ]\n",
        "\n",
        "\n",
        "def write_json(path: Path, obj: dict|list):\n",
        "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "def parse_chunk_index_from_info(path: Path) -> int|None:\n",
        "    \"\"\"Extract chunk index from our naming pattern: ..._chunkXXX.json\"\"\"\n",
        "    m = re.search(r\"_chunk(\\d{3})\\.json$\", path.name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "# ---------- Prepare or load stable order ----------\n",
        "ORDER_JSON = OUT_DIR / \"protein2desc_fixedans_idorder.json\"\n",
        "\n",
        "ids, id2seq, id2cap = load_swissprotclap(BASE_DIR)\n",
        "\n",
        "# optional manifest restriction\n",
        "if MANIFEST_PATH and Path(MANIFEST_PATH).exists():\n",
        "    import pandas as pd\n",
        "    dfm = pd.read_csv(MANIFEST_PATH)\n",
        "    restrict_ids = [str(x) for x in dfm[\"uniprot_id\"].astype(str).tolist()]\n",
        "    if MANIFEST_LIMIT and MANIFEST_LIMIT > 0:\n",
        "        restrict_ids = restrict_ids[:MANIFEST_LIMIT]\n",
        "    pool_initial = [i for i in restrict_ids if i in id2seq and i in id2cap]\n",
        "else:\n",
        "    pool_initial = ids[:]\n",
        "\n",
        "if ORDER_JSON.exists():\n",
        "    # Resume mode: load stable order + example\n",
        "    order_info = json.loads(ORDER_JSON.read_text(encoding=\"utf-8\"))\n",
        "    order_ids  = order_info[\"order_ids\"]\n",
        "    ex_uid     = order_info[\"system_example\"][\"ex_uid\"]\n",
        "    ex_seq     = order_info[\"system_example\"][\"ex_seq_trunc\"]\n",
        "    ex_gt      = order_info[\"system_example\"][\"ex_gt\"]\n",
        "    saved_seed = order_info.get(\"seed\", None)\n",
        "    saved_total = order_info.get(\"n_total_planned\", None)\n",
        "    if saved_total and saved_total != N_TOTAL:\n",
        "        print(f\"[NOTE] Existing order planned {saved_total}; current N_TOTAL={N_TOTAL}. Proceeding with saved order length={len(order_ids)}.\")\n",
        "    print(f\"[RESUME] Loaded order file with {len(order_ids)} ids; ex_uid={ex_uid}\")\n",
        "else:\n",
        "    # First run: create deterministic order & pick example\n",
        "    random.seed(SHUFFLE_SEED)\n",
        "    pool = pool_initial[:]\n",
        "    random.shuffle(pool)\n",
        "    # Pick example used in system prompt\n",
        "    ex_uid = pool[0]\n",
        "    ex_seq = id2seq[ex_uid]\n",
        "    ex_gt  = id2cap[ex_uid]\n",
        "    # Build the target order excluding example\n",
        "    usable = [x for x in pool if x != ex_uid]\n",
        "    if len(usable) < N_TOTAL:\n",
        "        print(f\"[WARN] Pool smaller than requested N_TOTAL={N_TOTAL}. Using {len(usable)}.\")\n",
        "    order_ids = usable[:N_TOTAL]\n",
        "\n",
        "    order_info = {\n",
        "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"seed\": SHUFFLE_SEED,\n",
        "        \"n_total_planned\": len(order_ids),\n",
        "        \"chunk_size\": CHUNK_SIZE,\n",
        "        \"system_example\": {\n",
        "            \"ex_uid\": ex_uid,\n",
        "            \"ex_seq_trunc\": trunc(ex_seq, 1200),\n",
        "            \"ex_gt\": ex_gt\n",
        "        },\n",
        "        \"order_ids\": order_ids\n",
        "    }\n",
        "    write_json(ORDER_JSON, order_info)\n",
        "    print(f\"[INIT] Wrote stable order: {ORDER_JSON.name}  |  ids={len(order_ids)}  |  ex_uid={ex_uid}\")\n",
        "\n",
        "# ---------- Determine which chunks already submitted ----------\n",
        "info_files = sorted(OUT_DIR.glob(\"protein2desc_fixedans_batch_info_*_chunk???.json\"))\n",
        "submitted_chunks = set()\n",
        "for p in info_files:\n",
        "    idx = parse_chunk_index_from_info(p)\n",
        "    if idx is not None:\n",
        "        submitted_chunks.add(idx)\n",
        "\n",
        "total_chunks = math.ceil(len(order_ids) / CHUNK_SIZE)\n",
        "remaining_chunks = [i for i in range(total_chunks) if i not in submitted_chunks]\n",
        "\n",
        "remaining_items = sum(\n",
        "    min(CHUNK_SIZE, len(order_ids) - (i * CHUNK_SIZE))\n",
        "    for i in remaining_chunks\n",
        ")\n",
        "\n",
        "# How many items to submit this run?\n",
        "to_submit_items = min(ITEMS_PER_RUN, remaining_items)\n",
        "to_submit_chunks = []\n",
        "items_counted = 0\n",
        "for ci in remaining_chunks:\n",
        "    chunk_start = ci * CHUNK_SIZE\n",
        "    chunk_end   = min((ci+1)*CHUNK_SIZE, len(order_ids))\n",
        "    sz = chunk_end - chunk_start\n",
        "    if items_counted + sz <= to_submit_items:\n",
        "        to_submit_chunks.append(ci)\n",
        "        items_counted += sz\n",
        "    else:\n",
        "        # partial chunk not supported: we submit full chunks; stop here\n",
        "        break\n",
        "\n",
        "print(f\"\\n[PROGRESS] total_ids={len(order_ids)}  chunk_size={CHUNK_SIZE}  total_chunks={total_chunks}\")\n",
        "print(f\"  already_submitted_chunks={len(submitted_chunks)}  remaining_chunks={len(remaining_chunks)}\")\n",
        "print(f\"  plan_this_run: submit_items={to_submit_items}  =>  chunks={len(to_submit_chunks)} -> {to_submit_chunks[:6]}{'...' if len(to_submit_chunks)>6 else ''}\")\n",
        "\n",
        "if not to_submit_chunks:\n",
        "    print(\"\\nNothing to submit. All planned chunks are already submitted (or ITEMS_PER_RUN=0).\")\n",
        "else:\n",
        "    # Build system prompt (use stored example)\n",
        "    SYSTEM_PROMPT = make_system_prompt_with_example(ex_uid, ex_seq, ex_gt)\n",
        "\n",
        "    # ---------- Submit each selected chunk ----------\n",
        "    for c in to_submit_chunks:\n",
        "        chunk_start = c * CHUNK_SIZE\n",
        "        chunk_end   = min((c+1) * CHUNK_SIZE, len(order_ids))\n",
        "        chunk_ids   = order_ids[chunk_start:chunk_end]\n",
        "        chunk_tag   = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        chunk_pad   = f\"{c:03d}\"\n",
        "\n",
        "        # file names for this chunk\n",
        "        BATCH_INPUT_JSONL = OUT_DIR / f\"protein2desc_fixedans_input_{chunk_tag}_chunk{chunk_pad}.jsonl\"\n",
        "        BATCH_META_JSON   = OUT_DIR / f\"protein2desc_fixedans_meta_{chunk_tag}_chunk{chunk_pad}.json\"\n",
        "        BATCH_INFO_JSON   = OUT_DIR / f\"protein2desc_fixedans_batch_info_{chunk_tag}_chunk{chunk_pad}.json\"\n",
        "\n",
        "        # assemble JSONL + meta\n",
        "        meta = {}\n",
        "        with open(BATCH_INPUT_JSONL, \"w\", encoding=\"utf-8\") as fout:\n",
        "            for i, uid in enumerate(chunk_ids):\n",
        "                seq = id2seq[uid]\n",
        "                gt  = id2cap[uid]  # ground truth answer\n",
        "                cid = f\"p2d_fixans_{chunk_tag}_c{chunk_pad}_{i:04d}_{uid}\"\n",
        "                meta[cid] = {\n",
        "                    \"uniprot_id\": uid,\n",
        "                    \"sequence\": trunc(seq, 2000),\n",
        "                    \"ground_truth\": gt,\n",
        "                    \"example_uid\": ex_uid,\n",
        "                    \"chunk_index\": c,\n",
        "                    \"chunk_size\": len(chunk_ids),\n",
        "                    \"order_start\": chunk_start,\n",
        "                    \"order_end\": chunk_end\n",
        "                }\n",
        "                body = {\n",
        "                    \"model\": MODEL,\n",
        "                    \"temperature\": TEMPERATURE,\n",
        "                    \"messages\": build_messages_fill_thinking_fixed_answer(SYSTEM_PROMPT, uid, seq, gt),\n",
        "                    \"max_tokens\": MAX_TOKENS,\n",
        "                }\n",
        "                fout.write(json.dumps({\n",
        "                    \"custom_id\": cid,\n",
        "                    \"method\": \"POST\",\n",
        "                    \"url\": \"/v1/chat/completions\",\n",
        "                    \"body\": body\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        write_json(BATCH_META_JSON, meta)\n",
        "\n",
        "        # submit the batch\n",
        "        with open(BATCH_INPUT_JSONL, \"rb\") as f:\n",
        "            input_file = client.files.create(file=f, purpose=\"batch\")\n",
        "\n",
        "        batch = client.batches.create(\n",
        "            input_file_id=input_file.id,\n",
        "            endpoint=\"/v1/chat/completions\",\n",
        "            completion_window=COMPLETION_WINDOW,\n",
        "        )\n",
        "\n",
        "        write_json(BATCH_INFO_JSON, {\n",
        "            \"batch_id\": batch.id,\n",
        "            \"input_file_id\": input_file.id,\n",
        "            \"status\": batch.status,\n",
        "            \"run_tag\": chunk_tag,\n",
        "            \"chunk_index\": c,\n",
        "            \"chunk_size\": len(chunk_ids),\n",
        "            \"n_total_planned\": len(order_ids),\n",
        "            \"order_range\": [chunk_start, chunk_end],\n",
        "            \"file_prefix\": f\"protein2desc_fixedans_*_{chunk_tag}_chunk{chunk_pad}.*\"\n",
        "        })\n",
        "\n",
        "        print(f\"[SUBMITTED] chunk={c:03d} items={len(chunk_ids):4d} batch_id={batch.id} status={batch.status}\")\n",
        "        if SUBMIT_SLEEP_SEC > 0:\n",
        "            time.sleep(SUBMIT_SLEEP_SEC)\n",
        "\n",
        "    print(\"\\nDone submitting this run.\\nTip: re-run this cell later to submit the next 10k (or adjust ITEMS_PER_RUN).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mW_x-IBprzL5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW_x-IBprzL5",
        "outputId": "1ca12e73-1b66-4647-a6de-1777f6e580d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[protein_sequence.txt] lines=1,082,318  records=541,159  even_lines=True\n",
            "[text_sequence.txt]    lines=1,082,318  records=541,159  even_lines=True\n",
            "\n",
            "OK: SwissProtCLAP contains 541,159 sequence-caption pairs (expected 541,159).\n"
          ]
        }
      ],
      "source": [
        "# === Verify SwissProtCLAP has 541,159 sequences ===\n",
        "from pathlib import Path\n",
        "\n",
        "# If BASE_DIR is not already defined in your notebook, set it here:\n",
        "try:\n",
        "    BASE_DIR\n",
        "except NameError:\n",
        "    BASE_DIR = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")\n",
        "\n",
        "sp = BASE_DIR / \"SwissProtCLAP\"\n",
        "seq_fp = sp / \"protein_sequence.txt\"\n",
        "txt_fp = sp / \"text_sequence.txt\"\n",
        "EXPECTED = 541_159\n",
        "\n",
        "def count_two_line_records(p: Path) -> tuple[int, int, bool]:\n",
        "    \"\"\"Return (records, total_lines, is_even). Each record is 2 lines: ID then value.\"\"\"\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
        "    total = 0\n",
        "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for total, _ in enumerate(f, 1):\n",
        "            pass\n",
        "    return total // 2, total, (total % 2 == 0)\n",
        "\n",
        "seq_n, seq_lines, seq_even = count_two_line_records(seq_fp)\n",
        "txt_n, txt_lines, txt_even = count_two_line_records(txt_fp)\n",
        "\n",
        "print(f\"[protein_sequence.txt] lines={seq_lines:,}  records={seq_n:,}  even_lines={seq_even}\")\n",
        "print(f\"[text_sequence.txt]    lines={txt_lines:,}  records={txt_n:,}  even_lines={txt_even}\")\n",
        "\n",
        "ok = (seq_even and txt_even and seq_n == txt_n == EXPECTED)\n",
        "print(\"\\nOK:\" if ok else \"\\nMISMATCH:\",\n",
        "      f\"SwissProtCLAP contains {seq_n:,} sequence-caption pairs (expected {EXPECTED:,}).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daPfxAbTt7tt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daPfxAbTt7tt",
        "outputId": "2b2bf314-ef13-4376-90bc-0a783afddec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovered 5 submitted chunks.\n",
            "{'chunk': 0, 'run_tag': '20250930-174241', 'kept': 980, 'altered_answers': 848, 'missing_thinking_tag': 0, 'one_tag_answer': 76, 'dir': '/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/sft_results/20250930-174241_chunk000'}\n",
            "\n",
            "--- SAMPLE (parsed) ---\n",
            "INSTR (trunc):  …\n",
            "OUTPUT (first 240 chars): <thinking>\n",
            "The sequence is relatively short and contains a high proportion of cysteine (C) and histidine (H) residues, which are often involved in metal ion coordination, suggesting a potential zinc finger motif. The presence of multiple ar …\n",
            "--- SAMPLE (raw) ---\n",
            "OUTPUT (first 240 chars): <thinking>\n",
            "The sequence is relatively short and contains a high proportion of cysteine (C) and histidine (H) residues, which are often involved in metal ion coordination, suggesting a potential zinc finger motif. The presence of multiple ar …\n",
            "{'chunk': 1, 'run_tag': '20250930-174245', 'kept': 996, 'altered_answers': 868, 'missing_thinking_tag': 0, 'one_tag_answer': 79, 'dir': '/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/sft_results/20250930-174245_chunk001'}\n",
            "{'chunk': 2, 'run_tag': '20250930-174248', 'kept': 1000, 'altered_answers': 880, 'missing_thinking_tag': 0, 'one_tag_answer': 71, 'dir': '/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/sft_results/20250930-174248_chunk002'}\n",
            "{'chunk': 3, 'run_tag': '20250930-174250', 'kept': 991, 'altered_answers': 881, 'missing_thinking_tag': 0, 'one_tag_answer': 75, 'dir': '/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/sft_results/20250930-174250_chunk003'}\n",
            "{'chunk': 4, 'run_tag': '20250930-174253', 'kept': 1000, 'altered_answers': 867, 'missing_thinking_tag': 0, 'one_tag_answer': 89, 'dir': '/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/sft_results/20250930-174253_chunk004'}\n",
            "\n",
            "=== FETCH SUMMARY ===\n",
            "{'chunks_discovered': 5, 'chunks_fetched_this_run': 5, 'chunks_already_done_skipped': 0, 'chunks_waiting_not_ready': 0, 'records_kept_total': 4967, 'answer_altered_total': 4344, 'missing_thinking_tag_total': 0, 'one_tag_answer_total': 390, 'results_root': '/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT/gpt_batch_protein2desc_fixed_answer/sft_results'}\n"
          ]
        }
      ],
      "source": [
        "# ===================== Fetch ALL submitted fixed-answer batches into subfolders =====================\n",
        "from pathlib import Path\n",
        "import json, re\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Config ---\n",
        "BASE_DIR  = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")  # change if needed\n",
        "OUT_DIR   = BASE_DIR / \"gpt_batch_protein2desc_fixed_answer\"\n",
        "\n",
        "RESULTS_ROOT = OUT_DIR / \"sft_results\"   # per-chunk subfolders land here\n",
        "RESULTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ONLY_COMPLETED     = True    # True: fetch only completed; False: attempt fetch if output_file_id exists anyway\n",
        "FORCE_REDOWNLOAD   = False   # True: always re-download and rebuild even if files exist\n",
        "BUILD_SFT_FILES    = True    # build sft_parsed.json and sft_raw.json\n",
        "PRINT_SAMPLE       = True    # show one parsed & raw sample per run\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# --- Helpers: parsing / extraction ---\n",
        "def fetch_batch_output_if_ready(batch_id: str) -> tuple[bool, bytes|str]:\n",
        "    \"\"\"Return (ok, data-or-status). ok=True & data=bytes of jsonl if downloadable; else ok=False & status str.\"\"\"\n",
        "    b = client.batches.retrieve(batch_id)\n",
        "    status = getattr(b, \"status\", \"unknown\")\n",
        "    if ONLY_COMPLETED and status != \"completed\":\n",
        "        return False, status\n",
        "    if not getattr(b, \"output_file_id\", None):\n",
        "        return False, status\n",
        "    content = client.files.content(b.output_file_id)\n",
        "    return True, content.read()\n",
        "\n",
        "def extract_assistant_content(obj) -> str:\n",
        "    \"\"\"Batch line → response.body.choices[0].message.content\"\"\"\n",
        "    resp = obj.get(\"response\") or {}\n",
        "    body = resp.get(\"body\") if isinstance(resp, dict) else None\n",
        "    src  = body if isinstance(body, dict) else resp\n",
        "    try:\n",
        "        return src[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def extract_system_message(obj) -> str:\n",
        "    \"\"\"Batch line → request.body.messages[*] (system)\"\"\"\n",
        "    req = obj.get(\"request\") or {}\n",
        "    rbody = req.get(\"body\") if isinstance(req, dict) else None\n",
        "    src = rbody if isinstance(rbody, dict) else req\n",
        "    msgs = src.get(\"messages\", []) if isinstance(src, dict) else []\n",
        "    for m in msgs:\n",
        "        if m.get(\"role\") == \"system\":\n",
        "            return m.get(\"content\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def _extract_tag_loose(text: str, tag: str):\n",
        "    if not text:\n",
        "        return \"\", 0, False\n",
        "    open_pat  = re.compile(fr\"<{tag}\\b[^>]*>\", re.I)\n",
        "    close_pat = re.compile(fr\"</{tag}\\s*>\", re.I)\n",
        "    m_open = open_pat.search(text)\n",
        "    if not m_open:\n",
        "        return \"\", 0, False\n",
        "    m_close = close_pat.search(text, m_open.end())\n",
        "    if m_close:\n",
        "        content = text[m_open.end():m_close.start()]\n",
        "        endpos  = m_close.end()\n",
        "    else:\n",
        "        content = text[m_open.end():]\n",
        "        endpos  = len(text)\n",
        "    content = content.strip().rstrip('\", \\t\\r\\n')\n",
        "    return content, endpos, True\n",
        "\n",
        "def parse_thinking_answer_loose(text: str):\n",
        "    \"\"\"Tolerant CoT parser: accepts missing closing tags and open-only <answer>.\"\"\"\n",
        "    if not text:\n",
        "        return \"\", \"\"\n",
        "    thinking, th_end, th_found = _extract_tag_loose(text, \"thinking\")\n",
        "    remainder = text[th_end:] if th_found else text\n",
        "    answer, _, an_found = _extract_tag_loose(remainder, \"answer\")\n",
        "    if not th_found and an_found:\n",
        "        m_open_ans = re.search(r\"<answer\\b[^>]*>\", remainder, re.I)\n",
        "        cutoff = m_open_ans.start() if m_open_ans else len(text)\n",
        "        pre = text[:cutoff].strip().rstrip('\", \\t\\r\\n')\n",
        "        thinking = pre if pre else thinking\n",
        "    if not th_found and not an_found:\n",
        "        thinking = text.strip().rstrip('\", \\t\\r\\n')\n",
        "        answer   = \"\"\n",
        "    return thinking, answer\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def parse_chunk_parts(fname: str):\n",
        "    \"\"\"\n",
        "    From 'protein2desc_fixedans_batch_info_<run_tag>_chunkXYZ.json' return (run_tag, chunk_pad).\n",
        "    \"\"\"\n",
        "    m = re.match(r\"protein2desc_fixedans_batch_info_(?P<tag>.+?)_chunk(?P<pad>\\d{3})\\.json$\", fname)\n",
        "    if not m:\n",
        "        return None, None\n",
        "    return m.group(\"tag\"), m.group(\"pad\")\n",
        "\n",
        "# --- Discover all batch_info files (any run) ---\n",
        "info_files = sorted(OUT_DIR.glob(\"protein2desc_fixedans_batch_info_*_chunk???.json\"))\n",
        "if not info_files:\n",
        "    raise FileNotFoundError(f\"No batch_info files found under {OUT_DIR}\")\n",
        "\n",
        "print(f\"Discovered {len(info_files)} submitted chunks.\")\n",
        "\n",
        "# --- Process each chunk ---\n",
        "global_kept = 0\n",
        "global_altered_answers = 0\n",
        "global_missing_thinking = 0\n",
        "global_one_tag_answer = 0\n",
        "fetched = 0\n",
        "skipped = 0\n",
        "waiting = 0\n",
        "\n",
        "for info_path in info_files:\n",
        "    run_tag, chunk_pad = parse_chunk_parts(info_path.name)\n",
        "    if not run_tag:\n",
        "        print(f\"[WARN] Skip unparsable info file name: {info_path.name}\")\n",
        "        continue\n",
        "\n",
        "    # per-chunk file names\n",
        "    meta_path = OUT_DIR / f\"protein2desc_fixedans_meta_{run_tag}_chunk{chunk_pad}.json\"\n",
        "    out_dir   = RESULTS_ROOT / f\"{run_tag}_chunk{chunk_pad}\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_jsonl_path = out_dir / \"output.jsonl\"\n",
        "    parsed_path    = out_dir / \"sft_parsed.json\"\n",
        "    raw_path       = out_dir / \"sft_raw.json\"\n",
        "\n",
        "    # load batch info\n",
        "    binfo = json.loads(info_path.read_text(encoding=\"utf-8\"))\n",
        "    batch_id = binfo[\"batch_id\"]\n",
        "\n",
        "    # fast-skip if everything already there (and not forcing)\n",
        "    if (out_jsonl_path.exists() and parsed_path.exists() and raw_path.exists()\n",
        "        and meta_path.exists() and not FORCE_REDOWNLOAD):\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    ok, data_or_status = fetch_batch_output_if_ready(batch_id)\n",
        "    if not ok:\n",
        "        waiting += 1\n",
        "        print(f\"[WAIT] {info_path.name}  status={data_or_status}\")\n",
        "        continue\n",
        "\n",
        "    # write downloaded output\n",
        "    out_jsonl_path.write_bytes(data_or_status)\n",
        "\n",
        "    # copy meta & info to subfolder for convenience\n",
        "    if meta_path.exists():\n",
        "        (out_dir / meta_path.name).write_text(meta_path.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
        "    (out_dir / info_path.name).write_text(info_path.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
        "\n",
        "    # Build SFTs (parsed + raw)\n",
        "    if BUILD_SFT_FILES:\n",
        "        assert meta_path.exists(), f\"Missing meta for chunk: {meta_path}\"\n",
        "        meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "        parsed_records = []\n",
        "        raw_records    = []\n",
        "        kept = 0\n",
        "        altered_answers = 0\n",
        "        missing_thinking_tag = 0\n",
        "        one_tag_answer = 0\n",
        "\n",
        "        with open(out_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line=line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                obj = json.loads(line)\n",
        "                if obj.get(\"error\"):\n",
        "                    continue\n",
        "\n",
        "                content = extract_assistant_content(obj) or \"\"\n",
        "                system_prompt = extract_system_message(obj)\n",
        "                cid  = obj.get(\"custom_id\")\n",
        "                mrow = meta.get(cid, {})\n",
        "                uid, seq, gt = mrow.get(\"uniprot_id\"), mrow.get(\"sequence\"), mrow.get(\"ground_truth\")\n",
        "\n",
        "                # Raw copy\n",
        "                raw_records.append({\n",
        "                    \"instruction\": system_prompt,\n",
        "                    \"input\": {\"uniprot_id\": uid, \"protein_sequence\": seq},\n",
        "                    \"output\": content,\n",
        "                    \"meta\": {\"ground_truth_caption\": gt, \"custom_id\": cid, \"run_tag\": run_tag, \"chunk\": int(chunk_pad)}\n",
        "                })\n",
        "\n",
        "                # Parsed, force answer to GT\n",
        "                thinking, model_answer = parse_thinking_answer_loose(content)\n",
        "                if \"<thinking\" not in content.lower():\n",
        "                    missing_thinking_tag += 1\n",
        "                if \"<answer\" in content.lower() and \"</answer>\" not in content.lower():\n",
        "                    one_tag_answer += 1\n",
        "                model_eq_gt = normalize_text(model_answer) == normalize_text(gt)\n",
        "                if not model_eq_gt:\n",
        "                    altered_answers += 1\n",
        "\n",
        "                parsed_records.append({\n",
        "                    \"instruction\": system_prompt,\n",
        "                    \"input\": {\"uniprot_id\": uid, \"protein_sequence\": seq},\n",
        "                    \"output\": f\"<thinking>\\n{thinking}\\n</thinking>\\n\\n<answer>\\n{gt}\\n</answer>\",\n",
        "                    \"meta\": {\n",
        "                        \"ground_truth_caption\": gt,\n",
        "                        \"custom_id\": cid,\n",
        "                        \"run_tag\": run_tag,\n",
        "                        \"chunk\": int(chunk_pad),\n",
        "                        \"model_answer_equal_to_gt\": model_eq_gt\n",
        "                    }\n",
        "                })\n",
        "                kept += 1\n",
        "\n",
        "        parsed_path.write_text(json.dumps(parsed_records, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "        raw_path.write_text(json.dumps(raw_records,    ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "        # per-chunk summary\n",
        "        print({\n",
        "            \"chunk\": int(chunk_pad),\n",
        "            \"run_tag\": run_tag,\n",
        "            \"kept\": kept,\n",
        "            \"altered_answers\": altered_answers,\n",
        "            \"missing_thinking_tag\": missing_thinking_tag,\n",
        "            \"one_tag_answer\": one_tag_answer,\n",
        "            \"dir\": str(out_dir)\n",
        "        })\n",
        "\n",
        "        # global rollup\n",
        "        global_kept += kept\n",
        "        global_altered_answers += altered_answers\n",
        "        global_missing_thinking += missing_thinking_tag\n",
        "        global_one_tag_answer += one_tag_answer\n",
        "\n",
        "        # Optional sample print\n",
        "        if PRINT_SAMPLE and parsed_records:\n",
        "            print(\"\\n--- SAMPLE (parsed) ---\")\n",
        "            print(\"INSTR (trunc):\", parsed_records[0][\"instruction\"][:200], \"…\")\n",
        "            print(\"OUTPUT (first 240 chars):\", parsed_records[0][\"output\"][:240], \"…\")\n",
        "            print(\"--- SAMPLE (raw) ---\")\n",
        "            print(\"OUTPUT (first 240 chars):\", raw_records[0][\"output\"][:240], \"…\")\n",
        "            PRINT_SAMPLE = False  # only once\n",
        "\n",
        "    fetched += 1\n",
        "\n",
        "# --- final report ---\n",
        "print(\"\\n=== FETCH SUMMARY ===\")\n",
        "print({\n",
        "    \"chunks_discovered\": len(info_files),\n",
        "    \"chunks_fetched_this_run\": fetched,\n",
        "    \"chunks_already_done_skipped\": skipped,\n",
        "    \"chunks_waiting_not_ready\": waiting,\n",
        "    \"records_kept_total\": global_kept,\n",
        "    \"answer_altered_total\": global_altered_answers,\n",
        "    \"missing_thinking_tag_total\": global_missing_thinking,\n",
        "    \"one_tag_answer_total\": global_one_tag_answer,\n",
        "    \"results_root\": str(RESULTS_ROOT)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n7aB6HOQ5qOQ",
      "metadata": {
        "id": "n7aB6HOQ5qOQ"
      },
      "source": [
        "## Generate 3Di Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hTQ64mLN5upt",
      "metadata": {
        "id": "hTQ64mLN5upt"
      },
      "outputs": [],
      "source": [
        "# === PDB/mmCIF -> 3Di JSONL builder (Python path via mini3di) ===\n",
        "# Installs (run once per runtime)\n",
        "!pip -q install mini3di biopython pandas tqdm\n",
        "\n",
        "import json, re, html, sys\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Tuple, List\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------- Config ----------\n",
        "BASE_DIR       = Path(\"/content/drive/MyDrive/LLM/Bioreasoner/data/hf/proteinDT\")\n",
        "SWISSPROT_DIR  = BASE_DIR / \"SwissProtCLAP\"                    # expects protein_sequence.txt / text_sequence.txt\n",
        "MANIFEST_CSV   = BASE_DIR / \"subset_1k\" / \"subset_manifest_1k.csv\"  # has uniprot_id column (and maybe caption_preview etc.)\n",
        "DOWNLOAD_DIR   = BASE_DIR / \"downloads\"                        # where you stored AF2/experimental files per UniProt (from earlier steps)\n",
        "OUT_JSONL      = BASE_DIR / \"protein2desc_3di.jsonl\"          # output file\n",
        "PROMPT_TMPL    = (\"You are a professional protein biologist. Based only on the amino-acid sequence, \"\n",
        "                  \"write a natural, concise, biologically accurate description of the protein.\\n\\n\"\n",
        "                  \"Protein sequence:\\n{seq}\\n\")\n",
        "# If you instead want to use the exact prompt you used for SFT: replace PROMPT_TMPL with that string.\n",
        "\n",
        "# ---------- Load SwissProtCLAP sequences & captions ----------\n",
        "def two_line_pairs(path: Path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        while True:\n",
        "            id_line = f.readline()\n",
        "            if not id_line:\n",
        "                break\n",
        "            val_line = f.readline()\n",
        "            if not val_line:\n",
        "                break\n",
        "            yield id_line.strip(), val_line.strip()\n",
        "\n",
        "def strip_html(s: str) -> str:\n",
        "    s = html.unescape(s or \"\")\n",
        "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "seq_fp = SWISSPROT_DIR / \"protein_sequence.txt\"\n",
        "txt_fp = SWISSPROT_DIR / \"text_sequence.txt\"\n",
        "assert seq_fp.exists() and txt_fp.exists(), f\"Missing SwissProtCLAP at {SWISSPROT_DIR}\"\n",
        "\n",
        "id2seq = {pid: seq for pid, seq in two_line_pairs(seq_fp)}\n",
        "id2cap = {pid: strip_html(txt) for pid, txt in two_line_pairs(txt_fp)}\n",
        "\n",
        "# ---------- Load manifest (which UniProt IDs to include) ----------\n",
        "dfm = pd.read_csv(MANIFEST_CSV)\n",
        "if \"uniprot_id\" not in dfm.columns:\n",
        "    # older manifest may use 'uniprot'\n",
        "    if \"uniprot\" in dfm.columns:\n",
        "        dfm = dfm.rename(columns={\"uniprot\": \"uniprot_id\"})\n",
        "    else:\n",
        "        raise ValueError(\"Manifest must include a 'uniprot_id' column.\")\n",
        "\n",
        "uids = [u for u in dfm[\"uniprot_id\"].astype(str).tolist() if u in id2seq]\n",
        "\n",
        "# ---------- mini3di encoder + parsers ----------\n",
        "import mini3di\n",
        "from Bio.PDB import PDBParser, MMCIFParser, PPBuilder\n",
        "from Bio.PDB.Polypeptide import three_to_one\n",
        "\n",
        "ENCODER = mini3di.Encoder()\n",
        "PDB_PARSER   = PDBParser(QUIET=True)\n",
        "MMCIF_PARSER = MMCIFParser(QUIET=True)\n",
        "\n",
        "def structure_seq_from_chain(chain) -> str:\n",
        "    \"\"\"Extract one-letter sequence from a Bio.PDB chain (ignoring non-standard residues).\"\"\"\n",
        "    seq = []\n",
        "    for res in chain:\n",
        "        if res.id[0] != \" \":  # skip hetero/waters\n",
        "            continue\n",
        "        if \"CA\" not in res:\n",
        "            continue\n",
        "        resname = res.get_resname().strip()\n",
        "        try:\n",
        "            aa = three_to_one(resname)\n",
        "        except KeyError:\n",
        "            continue\n",
        "        seq.append(aa)\n",
        "    return \"\".join(seq)\n",
        "\n",
        "def encode_chain_to_3di(chain) -> str:\n",
        "    \"\"\"Get 3Di string for a chain using mini3di.\"\"\"\n",
        "    states = ENCODER.encode_chain(chain)\n",
        "    seq3di = ENCODER.build_sequence(states)\n",
        "    return str(seq3di)\n",
        "\n",
        "def pick_best_chain_for_uid(structure, target_len: int) -> Optional[Tuple[str, str, int]]:\n",
        "    \"\"\"\n",
        "    Heuristic: pick the chain whose extracted AA length is (a) longest and\n",
        "    (b) within 40–120% of the target length (to avoid weird ligands/short peptides).\n",
        "    Returns (chain_id, aa_seq_from_structure, length) or None.\n",
        "    \"\"\"\n",
        "    best = None\n",
        "    for chain in structure.get_chains():\n",
        "        aaseq = structure_seq_from_chain(chain)\n",
        "        L = len(aaseq)\n",
        "        if L == 0:\n",
        "            continue\n",
        "        if target_len and not (0.4 * target_len <= L <= 1.2 * target_len):\n",
        "            continue\n",
        "        if best is None or L > best[2]:\n",
        "            best = (chain.id, aaseq, L)\n",
        "    return best\n",
        "\n",
        "def find_structure_files_for_uid(uid: str) -> List[Path]:\n",
        "    \"\"\"\n",
        "    Look under DOWNLOAD_DIR/<uid>/ and pick:\n",
        "      - AF2 PDB first: AF-<UID>-*.pdb\n",
        "      - else any AF2 CIF: AF-<UID>-*.cif\n",
        "      - else experimental .cif (e.g., 6ABC.cif) as fallback\n",
        "    Return ordered list of candidate files.\n",
        "    \"\"\"\n",
        "    d = DOWNLOAD_DIR / uid\n",
        "    if not d.exists():\n",
        "        return []\n",
        "    files = list(d.glob(\"AF-*-model_*.pdb\")) + list(d.glob(\"AF-*.pdb\"))\n",
        "    files += list(d.glob(\"AF-*-model_*.cif\")) + list(d.glob(\"AF-*.cif\"))\n",
        "    # experimental (RCSB) last\n",
        "    files += sorted([p for p in d.glob(\"*.cif\") if not p.name.startswith(\"AF-\")])\n",
        "    # de-dup while preserving order\n",
        "    seen, out = set(), []\n",
        "    for p in files:\n",
        "        if p.exists() and p not in seen:\n",
        "            out.append(p); seen.add(p)\n",
        "    return out\n",
        "\n",
        "def pdb_or_cif_to_3di(uid: str, aa_seq: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Try available files for this UniProt, return a 3Di string (lowercased for consistency),\n",
        "    preferring AF2 PDB, then AF2 CIF, then experimental CIF.\n",
        "    \"\"\"\n",
        "    candidates = find_structure_files_for_uid(uid)\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    for fp in candidates:\n",
        "        try:\n",
        "            if fp.suffix.lower() == \".pdb\":\n",
        "                struct = PDB_PARSER.get_structure(uid, str(fp))\n",
        "            else:\n",
        "                struct = MMCIF_PARSER.get_structure(uid, str(fp))\n",
        "        except Exception as e:\n",
        "            # parsing failed, try next candidate\n",
        "            continue\n",
        "\n",
        "        # pick best chain\n",
        "        target_len = len(aa_seq)\n",
        "        choice = pick_best_chain_for_uid(struct, target_len)\n",
        "        if choice is None:\n",
        "            continue\n",
        "\n",
        "        chain_id, aaseq_struct, L = choice\n",
        "\n",
        "        # Encode to 3Di\n",
        "        try:\n",
        "            chain = next(c for c in struct.get_chains() if c.id == chain_id)\n",
        "            s3di = encode_chain_to_3di(chain)\n",
        "            # optional sanity: if lengths off by a lot, still return but note difference\n",
        "            return s3di.lower()\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# ---------- Build JSONL ----------\n",
        "OUT_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
        "count_ok = 0\n",
        "count_no_struct = 0\n",
        "\n",
        "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for uid in tqdm(uids, desc=\"Building JSONL\"):\n",
        "        seq = id2seq.get(uid, \"\")\n",
        "        cap = id2cap.get(uid, \"\")\n",
        "        if not seq or not cap:\n",
        "            continue\n",
        "\n",
        "        s3di = pdb_or_cif_to_3di(uid, seq)\n",
        "        if s3di is None:\n",
        "            count_no_struct += 1\n",
        "\n",
        "        prompt = PROMPT_TMPL.format(seq=seq)\n",
        "        record = {\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": cap,          # supervised target\n",
        "            \"aa_seq\": seq,            # optional but recommended\n",
        "            \"stru_str\": s3di or \"\"    # optional; empty if unavailable\n",
        "        }\n",
        "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "        count_ok += 1\n",
        "\n",
        "print(f\"\\nWrote {count_ok} examples to {OUT_JSONL}\")\n",
        "print(f\"Missing structure for {count_no_struct} of them (stru_str left empty).\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
